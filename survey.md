# A Survey on Algorithmic Recourse (in Japanese)

**ルール**
- DBLPを「recourse」「counterfactual」「action」で検索
- タイトルと著者とURLをコピペ
- アブストラクトとその和訳（DeepL）をコピペ

[toc]

## 2024
### [NeurIPS2024 (to appear)]()

### [ICML2024](https://dblp.org/db/conf/icml/icml2024.html)
#### [Learning Decision Trees and Forests with Algorithmic Recourse (Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike)](https://openreview.net/forum?id=blGpu9aGs6)
<details>
    <summary>
        本稿では、リコースアクションの存在を保証しつつ、正確なツリーベースのモデルを学習するための新しいアルゴリズムを提案する。アルゴリズム的リコース（AR）は、モデルによって与えられた望ましくない予測結果を変更するためのリコースアクションを提供することを目的とする。典型的なAR手法は、実行可能なアクションの中で必要な労力を最小化する最適化タスクを解くことで、妥当なアクションを提供する。しかし実際には、予測性能のみに最適化されたモデルには、そのようなアクションは必ずしも存在しない。この問題を緩和するために、我々は、できるだけ多くのインスタンスに対して妥当なアクションの存在を保証するという制約の下で、正確な分類木を学習するというタスクを定式化する。そして、敵対的学習技術を活用した効率的なトップダウン貪欲アルゴリズムを提案する。また、提案するアルゴリズムが、樹木アンサンブルを学習するための一般的なフレームワークとして知られるランダムフォレストに適用可能であることを示す。実験の結果、我々の手法は、精度や計算効率を大幅に低下させることなく、ベースラインよりも多くのインスタンスに対して妥当な行動を提供することに成功した。
    </summary>
    This paper proposes a new algorithm for learning accurate tree-based models while ensuring the existence of recourse actions. Algorithmic Recourse (AR) aims to provide a recourse action for altering the undesired prediction result given by a model. Typical AR methods provide a reasonable action by solving an optimization task of minimizing the required effort among executable actions. In practice, however, such actions do not always exist for models optimized only for predictive performance. To alleviate this issue, we formulate the task of learning an accurate classification tree under the constraint of ensuring the existence of reasonable actions for as many instances as possible. Then, we propose an efficient top-down greedy algorithm by leveraging the adversarial training techniques. We also show that our proposed algorithm can be applied to the random forest, which is known as a popular framework for learning tree ensembles. Experimental results demonstrated that our method successfully provided reasonable actions to more instances than the baselines without significantly degrading accuracy and computational efficiency.
</details>

#### [Counterfactual Metarules for Local and Global Recourse (Tom Bewley, Salim I. Amoukou, Saumitra Mishra, Daniele Magazzeni, Manuela Veloso)](https://openreview.net/forum?id=Ad9msn1SKC)
<details>
    <summary>
        T-CRExは、局所的・大域的な反事実的説明（CE）のための、モデルに依存しない新しい手法であり、個人と集団の両方に対する救済の選択肢を一般化されたルールの形で要約する。T-CRExは、ツリーベースの代理モデルを利用して、反事実的ルールを学習し、その最適化領域を示すメタルールも学習する。実験によれば、T-CRExは既存のルールベースのベースラインと比較して、CEに求められる様々な要求を満たす優れた集計性能を達成し、同時に実行速度も桁違いに高速である。
    </summary>
    We introduce T-CREx, a novel model-agnostic method for local and global counterfactual explanation (CE), which summarises recourse options for both individuals and groups in the form of generalised rules. It leverages tree-based surrogate models to learn the counterfactual rules, alongside metarules denoting their regimes of optimality, providing both a global analysis of model behaviour and diverse recourse options for users. Experiments indicate that T-CREx achieves superior aggregate performance over existing rule-based baselines on a range of CE desiderata, while being orders of magnitude faster to run.
</details>

#### [CF-OPT: Counterfactual Explanations for Structured Prediction (Germain Vivier-Ardisson, Alexandre Forel, Axel Parmentier, Thibaut Vidal)](https://openreview.net/forum?id=xSkIxKdO08)
<details>
    <summary>
        ディープニューラルネットワークの最適化層は、構造化学習において人気が高まっており、様々なアプリケーションの技術水準を向上させている。しかし、これらのパイプラインは、ディープニューラルネットワークのような高度に非線形な予測モデルと、一般的に複雑なブラックボックスソルバーである最適化層という、2つの不透明な層で構成されているため、解釈可能性に欠けている。我々の目標は、反事実的な説明を提供することによって、このような手法の透明性を向上させることである。我々は、変分オートエンコーダをベースに、反事実を得るための原理的な方法を構築する。最後に、VAEトレーニングのための古典的な損失の変形を導入し、我々の特定の構造化された文脈における性能を向上させる。これらはCF-OPTの基礎となるものであり、構造化学習アーキテクチャの幅広いクラスに対して、反事実的説明を見つけることができる一次最適化アルゴリズムである。我々の数値結果は、最近の文献から得られた問題に対して、近い説明ともっともらしい説明の両方が得られることを示している。
    </summary>
    Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.
</details>

#### [Trustworthy Actionable Perturbations (Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon)](https://openreview.net/forum?id=zkjGpZrIX3)
<details>
    <summary>
        反事実、すなわち異なる結果を導く修正入力は、機械学習分類器が使用するロジックを理解し、望ましくない分類を変更する方法を理解するための重要なツールである。しかし、たとえ反事実が分類器の判断を変えたとしても、その反事実が真のクラス確率に影響を与えない可能性がある。つまり、反事実は敵対的な攻撃のように振る舞い、分類器を「愚弄」する可能性がある。我々は、Trustworthy Actionable Perturbations (TAP)と呼ぶ、真の基礎となる確率を有益に変化させる修正入力を作成するための新しいフレームワークを提案する。これには、TAPが敵対的に作用するのではなく、真のクラス確率を変更することを保証するための新しい検証手順が含まれる。また、我々のフレームワークには、現実世界において変化をもたらすのに適した、新しいコスト、報酬、目標の定義が含まれている。検証手順に関するPAC学習可能性の結果を示し、報酬を測定するための新しい方法を理論的に分析する。また、TAPを作成するための方法論を開発し、これまでの反事実的手法で達成された結果と比較する。
    </summary>
    Counterfactuals, or modified inputs that lead to a different outcome, are an important tool for understanding the logic used by machine learning classifiers and how to change an undesirable classification. Even if a counterfactual changes a classifier's decision, however, it may not affect the true underlying class probabilities, i.e. the counterfactual may act like an adversarial attack and ``fool'' the classifier. We propose a new framework for creating modified inputs that change the true underlying probabilities in a beneficial way which we call Trustworthy Actionable Perturbations (TAP). This includes a novel verification procedure to ensure that TAP change the true class probabilities instead of acting adversarially. Our framework also includes new cost, reward, and goal definitions that are better suited to effectuating change in the real world. We present PAC-learnability results for our verification procedure and theoretically analyze our new method for measuring reward. We also develop a methodology for creating TAP and compare our results to those achieved by previous counterfactual methods.
</details>

### [IJCAI2024](https://www.ijcai.org/proceedings/2024/)
#### [A New Paradigm for Counterfactual Reasoning in Fairness and Recourse (Lucius E.J. Bynum, Joshua R. Loftus, Julia Stoyanovich)](https://www.ijcai.org/proceedings/2024/784)
<details>
    <summary>
        反実仮想は、人工知能（AI）システムを監査し理解するための数多くの技術を支えている。 この文献における反事実推論の伝統的なパラダイムは介入的反事実であり、そこでは仮想的な介入が想像され、シミュレートされる。 このため、AIにおける法的保護と人口統計データに関する因果推論の出発点は、民族、人種、性別、障害、年齢など、法的に保護された特性に対する想像上の介入である。 例えば、あなたの人種が違っていたらどうなっていただろうかと問うのである。 このパラダイムの本質的な限界は、人種に関する介入のような人口統計学的介入がうまく定義されなかったり、介入的反事実の形式論に当てはまらなかったりすることである。 そこでは、法的に保護された特性に対する仮定の介入を想像するのではなく、これらの特性を固定したまま別の初期条件を想像するのである。 その代わりに、実際に自分がそうである、あるいはそうなりうるという反事実的な結果を説明するものは何か、と問うのである。 この別の枠組みは、同じ社会的懸念の多くに取り組むことを可能にするが、人口統計学的介入に依存しない根本的に異なる質問をしながらそうすることを可能にする。 
    </summary>
    Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions — like interventions on race — may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.
</details>

#### [Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective (Tomu Tominaga, Naomi Yamashita, Takeshi Kurashima)](https://www.ijcai.org/proceedings/2024/876)
<details>
    <summary>
        本研究では、アルゴリズム的リコース（AIシステムによって下された不利な決定を覆すために、個人を支援する反事実的行動計画（すなわちリコース）を生成するプロセス）の基礎的前提を批判的に検討する。 アルゴリズミック・リコースの根底にある仮定は、個人が現在の状態と望ましい状態との間のギャップを最小化するようなリコースを受け入れ、それに基づいて行動するというものである。 しかし、この仮定は経験的に検証されていない。 この問題に対処するため、我々は362人の参加者を対象にユーザー研究を実施し、現在と望ましい状態のギャップの指標である距離関数を最小化することが、本当に提案されたリカースを受け入れ、行動することを促すかどうかを評価した。 我々の発見は、微妙な情景を明らかにした：参加者のリコースの受け入れは、リコース距離と相関しなかった。 さらに、参加者のリコースに対する行動意欲は、リコース距離が最小のときにピークに達するが、それ以外は一定であった。 これらの知見は、アルゴリズムによるリコース研究の一般的な前提に疑問を投げかけ、人間中心のリコース生成への道を開くために評価機能を再考する必要性を示唆している。
    </summary>
    In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.
</details>

#### [CMACE: CMAES-based Counterfactual Explanations for Black-box Models (Xudong Yin, Yao Yang)](https://www.ijcai.org/proceedings/2024/60)
<details>
    <summary>
        説明型人工知能は機械学習において重要な役割を担っている。それは、例えば信用融資のような意思決定の場面で広く応用されているからである。 反実仮想説明（CFE）は、「もしも」、すなわちモデル入力がわずかに変化したらどうなったかを問う、新しいタイプの説明手法である。 この問いに答えるために、Counterfactual Explanationは、異なるモデル決定を導くモデル入力の最小摂動を見つけることを目的としている。 モデルにとらわれないアプローチと比較すると、特定のタイプのモデルに対してのみ設計されたモデル固有のCFEアプローチは、モデルの内部構造にアクセスできるため、最適な反事実的摂動を見つけることにおいて、通常、より良い性能を持つ。 このジレンマに対処するため、本研究ではまず、CMAES-based Counterfactual Explanations(CMACE)を提案する。CMACEは、共分散行列適応進化戦略(CMA-ES)と、学習サンプルの事前情報を利用してCMA-ESのための反事実の平均と共分散パラメータの良好な初期化を提供するウォームスタートスキームに基づく、効果的なモデル非依存的反事実生成アプローチである。 CMACEは、様々な実験設定において、他の最新（SOTA）のモデル不可知的アプローチ（Bayesian Counterfactual Generator, BayCon）を大幅に上回る性能を示した。 また、広範な実験により、CMACEは、勾配ベースの最適化を用いたツリーベースのモデル用に設計されたSOTAモデル固有のアプローチ（Flexible Optimizable Counterfactual Explanations for Tree Ensembles, FOCUS）よりも優れていることが実証された。
    </summary>
    Explanatory Artificial Intelligence plays a vital role in machine learning, due to its widespread application in decision-making scenarios, e.g., credit lending. Counterfactual Explanation (CFE) is a new kind of explanatory method that involves asking “what if ”, i.e. what would have happened if model inputs slightly change. To answer the question, Counterfactual Explanation aims at finding a minimum perturbation in model inputs leading to a different model decision. Compared with model-agnostic approaches, model-specific CFE approaches designed only for specific type of models usually have better performance in finding optimal counterfactual perturbations, owing to access to the inner workings of models. To deal with this dilemma, this work first proposes CMAES-based Counterfactual Explanations (CMACE): an effective model-agnostic counterfactual generating approach based on Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and a warm starting scheme that provides good initialization of the counterfactual's mean and covariance parameters for CMA-ES taking advantage of prior information of training samples. CMACE significantly outperforms another state-of-art (SOTA) model-agnostic approach (Bayesian Counterfactual Generator, BayCon) with various experimental settings. Extensive experiments also demonstrate that CMACE is superior to a SOTA model-specific approach (Flexible Optimizable Counterfactual Explanations for Tree Ensembles, FOCUS) that is designed for tree-based models using gradient-based optimization.
</details>

#### [Robust Counterfactual Explanations in Machine Learning: A Survey (Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni)](https://www.ijcai.org/proceedings/2024/894)
<details>
    <summary>
       反実仮想説明（CE）は、機械学習モデルの予測によって影響を受けた被験者にアルゴリズムによる救済を提供するのに理想的であると提唱されている。 CEは影響を受ける被験者にとって有益であるが、最近の研究では、CEを得るための最新の手法のロバスト性に関する深刻な問題が露呈している。 ロバスト性の欠如はCEの妥当性を損なう可能性があるため、このリスクを軽減する技術が求められている。 本サーベイでは、急成長しているロバストCEの分野の研究をレビューし、それらが考慮するロバスト性の形態について詳細な分析を行う。 また、既存の解決策とその限界についても議論し、今後の開発のための確かな基礎を提供する。 
    </summary>
    Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
</details>


### [KDD2024](https://dblp.org/db/conf/kdd/kdd2024.html)
#### [Unraveling Block Maxima Forecasting Models with Counterfactual Explanation (Yue Deng, Asadullah Hill Galib, Pang-Ning Tan, Lifeng Luo)](https://dl.acm.org/doi/10.1145/3637528.3671923)
<details>
    <summary>
        疾病監視、交通管理、気象予測は、時系列のブロック最大値予測から恩恵を受ける可能性のある重要なアプリケーションの一部である。 ブロック・マキシマ予測へのディープ・ニューラル・ネットワーク・モデルの利用が増加するにつれて、このようなブラックボックス・モデルの内部構造を解明できる説明可能なAI手法の必要性も高まっている。 このニーズを満たすために、本稿ではブロック最大値予測モデルのための新しい反事実説明フレームワークを提示する。 既存の手法とは異なり、我々の提案するフレームワークであるDiffusionCFは、予測された極端なブロック最大値を説明するのに役立つ時系列中の異常なパターンを識別するために、ディープアノマリー検出と条件付き拡散モデルを組み合わせる。 幾つかの実世界のデータセットを用いた実験結果は、様々なメトリクス、特に情報量と接近度に従って評価した場合、DiffusionCFが他のベースライン手法よりも優れていることを実証する。
    </summary>
    Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF.
</details>

#### [Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals (Bardh Prenkaj, Mario Villaizán-Vallelado, Tobias Leemann, Gjergji KasneciAuthors)](https://dl.acm.org/doi/10.1145/3637528.3671831)
<details>
    <summary>
        我々は、動的に変化するグラフデータの生成的分類と反事実的説明のための新しいアプローチであるGRACIE（Graph Recalibration and Adaptive Counterfactual Inspection and Explanation）を発表する。 我々は、生成的分類器のレンズを通してグラフ分類問題を研究する。 入力グラフのもっともらしい反事実を特定し、対比的最適化によって決定境界を再調整することによって更新する、動的な自己教師付き潜在変数モデルを提案する。 先行研究とは異なり、学習されたグラフ表現間の線形分離性に依存することなく、妥当な反事実を発見する。 さらに、GRACIEは、潜在空間における確率的サンプリングやグラフマッチングのヒューリスティックを必要としない。 我々の研究は、潜在空間における生成的分類と損失関数の間の暗黙のリンクを抽出するものであり、このアーキテクチャにおける最近の成功を理解するための重要な洞察である。 さらに、有効性と潜在空間の中心領域への説明対象インスタンスの引き込みとの間の本質的なトレードオフを観察し、我々の理論的知見を経験的に実証する。 合成グラフデータと実世界のグラフデータを用いた広範な実験において、動的なデータランドスケープという困難な設定においても、反事実の集合をサンプリングする際に、妥当性が～99%に達するという大幅な改善を達成した。
    </summary>
    We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.
</details>

#### [Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning (Danqing Wang, Antonis Antoniades, Kha-Dinh Luong, Edwin Zhang, Mert Kosan, Jiachen Li, Ambuj Singh, William Yang Wang, Lei Li)](https://dl.acm.org/doi/10.1145/3637528.3672045)
<details>
    <summary>
        グラフニューラルネットワーク（GNN）の反実仮想的説明は、グラフ構造で自然に表現できるデータを理解するための強力な方法を提供する。 さらに、多くの領域において、問題となっているモデルやデータのハイレベルな特性をより良く説明できるような、データ駆動型のグローバルな説明やルールを導出することが非常に望まれている。 しかし、実世界のデータセットでは、人間が注釈を付けたグランドトゥルースがないため、グローバルな反事実的説明を評価することは困難であり、分子科学のような分野での利用が制限されている。 さらに、これらのデータセットの規模が大きくなっているため、ランダムサーチベースの手法には課題がある。 本論文では、分子物性予測のための新しいグローバル説明モデルRLHEXを開発する。 RLHEXは、反実仮想的な説明を人間が定義した原理に合わせることで、説明をより解釈しやすくし、専門家が評価しやすくする。 RLHEXには、グローバルな説明を生成するためのVAEベースのグラフ生成器と、潜在表現空間を人間が定義した原理に合わせるためのアダプターが含まれている。 Proximal Policy Optimization (PPO)により最適化されたRLHEXにより生成されたグローバル説明は、4.12%多くの入力グラフをカバーし、3つの分子データセットにおいて、反事実説明セットと入力セットの間の距離を平均0.47%減少させた。 RLHEXは、人間が設計した様々な原理を反事実説明生成プロセスに組み込む柔軟なフレームワークを提供し、これらの説明をドメインの専門知識と整合させる。
    </summary>
    Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX.
</details>


### [ICLR2024](https://dblp.org/db/conf/iclr/iclr2024.html)
#### [Prediction without Preclusion: Recourse Verification with Reachable Sets (Avni Kothari, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun)](https://openreview.net/forum?id=SCQfYpdoGE)
<details>
    <summary>
       機械学習モデルは、ローンや就職面接、公的給付を誰が受けるかを決めるためによく使われる。 このような場 合のモデルは、行動可能性を考慮せずに特徴を使用する。 つまり、ローンや面接を拒否された個人は、実際、信用や雇用へのアクセスから排除されるのである。 この研究では、モデルが決定対象に固定的な予測を割り当てているかどうかをテストするために、リコース検証（recourse verification）と呼ばれる手順を導入する。 我々は、到達可能集合を用いた検証のための、モデルにとらわれないアプローチを提案する。 我々は、離散的な特徴空間に対して到達可能集合を構築する手法を開発し、その予測値を問い合わせるだけであらゆるモデルの応答性を証明することができる。 我々は、消費者金融のデータセットにおけるリコースの実現不可能性に関する包括的な実証研究を行う。 我々の結果は、モデルが固定的な予測値を割り当てることによって、いかに不注意にアクセスを妨げるかを浮き彫りにし、モデル開発においてアクショナビリティを考慮する必要性を強調している。 
    </summary>
    Machine learning models are often used to decide who receives a loan, a job interview, or a public benefit. Models in such settings use features without considering their actionability. As a result, they can assign predictions that are \emph{fixed} -- meaning that individuals who are denied loans and interviews are, in fact, precluded from access to credit and employment. In this work, we introduce a procedure called recourse verification to test if a model assigns fixed predictions to its decision subjects. We propose a model-agnostic approach for verification with reachable sets -- i.e., the set of all points that a person can reach through their actions in feature space. We develop methods to construct reachable sets for discrete feature spaces, which can certify the responsiveness of any model by simply querying its predictions. We conduct a comprehensive empirical study on the infeasibility of recourse on datasets from consumer finance. Our results highlight how models can inadvertently preclude access by assigning fixed predictions and underscore the need to account for actionability in model development.
</details>

#### [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations (Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah)](https://openreview.net/forum?id=qoHeuRAcSl)
<details>
    <summary>
        物理的領域における大規模言語モデルの常識的推論の根拠付けは、具現化AIにとって極めて重要でありながら未解決の問題である。 先行研究が、記号空間におけるプランニングのためにLLMを直接活用することに焦点を当てているのに対し、本研究では、マルチステップのデモンストレーションに暗黙的に含まれるタスク構造と制約の探索を導くためにLLMを使用する。 具体的には、LLMの高レベル言語表現とロボットの低レベル物理軌道の間の抽象化レイヤとして機能するために、特定の動作制約によってロボット構成をグループ化するモードファミリーの概念を操作計画文献から借用する。 合成的な摂動で人間の実演を数回再生することにより、実演の状態空間に対するカバレッジを生成し、成功した実行とタスクに失敗した反事実を追加する。 我々の説明ベースの学習フレームワークは、失敗から成功の軌跡を予測するために、エンドツーエンドの微分可能なニューラルネットワークを学習し、副産物として、密なラベリングなしに、モードファミリーの低レベルの状態と画像を接地する分類器を学習する。 学習された接地分類器はさらに、言語計画を解釈可能な方法で物理領域の反応ポリシーに変換するために使用することができる。 我々は、2次元ナビゲーションとシミュレーションされた実際のロボット操作タスクを通して、我々のアプローチが模倣学習の解釈可能性と反応性を向上させることを示す。
    </summary>
    Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/
</details>

#### [UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models (Hyunju Kang, Geonhee Han, Hogun Park)](https://openreview.net/forum?id=0j9ZDzMPqr)
<details>
    <summary>
        グラフニューラルネットワーク(GNN)などのノード表現学習は、機械学習における重要な学習手法の一つとなっており、信頼性の高い説明生成の需要が高まっている。 教師ありノード表現学習における説明生成に関する研究は盛んに行われているが、教師なしモデルの説明はあまり研究されていない。 このギャップを解決するために、我々は教師なしノード表現学習における反実仮想（CF）説明生成法を提案する。この方法は、学習された埋め込み空間において、摂動によって注目ノードの最近傍に大きな変化を引き起こす最も重要な部分グラフを同定することを目的とする。 k-最近傍ベースのCF説明手法は、トップkリンク予測やクラスタリングといった教師なしタスクの下流を理解するための、シンプルでありながら極めて重要な情報を提供する。 さらに、教師なしノード表現学習法に対する表現力豊かなCF説明を生成するための、モンテカルロ木探索(MCTS)に基づく説明可能性手法を紹介する。 提案手法は、教師なしGraphSAGEとDGIの両方について、6つのデータセットで改善された性能を示す。
    </summary>
    Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the nearest neighbors of a node of interest in the learned embedding space upon perturbation. The k-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-k link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for Unsupervised Node Representation learning methods, which we call UNR-Explainer. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI.
</details>

#### [Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals (Yair Ori Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart)](https://openreview.net/forum?id=UMfcdRIotC)
<details>
    <summary>
        自然言語処理システムの予測に関する因果関係の説明は、安全性を確保し、信頼を確立するために不可欠である。 しかし、既存の手法では、モデルの予測を効果的、効率的に説明できないことが多く、また、モデルに依存することが多い。 本稿では、モデルにとらわれない説明に取り組み、反事実（CF）近似のための2つのアプローチを提案する。 第一のアプローチはCF生成であり、大規模言語モデル（LLM）が、交絡する概念を変更しないまま、特定のテキスト概念を変更するように促す。 このアプローチは非常に効果的であることが実証されているが、推論時にLLMを適用することはコストがかかる。 そこで我々は、マッチングに基づく第二のアプローチを提示し、学習時にLLMによって導かれ、専用の埋め込み空間を学習する手法を提案する。 この空間は与えられた因果グラフに忠実であり、CFに近似するマッチングを効果的に識別するのに役立つ。 忠実な説明を構築するためにはCFを近似することが必要であることを理論的に示した後、我々のアプローチをベンチマークし、数十億のパラメータを持つLLMを含むいくつかのモデルを説明する。 我々の実証結果は、モデルにとらわれない説明器としてのCF生成モデルの優れた性能を実証している。 さらに、テスト時間のリソースがはるかに少ない我々のマッチングアプローチも、多くのベースラインを上回る効果的な説明を提供する。 また、Top-K手法はテストされた全ての手法を普遍的に改善することがわかった。 最後に、モデル説明のための新しいベンチマークを構築する際のLLMの可能性を示し、その後に我々の結論を検証する。 我々の研究は、自然言語処理システムを解釈するための効率的で正確なアプローチのための新たな道筋を示すものである。
    </summary>
    Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.
</details>


### [AISTATS2024](https://dblp.org/db/conf/aistats/aistats2024.html)
#### [The Risks of Recourse in Binary Classification (Hidde Fokkema, Damien Garreau, Tim van Erven)](https://proceedings.mlr.press/v238/fokkema24a.html)
<details>
    <summary>
        アルゴリズム・リコース（algorithmic recourse）は、機械学習システムによる不利な決定をユーザーが覆すための説明を提供する。 しかし、これまでのところ、遡求を提供することが有益であるか否かについては、ほとんど注目されていない。 我々は抽象的な学習理論的枠組みを導入し、アルゴリズムによる遡及がある場合とない場合の分類のリスク（すなわち期待損失）を比較する。 これにより、集団レベルにおいて、どのような場合に再利用を提供することが有益か有害かという問いに答えることができる。 意外なことに、我々は、遡及手段を提供することが有害であることが判明する多くのもっともらしいシナリオが存在することを発見した。 さらに、分類器を配置する側に、遡及手段を提供しなければならないことを予期して戦略を立てるインセンティブがあるかどうかを調査したところ、ユーザにとって不利になるような遡及手段を提供することがあることがわかった。 従って、アルゴリズムによる救済を提供することは、システムレベルにおいても有害である可能性がある。 我々は、シミュレーションと実世界のデータを用いた実験で、理論的な発見を確認した。 全体として、我々は、アルゴリズムによるリコースの現在のコンセプトは、確実には有益ではなく、したがって再考が必要であると結論づける。
    </summary>
    Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e., expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level. We confirm our theoretical findings in experiments on simulated and real-world data. All in all, we conclude that the current concept of algorithmic recourse is not reliably beneficial, and therefore requires rethinking.
</details>

#### [Manifold-Aligned Counterfactual Explanations for Neural Networks (Asterios Tsiourvas, Wei Sun, Georgia Perakis)](https://proceedings.mlr.press/v238/tsiourvas24a.html)
<details>
    <summary>
        我々は、ニューラルネットワークのための最適な多様体整合的な反事実説明を見つける問題について研究する。 複雑な混合整数最適化(MIP)問題を解く既存のアプローチは、しばしばスケーラビリティの問題に悩まされ、実用的な有用性が制限される。 さらに、解がデータ多様体に従うことが保証されていないため、非現実的な反事実説明が生じる。 これらの課題に対処するため、我々はまず、非線形性の高い局所外れ値因子（LOF）メトリックを混合整数制約として再定式化することにより、多様体の整合を明示的に強制するMIP定式化を提示する。 計算上の課題に対処するため、学習済みニューラルネットワークの幾何学的形状を活用し、探索空間を「生きた」ポリトープ、すなわち、少なくとも1つの実際のデータ点を含む領域に制約することにより、最初の大きくて解きにくい最適化問題を、一連の非常に小さくて解きやすい問題に縮小する効率的な分解スキームを提案する。 実世界のデータセットを用いた実験により、最適かつ現実的な反事実の説明と計算の追跡可能性の両方を生成する上で、我々のアプローチの有効性が実証された。
    </summary>
    We study the problem of finding optimal manifold-aligned counterfactual explanations for neural networks. Existing approaches that involve solving a complex mixed-integer optimization (MIP) problem frequently suffer from scalability issues, limiting their practical usefulness. Furthermore, the solutions are not guaranteed to follow the data manifold, resulting in unrealistic counterfactual explanations. To address these challenges, we first present a MIP formulation where we explicitly enforce manifold alignment by reformulating the highly nonlinear Local Outlier Factor (LOF) metric as mixed-integer constraints. To address the computational challenge, we leverage the geometry of a trained neural network and propose an efficient decomposition scheme that reduces the initial large, hard-to-solve optimization problem into a series of significantly smaller, easier-to-solve problems by constraining the search space to “live” polytopes, i.e., regions that contain at least one actual data point. Experiments on real-world datasets demonstrate the efficacy of our approach in producing both optimal and realistic counterfactual explanations, and computational traceability.
</details>


### [AAAI2024](https://dblp.org/db/conf/aaai/aaai2024.html)
#### [SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies (Wu, H., Sharma, S., Patra, S., & Gopalakrishnan, S.)](https://ojs.aaai.org/index.php/AAAI/article/view/29522)
<details>
    <summary>
        我々は、ニューラルネットワークのための最適な多様体整合的な反事実説明を見つける問題について研究する。 複雑な混合整数最適化(MIP)問題を解く既存のアプローチは、しばしばスケーラビリティの問題に悩まされ、実用的な有用性が制限される。 さらに、解がデータ多様体に従うことが保証されていないため、非現実的な反事実説明が生じる。 これらの課題に対処するため、我々はまず、非線形性の高い局所外れ値因子（LOF）メトリックを混合整数制約として再定式化することにより、多様体の整合を明示的に強制するMIP定式化を提示する。 計算上の課題に対処するため、学習済みニューラルネットワークの幾何学的形状を活用し、探索空間を「生きた」ポリトープ、すなわち、少なくとも1つの実際のデータ点を含む領域に制約することにより、最初の大きくて解きにくい最適化問題を、一連の非常に小さくて解きやすい問題に縮小する効率的な分解スキームを提案する。 実世界のデータセットを用いた実験により、最適かつ現実的な反事実の説明と計算の追跡可能性の両方を生成する上で、我々のアプローチの有効性が実証された。
    </summary>
    With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receiving a favorable decision. Prior work on sequential algorithmic recourse---which recommends a series of changes---focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safe Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures "Value at Risk" and "Conditional Value at Risk" from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different risk-aversion levels using risk measures and recourse desiderata (sparsity and proximity). 
</details>

#### [Providing Fair Recourse over Plausible Groups (Yetukuri, J., Hardy, I., Vorobeychik, Y., Ustun, B., & Liu, Y.)](https://ojs.aaai.org/index.php/AAAI/article/view/30175)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    Machine learning models now automate decisions in applications where we may wish to provide recourse to adversely affected individuals. In practice, existing methods to provide recourse return actions that fail to account for latent characteristics that are not captured in the model (e.g., age, sex, marital status). In this paper, we study how the cost and feasibility of recourse can change across these latent groups. We introduce a notion of group-level plausibility to identify groups of individuals with a shared set of latent characteristics. We develop a general-purpose clustering procedure to identify groups from samples. Further, we propose a constrained optimization approach to learn models that equalize the cost of recourse over latent groups. We evaluate our approach through an empirical study on simulated and real-world datasets, showing that it can produce models that have better performance in terms of overall costs and feasibility at a group level.
</details>

#### [Counterfactual Explanations for Misclassified Images: How Human and Machine Explanations Differ (Abstract Reprint) (Delaney, E., Pakrashi, A., Greene, D., & Keane, M. T.)](https://ojs.aaai.org/index.php/AAAI/article/view/30596)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    Counterfactual explanations have emerged as a popular solution for the eXplainable AI (XAI) problem of elucidating the predictions of black-box deep-learning systems because people easily understand them, they apply across different problem domains and seem to be legally compliant. Although over 100 counterfactual methods exist in the XAI literature, each claiming to generate plausible explanations akin to those preferred by people, few of these methods have actually been tested on users (∼7%). Even fewer studies adopt a user-centered perspective; for instance, asking people for their counterfactual explanations to determine their perspective on a “good explanation”. This gap in the literature is addressed here using a novel methodology that (i) gathers human-generated counterfactual explanations for misclassified images, in two user studies and, then, (ii) compares these human-generated explanations to computationally-generated explanations for the same misclassifications. Results indicate that humans do not “minimally edit” images when generating counterfactual explanations. Instead, they make larger, “meaningful” edits that better approximate prototypes in the counterfactual class. An analysis based on “explanation goals” is proposed to account for this divergence between human and machine explanations. The implications of these proposals for future work are discussed.
</details>

#### [A General Search-Based Framework for Generating Textual Counterfactual Explanations (Gilo, D., & Markovitch, S.)](https://ojs.aaai.org/index.php/AAAI/article/view/29764)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    One of the prominent methods for explaining the decision of a machine-learning classifier is by a counterfactual example. Most current algorithms for generating such examples in the textual domain are based on generative language models. Generative models, however, are trained to minimize a specific loss function in order to fulfill certain requirements for the generated texts. Any change in the requirements may necessitate costly retraining, thus potentially limiting their applicability. In this paper, we present a general search-based framework for generating counterfactual explanations in the textual domain. Our framework is model-agnostic, domain-agnostic, anytime, and does not require retraining in order to adapt to changes in the user requirements. We model the task as a search problem in a space where the initial state is the classified text, and the goal state is a text in a given target class. Our framework includes domain-independent modification operators, but can also exploit domain-specific knowledge through specialized operators. The search algorithm attempts to find a text from the target class with minimal user-specified distance from the original classified object.
</details>

#### [Robust Stochastic Graph Generator for Counterfactual Explanations (Prado-Romero, M. A., Prenkaj, B., & Stilo, G.)](https://ojs.aaai.org/index.php/AAAI/article/view/30149)
<details>
    <summary>
        反事実的説明（Counterfactual Explanation：CE）手法は、AIシステムに関わるユーザに洞察を提供する手段として注目されている。 医療画像や自律走行車などの領域で広く研究されている一方で、グラフ反実仮想説明（GCE）手法は比較的十分に研究されていない。 GCEは、基礎となる予測モデルに基づいた異なる結果を持つ、元のグラフに類似した新しいグラフを生成する。 これらの GCE 技術のうち、生成メカニズムに根ざしたものは、芸術的なスタイルや自然言語モデリングなど、他の領域で印象的な成果を示しているにもかかわらず、比較的限定的な調査しか受けていない。 生成的説明器が好まれるのは、入力グラフの摂動を自律的に獲得することで、推論中に反事実的な事例を生成する能力に由来する。 本研究では、RSGG-CEを導入する。RSGG-CEは、学習された潜在空間から、部分的に順序付けられた生成順序を考慮して、反事実的な事例を生成することができる、新しい反事実的説明のための頑健な確率的グラフ生成器である。 さらに、RSGG-CEの性能をSoA生成説明器と比較するための定量的・定性的分析を行い、もっともらしい反事実候補を生成する能力が向上していることを明らかにする。
    </summary>
    Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanations able to produce counterfactual examples from the learned latent space considering a partially ordered generation sequence. Furthermore, we undertake quantitative and qualitative analyses to compare RSGG-CE's performance against SoA generative explainers, highlighting its increased ability to engendering plausible counterfactual candidates.
</details>

#### [Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks (Wang, Z., Zeng, Q., Lin, W., Jiang, M., & Tan, K. C.)](https://ojs.aaai.org/index.php/AAAI/article/view/30168)
<details>
    <summary>
        公平なグラフ・ニューラル・ネットワーク（GNN）は、実生活における大きな賭けに対してアルゴリズムの公平性を促進するために数多く提案されている。 一方、説明可能性は、一般に、人間が理解可能な説明を提供することによって、機械学習の実務家がモデルをデバッグするのを助けるために提案されている。 しかし、GNNにおける公平性診断のための説明を生成するための説明可能性に関する研究はほとんど行われていない。 本稿では、説明可能性の観点から、どのような部分グラフパターンがGNNの偏った振る舞いを引き起こすのか、また、偏りを是正するために実務者はどのような行動を取ることができるのか、という問題を探求する。 この2つの問いに答えることで、本論文は差別的振る舞いの原因となる、コンパクトで診断可能、かつ実行可能な説明を生成することを目的とする。 具体的には、診断可能で行動可能な説明を生成する問題を、多目的組み合わせ最適化問題として定式化する。 この問題を解決するために、GNNの説明可能性と公平性を一度に確保するための多目的進化アルゴリズムを提示する。 特に、進化アルゴリズムの計算効率を高めるために、影響ノードに基づく勾配近似を開発する。 提案するフレームワークの有効性を説明するために、理論的分析を行う。 広範な実験を行い、分類性能、公平性、解釈可能性の観点から提案手法の優位性を実証する。
    </summary>
    A plethora of fair graph neural networks (GNNs) have been proposed to promote algorithmic fairness for high-stake real-life contexts. Meanwhile, explainability is generally proposed to help machine learning practitioners debug models by providing human-understandable explanations. However, seldom work on explainability is made to generate explanations for fairness diagnosis in GNNs. From the explainability perspective, this paper explores the problem of what subgraph patterns cause the biased behavior of GNNs, and what actions could practitioners take to rectify the bias? By answering the two questions, this paper aims to produce compact, diagnostic, and actionable explanations that are responsible for discriminatory behavior. Specifically, we formulate the problem of generating diagnostic and actionable explanations as a multi-objective combinatorial optimization problem. To solve the problem, a dedicated multi-objective evolutionary algorithm is presented to ensure GNNs' explainability and fairness in one go. In particular, an influenced nodes-based gradient approximation is developed to boost the computation efficiency of the evolutionary algorithm. We provide a theoretical analysis to illustrate the effectiveness of the proposed framework. Extensive experiments have been conducted to demonstrate the superiority of the proposed method in terms of classification performance, fairness, and interpretability.
</details>


### [FAccT2024](https://dblp.org/db/conf/fat/facct2024.html)
#### [Actionable Recourse for Automated Decisions: Examining the Effects of Counterfactual Explanation Type and Presentation on Lay User Understanding (Peter M. VanNostrand, Dennis M. Hofmann, Lei Ma, Elke A. Rundensteiner)](https://dl.acm.org/doi/10.1145/3630106.3658997)
<details>
    <summary>
        自動意思決定システムは、ネガティブな結果が意思決定対象者に大きな影響を与える可能性のある、雇用や信用承認などの領域でますます導入されるようになっている。 そのため、最近の研究では、意思決定対象者が意思決定システムを理解し、結果を変更するために行動可能な手段を取ることを可能にする説明を提供することに焦点が当てられている。 一般的な反実仮想説明技法は、否定的な結果を肯定的な結果に変えるインスタンスへの変更を説明することによって、これを達成することを目的としている。 残念なことに、多くの反実仮想的アプローチのうち、どれが最もこの目的を達成できるかを評価するためのユーザー評価はほとんど行われていない。 本研究では、クラウドソースによる被験者間ユーザー研究（N = 252）を実施し、自動意思決定システムに対する素人の意思決定被験者の理解に及ぼす反事実説明の種類と提示の効果を検証する。 その結果、地域ベースの反実仮想タイプは、点ベースのタイプと比較して、客観的理解、主観的理解、回答確信度を有意に増加させることがわかった。 また、反実仮想の提示は回答時間に有意に影響し、回答確信度に対する反実仮想タイプの効果を緩和するが、理解度には影響しないことがわかった。 質的分析により、意思決定主体が異なる説明構成とどのように相互作用するかが明らかになり、説明正当化に対する満たされていないニーズが浮き彫りになった。 我々の結果は、自動化された意思決定ワークフローにおいて、実用的な行動可能手段を達成し、一般利用者に正義と機会を求める力を与えるための、反実仮想的説明技術の開発に対する貴重な洞察と提言を提供する。
    </summary>
    Automated decision-making systems are increasingly deployed in domains such as hiring and credit approval where negative outcomes can have substantial ramifications for decision subjects. Thus, recent research has focused on providing explanations that help decision subjects understand the decision system and enable them to take actionable recourse to change their outcome. Popular counterfactual explanation techniques aim to achieve this by describing alterations to an instance that would transform a negative outcome to a positive one. Unfortunately, little user evaluation has been performed to assess which of the many counterfactual approaches best achieve this goal. In this work, we conduct a crowd-sourced between-subjects user study (N = 252) to examine the effects of counterfactual explanation type and presentation on lay decision subjects’ understandings of automated decision systems. We find that the region-based counterfactual type significantly increases objective understanding, subjective understanding, and response confidence as compared to the point-based type. We also find that counterfactual presentation significantly effects response time and moderates the effect of counterfactual type for response confidence, but not understanding. A qualitative analysis reveals how decision subjects interact with different explanation configurations and highlights unmet needs for explanation justification. Our results provide valuable insights and recommendations for the development of counterfactual explanation techniques towards achieving practical actionable recourse and empowering lay users to seek justice and opportunity in automated decision workflows.
</details>

#### [CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale (Ayan Majumdar, Isabel Valera)](https://dl.acm.org/doi/10.1145/3630106.3659003)
<details>
    <summary>
        例えば、融資、雇用、教育において即座に決定を下すオンライン・プラットフォームなど、大規模な意思決定プロセスを自動化するためにアルゴリズムが使われるようになってきている。 このような自動化されたシステムによって不利な決定が下された場合、影響を受けた個人がそれを覆すことができるような勧告を、即座に否定的な決定に付随させることによって、救済を可能にすることが不可欠である。 しかし、大規模な環境においてアルゴリズムによる救済を提供することの実際的な課題は無視できない。行動に移せる救済勧告を与えるには、応募者の特徴間の関係に関する因果的な知識だけでなく、却下された応募者ごとに複雑な組み合わせ最適化問題を解く必要がある。 本研究では、因果関係のある再コース推薦を大規模に生成するための新しいフレームワークであるCARMAを紹介する。 CARMAは、因果情報が限られた実用的な設定において、事前に訓練された最新の因果生成モデルを活用し、再コース推薦を見つける。 さらに重要なことに、CARMAは、複雑なリコース最適化問題を予測タスクとしてキャストすることで、これらの推奨を見つけるスケーラビリティに対処している。 新しいニューラルネットワークベースのフレームワークをトレーニングすることで、CARMAは最適なリコースアクションの監視を必要とせずに予測タスクを効率的に解く。 我々の広範な評価により、学習後にCARMA上で推論を実行すると、因果的なリコースが確実に償却され、最適なリコメンデーションが瞬時に生成されることが示された。 CARMAは柔軟性があり、アルゴリズムによる意思決定や事前に訓練された因果生成モデルに対して、微分可能性が確保されている限り、その最適化は汎用的である。 さらに、CARMAをケーススタディで紹介し、難易度や所要時間などの要因に基づく母集団レベルの特徴嗜好を容易に取り入れることで、因果的な再コース推奨を調整する能力を説明する。
    </summary>
    Algorithms are increasingly used to automate large-scale decision-making processes, e.g., online platforms that make instant decisions in lending, hiring, and education. When such automated systems yield unfavorable decisions, it is imperative to allow for recourse by accompanying the instantaneous negative decisions with recommendations that can help affected individuals to overturn them. However, the practical challenges of providing algorithmic recourse in large-scale settings are not negligible: giving recourse recommendations that are actionable requires not only causal knowledge of the relationships between applicant features but also solving a complex combinatorial optimization problem for each rejected applicant. In this work, we introduce CARMA, a novel framework to generate causal recourse recommendations at scale. For practical settings with limited causal information, CARMA leverages pre-trained state-of-the-art causal generative models to find recourse recommendations. More importantly, CARMA addresses the scalability of finding these recommendations by casting the complex recourse optimization problem as a prediction task. By training a novel neural-network-based framework, CARMA efficiently solves the prediction task without requiring supervision for optimal recourse actions. Our extensive evaluations show that post-training, running inference on CARMA reliably amortizes causal recourse, generating optimal and instantaneous recommendations. CARMA exhibits flexibility, as its optimization is versatile with respect to the algorithmic decision-making and pre-trained causal generative models, provided their differentiability is ensured. Furthermore, we showcase CARMA in a case study, illustrating its ability to tailor causal recourse recommendations by readily incorporating population-level feature preferences based on factors such as difficulty or time needed.
</details>

#### [MiMICRI: Towards Domain-centered Counterfactual Explanations of Cardiovascular Image Classification Models (Grace Guo, Lifu Deng, Animesh Tandon, Alex Endert, Bum Chul Kwon)](https://dl.acm.org/doi/10.1145/3630106.3659011)
<details>
    <summary>
        近年、一般に公開された大規模な医用画像データセットの普及により、心血管画像の分類と解析のための人工知能（AI）モデルが急増している。 同時に、これらのモデルが重大な影響を及ぼす可能性があることから、特定の画像入力が与えられた場合にモデルの予測を説明することを目的とした、さまざまな説明可能なAI（XAI）手法の開発が進められている。 しかし、これらの手法の多くは、領域専門家と共に開発・評価されたものではなく、また、説明は、医療専門家や領域知識の観点から文脈化されていない。 本論文では、心血管画像分類モデルのドメイン中心の反事実的説明を提供する、新しいフレームワークとPythonライブラリMiMICRIを提案する。 MiMICRIは、ユーザが対話的に、形態学的構造に対応する医用画像のセグメントを選択し、置き換えることを支援する。 生成された反実仮想から、ユーザーは各セグメントがモデルの予測に与える影響を評価し、既知の医学的事実に対してモデルを検証することができる。 我々はこのライブラリを2人の医療専門家を用いて評価した。 我々の評価は、領域中心のXAIアプローチがモデル説明の解釈可能性を高め、専門家が関連する領域知識の観点からモデルについて推論するのを助けることができることを示している。 しかしながら、生成された反事実の臨床的妥当性についての懸念も浮上した。 最後に、MiMICRIフレームワークの一般化可能性と信頼性、およびヘルスケア文脈におけるモデル解釈可能性のためのドメイン中心XAI手法の開発に対する我々の知見の意味について議論する。
    </summary>
    The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.
</details>



## 2023
### [NeurIPS2023](https://dblp.org/db/conf/nips/neurips2023.html)
#### [Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models (Chirag Raman, Alec Nonnemaker, Amelia Villegas-Morcillo, Hayley Hung, Marco Loog)](https://papers.nips.cc/paper_files/paper/2023/hash/694ec0018b9fd0ebe863ec29fa5a89b9-Abstract-Conference.html)
<details>
    <summary>
        確率的多変量時系列予測（回帰）設定における反事実推論のための、その場限りの顕著性に基づく説明の枠組みを提案する。 複数の社会科学分野の研究から導き出されたMillerの説明の枠組みを基に、反事実推論と顕著性に基づく説明手法の間の概念的なつながりを確立する。 顕著性の原則的な概念の欠如に対処するため、人間の前注意的視覚認知に基づく情報理論的顕著性の統一的定義を活用し、それを予測設定に拡張する。 具体的には、確率的予測を行う際に、どの観測されたタイムステップが基礎モデルにとって顕著に見えるかを特定するために、一般的に使用される密度関数の閉形式を得る。 実世界のデータでは得られない真実の顕著性を確立するために、合成データを用いて原理的な方法で我々のフレームワークを実証的に検証する。 最後に、実世界のデータと予測モデルを用いて、本フレームワークが、野生の特徴間の因果関係に関する新しいデータ駆動型の仮説を形成する際に、領域の専門家をどのように支援できるかを示す。
    </summary>
    We propose a post hoc saliency-based explanation framework for counterfactual reasoning in probabilistic multivariate time-series forecasting (regression) settings. Building upon Miller's framework of explanations derived from research in multiple social science disciplines, we establish a conceptual link between counterfactual reasoning and saliency-based explanation techniques. To address the lack of a principled notion of saliency, we leverage a unifying definition of information-theoretic saliency grounded in preattentive human visual cognition and extend it to forecasting settings. Specifically, we obtain a closed-form expression for commonly used density functions to identify which observed timesteps appear salient to an underlying model in making its probabilistic forecasts. We empirically validate our framework in a principled manner using synthetic data to establish ground-truth saliency that is unavailable for real-world data. Finally, using real-world data and forecasting models, we demonstrate how our framework can assist domain experts in forming new data-driven hypotheses about the causal relationships between features in the wild.
</details>

#### [COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs (Tiep Le, VASUDEV LAL, Phillip Howard)](https://papers.nips.cc/paper_files/paper/2023/hash/e14e4cb8266184ceb234973dfe07faed-Abstract-Datasets_and_Benchmarks.html)
<details>
    <summary>
        反事実例は、自然言語処理（NLP）の分野において、データセット中の偽の相関に対する言語モデルの評価と頑健性の向上の両方に有用であることが証明されている。 自然言語処理においてその有用性が実証されているにもかかわらず、マルチモーダルな反事実例は、反事実の変化を最小限に抑えた画像とテキストのペアデータを作成することが困難であるため、比較的未開拓であった。 この課題を解決するために、我々はテキスト-画像拡散モデルを用いて反事実例を自動生成するスケーラブルなフレームワークを紹介する。 COCO-Counterfactualsは、MS-COCOデータセットに基づく、画像とテキストキャプションの対からなるマルチモーダルな反事実データセットである。 COCO-Counterfactualsの品質を人間による評価によって検証し、既存のマルチモーダルモデルが、我々の反事実画像とテキストのペアによって挑戦されることを示す。 さらに、COCO-Counterfactualsが、訓練データの増強を通じて、マルチモーダル視覚言語モデルの領域外汎化を改善するのに有用であることを実証する。 我々のコードとCOCO-Counterfactualsデータセットを公開する。
    </summary>
    Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code and the COCO-Counterfactuals dataset publicly available.
</details>


### [ICML2023](https://dblp.org/db/conf/icml/icml2023.html)
#### [On the Impact of Algorithmic Recourse on Social Segregation (Ruijiang Gao, Himabindu Lakkaraju)](https://proceedings.mlr.press/v202/gao23d.html)
<details>
    <summary>
        予測モデルが実世界の様々な用途に浸透するにつれ、こうしたモデルの結果によって悪影響を受けた個人が、救済の手段を確実に得られるようにすることが重要になってきている。 このため、近年、アルゴリズムによる救済に関する研究が活発化している。 救済措置は、影響を受ける個人にとっては非常に有益なものですが、大規模に実施されると、データ分配のシフトやその他の意図しない結果を招く可能性があります。 しかし、アルゴリズムによるリコースの実施後の影響を理解する研究はほとんどありません。 本研究では、アルゴリズムによるリコースの社会的影響の遅れを分析する最初の試みの1つを行うことで、前述のギャップに対処する。 そのために、最先端のアルゴリズムが出力するリコースを理論的かつ実証的に分析する。 我々の分析は、エンドユーザーによるリソースの大規模な実装が、社会的分離を悪化させる可能性があることを示す。 この問題に対処するために、我々は、分離の可能性を最小化するだけでなく、現実的なリソースを提供するために、暗黙的及び明示的な条件付き生成モデルを活用する新しいアルゴリズムを提案する。 実世界のデータセットを用いた広範な実験により、提案するアプローチの有効性を実証する。
    </summary>
    As predictive models seep into several real-world applications, it has become critical to ensure that individuals who are negatively impacted by the outcomes of these models are provided with a means for recourse. To this end, there has been a growing body of research on algorithmic recourse in recent years. While recourses can be extremely beneficial to affected individuals, their implementation at a large scale can lead to potential data distribution shifts and other unintended consequences. However, there is little to no research on understanding the impact of algorithmic recourse after implementation. In this work, we address the aforementioned gaps by making one of the first attempts at analyzing the delayed societal impact of algorithmic recourse. To this end, we theoretically and empirically analyze the recourses output by state-of-the-art algorithms. Our analysis demonstrates that large-scale implementation of recourses by end users may exacerbate social segregation. To address this problem, we propose novel algorithms which leverage implicit and explicit conditional generative models to not only minimize the chance of segregation but also provide realistic recourses. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed approaches.
</details>

#### [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees (Faisal Hamman, Erfaun Noorani, Saumitra Mishra, Daniele Magazzeni, Sanghamitra Dutta)](https://proceedings.mlr.press/v202/hamman23a.html)
<details>
    <summary>
        モデルが更新されたり、わずかでも変更されたりしても有効であり続けるような、ロバストな反事実説明を生成することに新たな関心が集まっています。 ロバストな反事実を見つけるために、既存の文献では、元のモデル m と新しいモデル M がパラメータ空間で境界がある、すなわち、$Params(M)−Params(m) \leq \Delta$ を仮定しています。 しかし、モデルは、与えられたデータセット上での予測や精度がほとんど変化しないまま、パラメータ空間内で大きく変化することがよくある。 本研究では、自然発生的モデル変化と呼ばれる数学的抽象化を導入し、データ多様体上にある点の予測値の変化が制限されるような、パラメータ空間における任意の変化を可能にする。 次に、ニューラルネットワークのような微分可能なモデルに対して、モデル変化に対する反事実の頑健性を定量化するための、安定性（Stability）と呼ぶ尺度を提案する。 我々の主な貢献は、我々の尺度によって定義されるStabilityの値が十分に高い反事実は、高い確率で潜在的な「自然発生的」モデル変更後も有効であることを示すことである（独立ガウシアンのリプシッツ関数の濃度境界を活用）。 我々の定量化は、常に利用可能とは限らないデータ点周りの局所リプシッツ定数に依存するため、我々の提案する尺度の実用的な緩和も検討し、それらをどのように組み込めば、近い、現実的で、潜在的なモデル変更後も有効なニューラルネットワークのロバストな反事実を見つけることができるかを実験的に実証する。
    </summary>
    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model m and the new model M are bounded in the parameter space, i.e., $Params(M)−Params(m) \leq \Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed naturally-occurring model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure – that we call Stability – to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counterfactuals with sufficiently high value of Stability as defined by our measure will remain valid after potential “naturally-occurring” model changes with high probability (leveraging concentration bounds for Lipschitz function of independent Gaussians). Since our quantification depends on the local Lipschitz constant around a data point which is not always available, we also examine practical relaxations of our proposed measure and demonstrate experimentally how they can be incorporated to find robust counterfactuals for neural networks that are close, realistic, and remain valid after potential model changes.
</details>

#### [GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations (Dan Ley, Saumitra Mishra, Daniele Magazzeni)](https://proceedings.mlr.press/v202/ley23a.html)
<details>
    <summary>
        反実仮想的説明は、説明可能性において広く研究されており、公平性、再帰性、モデル理解において顕著な応用依存的な手法がある。 しかし、これらの手法の大きな欠点は、ローカルレベルやインスタンスレベルを超えた説明ができないことである。 多くの研究がグローバルな説明の概念に触れているが、一般的には、グローバルな特性を確認することを期待して、大量のローカルな説明を集約することを提案している。 一方、実務家はより効率的でインタラクティブな説明可能性ツールを求めている。 我々はこの機会に、特に高次元のデータセットや連続的特徴の存在下で、現在の最新技術に関連する信頼性とスケーラビリティの問題に取り組む柔軟なフレームワーク、Global and Efficient Counterfactual Explanations (GLOBE-CE)を提案する。 さらに、カテゴリ特徴変換のユニークな数学的分析を提供し、我々の手法に利用する。 GLOBE-CEは、一般に公開されているデータセットとユーザスタディを用いた実験評価により、複数の評価指標（速度、信頼性など）において、現在の最先端技術を大幅に上回る性能を示すことが実証された。
    </summary>
    Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global and Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathematical analysis of categorical feature translations, utilising it in our method. Experimental evaluation with publicly available datasets and user studies demonstrate that GLOBE-CE performs significantly better than the current state-of-the-art across multiple metrics (e.g., speed, reliability).
</details>

#### [Self-Interpretable Time Series Prediction with Counterfactual Explanations (Jingquan Yan, Hao Wang)](https://proceedings.mlr.press/v202/yan23d.html)
<details>
    <summary>
        解釈可能な時系列予測は、ヘルスケアや自律走行などのセーフティクリティカルな分野において極めて重要である。 既存の手法のほとんどは、時系列のセグメントに重要なスコアを割り当てることによって予測を解釈することに焦点を当てている。 本論文では、異なる、より挑戦的なルートを取り、時系列予測に対する反実仮想的で実行可能な説明を生成する、Counterfactual Time Series（CounTS）と呼ばれる、自己解釈可能なモデルの開発を目指す。 具体的には、時系列の反事実的説明の問題を形式化し、関連する評価プロトコルを確立し、時系列のアブダクション、アクション、予測の反事実的推論能力を備えた変分ベイズ深層学習モデルを提案する。 最先端のベースラインと比較して、我々の自己解釈可能なモデルは、同等の予測精度を維持しながら、より優れた反事実的説明を生成することができる。
    </summary>
    Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
</details>


### [IJCAI2023](https://dblp.org/db/conf/ijcai/ijcai2023.html)
#### [Incentivizing Recourse through Auditing in Strategic Classification (Andrew Estornell, Yatong Chen, Sanmay Das, Yang Liu, Yevgeniy Vorobeychik)]()
<details>
    <summary>
        個人の生活や幸福に直接影響を与えるような重大な意思決定の自動化が進むにつれ、多くの重要な検討事項が提起される。 その中でも特に重要なのは、より望ましい結果を得ようとする個人の戦略的行動である。 このような行動には2つの形態があり、1）個人属性の虚偽申告、2）リコース、つまりそのような属性を真に変える行動、がよく研究される。 前者は欺瞞を伴い、本質的に望ましくないが、後者は真の個人の資質を変える限りにおいて望ましい目標である可能性がある。 我々は、個人の戦略的選択として、虚偽報告と遡及を統一的な枠組みで研究する。 特に、属性操作よりも遡及行動にインセンティブを与える手段として監査を提案し、効用最大化型と遡及最大化型の2種類のプリンシパルに対する最適な監査政策を特徴付ける。 さらに、操作に対するリコースのインセンティブとしての補助金について考察し、効用最大化型のプリンシパルでさえ、そのような補助金を提供するためにかなりの監査予算を割くことを厭わないであろうことを示す。 最後に、失敗した監査に対する罰金を最適化する問題を検討し、監査の結果として集団が被る総費用を制約する。
    </summary>
    The increasing automation of high-stakes decisions with direct impact on the lives and well-being of individuals raises a number of important considerations. Prominent among these is strategic behavior by individuals hoping to achieve a more desirable outcome. Two forms of such behavior are commonly studied: 1) misreporting of individual attributes, and 2) recourse, or actions that truly change such attributes. The former involves deception, and is inherently undesirable, whereas the latter may well be a desirable goal insofar as it changes true individual qualification. We study misreporting and recourse as strategic choices by individuals within a unified framework. In particular, we propose auditing as a means to incentivize recourse actions over attribute manipulation, and characterize optimal audit policies for two types of principals, utility-maximizing and recourse-maximizing. Additionally, we consider subsidies as an incentive for recourse over manipulation, and show that even a utility-maximizing principal would be willing to devote a considerable amount of audit budget to providing such subsidies. Finally, we consider the problem of optimizing fines for failed audits, and bound the total cost incurred by the population as a result of audits.
</details>


### [KDD2023](https://dblp.org/db/conf/kdd/kdd2023.html)
#### [CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations (Hangzhi Guo, Thanh H. Nguyen, Amulya Yadav)](https://dl.acm.org/doi/10.1145/3580305.3599290)
<details>
    <summary>
        本研究では、機械学習（ML）モデルの学習と、それに対応する反事実（CF）説明の生成を、単一のエンドツーエンドパイプラインに統合した、新しいエンドツーエンド学習フレームワークであるCounterNetを紹介する。 反実仮想的説明は対照的なケースを提供する、すなわち、そのインスタンスに関するMLモデルの予測を予め定義された出力に変更する、あるインスタンスの特徴値に対する最小の修正を見つけようとするものである。 CF説明を生成する先行技術は、2つの主要な制限に苦しんでいる：(i)それらの全ては、独自のMLモデルで使用するために設計されたポストホックメソッドである---その結果、CF説明を生成するためのそれらの手順は、モデルの予測と説明の間の不整合をもたらすMLモデルの訓練によって知らされていない、(ii)それらのほとんどは、各入力データ点に対するCF説明を見つけるために、別々の時間集約的な最適化問題を解くことに依存している（これは、それらの実行時間に悪影響を与える）。 本研究では、予測モデルの学習と反実仮想（CF）説明の生成を単一のパイプラインに統合するエンドツーエンドの学習フレームワークであるCounterNetを提示することで、（CF説明を生成する）一般的なポストホックパラダイムからの斬新な出発を行う。 ポストホックメソッドとは異なり、CounterNetはCF説明生成の最適化を予測モデルと共に一度だけ可能にする。 我々は、CounterNetのネットワークを効果的に訓練するのに役立つブロック単位の座標降下手順を採用する。 複数の実世界データセットに対する我々の広範な実験により、CounterNetは高品質の予測を生成し、どのような新しい入力インスタンスに対しても一貫して100%のCF妥当性と低い近接スコアを達成し（それによりコストと妥当性のトレードオフをバランスよく達成する）、既存の最先端ベースラインよりも3倍高速に動作することが示された。
    </summary>
    This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. Unlike post-hoc methods, CounterNet enables the optimization of the CF explanation generation only once together with the predictive model. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100% CF validity and low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.
</details>

#### [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations (Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin V. Bonilla, Gholamreza Haffari, Dinh Phung)](https://dl.acm.org/doi/10.1145/3580305.3599343)
<details>
    <summary>
        解釈可能な機械学習は、説明不可能なことで長い間悪名高い複雑なブラックボックスシステムの推論プロセスを理解しようとするものである。 このアプローチは、ユーザーが結果を変えるために何ができるかを提案する、反実仮想的な説明によるものである。 反事実的な例は、ブラックボックス分類器からのオリジナルの予測に対抗するものでなければならないだけでなく、実用的なアプリケーションのための様々な制約を満たすものでなければなりません。 多様性は重要な制約の1つであるが、あまり議論されていない。 多様な反事実は理想的であるが、他のいくつかの制約に同時に対処することは計算上困難である。 さらに、公開された反事実データに対するプライバシーの懸念が高まっている。 このため、我々は、反事実制約を効果的に扱い、限られた私的説明モデルのプールに貢献する、特徴ベースの学習フレームワークを提案する。 我々は、行動可能性と妥当性の多様な反事実を生成する際に、本手法の柔軟性と有効性を実証する。 我々の反実仮想エンジンは、同容量のものよりも効率的であると同時に、最も低い再識別リスクをもたらす。
    </summary>
    Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.
</details>


### [ICLR2023](https://dblp.org/db/conf/iclr/iclr2023.html)
#### [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse (Martin Pawelczyk, Teresa Datta, Johan Van den Heuvel, Gjergji Kasneci, Himabindu Lakkaraju)](https://openreview.net/forum?id=sC-PmTsiTB)
<details>
    <summary>
        機械学習モデルが実世界の環境において結果的な意思決定を行うために採用されることが多くなっているため、これらのモデルの予測によって不利な影響を受けた（例えば、融資が拒否された）個人が、救済の手段を確実に提供されることが重要になっている。 影響を受ける個人のための救済手段を構築するために、いくつかのアプローチが提案されているが、これらの方法によって出力される救済手段は、低コスト（すなわち、実装の容易さ）を達成するか、小さな摂動（すなわち、救済手段のノイズの多い実装）に対して頑健であるかのいずれかであり、救済コストと頑健性の間の本質的なトレードオフのため、両方ではない。 さらに、先行するアプローチは、前述のトレードオフをナビゲートするためのエージェンシーをエンドユーザーに提供していない。 本研究では、ユーザがリコースコスト対ロバストネスのトレードオフを効果的に管理できるようにする初のアルゴリズムフレームワークを提案することにより、上記の課題に取り組む。 より具体的には、我々のフレームワークProbabilistically ROBust rEcourse (PROBE)は、リコースに小さな変更が加えられた場合、すなわちリコースが多少ノイズを含んで実装された場合に、リコースが無効になる確率（リコース無効率）をユーザに選択させる。 この目的のために、我々は、達成された（結果の）リコース無効率と望ましいリコース無効率との間のギャップを同時に最小化し、リコースコストを最小化し、さらに、結果のリコースが正のモデル予測を達成することを保証する、新しい目的関数を提案する。 我々は、異なるクラスの基礎モデル（線形モデル、木ベースモデルなど）に対する、任意のインスタンスに対応する再コース無効率を特徴付ける新しい理論結果を開発し、提案する目的を効率的に最適化するためにこれらの結果を活用する。 複数の実世界データセットを用いた実験評価により、提案フレームワークの有効性を実証する。
    </summary>
    As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcourse (PROBE) lets users choose the probability with which a recourse could get invalidated (recourse invalidation rate) if small changes are made to the recourse i.e., the recourse is implemented somewhat noisily. To this end, we propose a novel objective function which simultaneously minimizes the gap between the achieved (resulting) and desired recourse invalidation rates, minimizes recourse costs, and also ensures that the resulting recourse achieves a positive model prediction. We develop novel theoretical results to characterize the recourse invalidation rates corresponding to any given instance w.r.t. different classes of underlying models (e.g., linear models, tree based models etc.), and leverage these results to efficiently optimize the proposed objective. Experimental evaluation with multiple real world datasets demonstrates the efficacy of the proposed framework.
</details>

#### [Distributionally Robust Recourse Action (Duy Nguyen, Ngoc Bui, Viet Anh Nguyen)](https://openreview.net/forum?id=E3ip6qBLF7)
<details>
    <summary>
        リコース・アクションは、インスタンスを修正して別の結果を得るための具体的な方法を示すことで、特定のアルゴリズム決定を説明することを目的としている。 既存のリコース生成手法は、機械学習モデルが時間の経過とともに変化しないことを前提としていることが多い。 しかし、この仮定はデータ分布の変化により、実際には必ずしも成立せず、この場合、再コースアクションは無効となる可能性がある。 この欠点を改善するために、我々は分布にロバストな再コースアクション（DiRRAc）フレームワークを提案する。 我々はまず、ロバスト化された再コース設定を最小-最大最適化問題として定式化し、最大問題はモデルパラメータ分布の曖昧さ集合上のゲルブリッヒ距離によって指定される。 次に、最小-最大目的に従ってロバストリコースを求める投影勾配降下アルゴリズムを提案する。 また、我々のDiRRAcフレームワークが、混合重みの誤指定に対するヘッジに拡張可能であることを示す。 合成データセットと3つの実世界データセットの両方を用いた数値実験により、ロバストなリコースを生成する最先端のリコース手法に対する我々の提案するフレームワークの利点を実証する。
    </summary>
    A recourse action aims to explain a particular algorithmic decision by showing one specific way in which the instance could be modified to receive an alternate outcome. Existing recourse generation methods often assume that the machine learning model does not change over time. However, this assumption does not always hold in practice because of data distribution shifts, and in this case, the recourse action may become invalid. To redress this shortcoming, we propose the Distributionally Robust Recourse Action (DiRRAc) framework, which generates a recourse action that has high probability of being valid under a mixture of model shifts. We first formulate the robustified recourse setup as a min-max optimization problem, where the max problem is specified by Gelbrich distance over an ambiguity set around the distribution of model parameters. Then we suggest a projected gradient descent algorithm to find a robust recourse according to the min-max objective. We also show that our DiRRAc framework can be extended to hedge against the misspecification of the mixture weights. Numerical experiments with both synthetic and three real-world datasets demonstrate the benefits of our proposed framework over the state-of-the-art recourse methods, which generate robust recourses.
</details>

#### [On the Trade-Off between Actionable Explanations and the Right to be Forgotten (Martin Pawelczyk, Tobias Leemann, Asia Biega, Gjergji Kasneci)](https://openreview.net/forum?id=HWt4BBZjVW)
<details>
    <summary>
        機械学習（ML）モデルがますます重要なアプリケーションに導入されるようになり、政策立案者たちはデータ保護規制の強化を提案している（GDPR、CCPAなど）。 重要な原則のひとつは「忘れられる権利」であり、ユーザーに自分のデータを削除してもらう権利を与える。 もう1つの重要な原則は、ユーザーが不利な決定を覆すことを可能にする、アルゴリズムによる救済としても知られる「行動可能な説明を受ける権利」である。 現在までのところ、これら2つの原則を同時に運用できるかどうかは不明である。 そこで我々は、データ削除要求の文脈におけるリコースの無効化問題を導入し、研究する。 より具体的には、一般的な最新アルゴリズムの挙動を理論的・経験的に解析し、少数のデータ削除要求（例えば1～2件）が予測モデルの更新を正当化する場合、これらのアルゴリズムによって生成されたリコースが無効になる可能性が高いことを示す。 微分可能なモデルの設定のために、我々は、削除された場合に、無効化されたリソースの割合を最大化する、重要な訓練ポイントの最小のサブセットを特定するフレームワークを提案する。我々のフレームワークを用いて、訓練セットからわずか2個のデータインスタンスを削除するだけで、一般的な最先端のアルゴリズムが出力する全てのリソースの最大95％を無効化できることを実証的に示す。 このように、我々の研究は、「忘れられる権利」の文脈における「行動可能な説明を受ける権利」の互換性に関する基本的な問題を提起し、同時に、リコースの頑健性の決定要因に関する建設的な洞察を提供する。
    </summary>
    As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the “right to be forgotten” which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms. Thus, our work raises fundamental questions about the compatibility of ``the right to an actionable explanation'' in the context of the ``right to be forgotten'', while also providing constructive insights on the determining factors of recourse robustness.
</details>


### [AISTATS2023](https://dblp.org/db/conf/aistats/aistats2023.html)
#### [Feasible Recourse Plan via Diverse Interpolation (Duy Nguyen, Ngoc Bui, Viet Anh Nguyen)](https://proceedings.mlr.press/v206/nguyen23b.html)
<details>
    <summary>
        アルゴリズムの決定を説明し、実用的なフィードバックを推奨することは、機械学習アプリケーションにとってますます重要になっている。 近年、ユーザの幅広い嗜好をカバーするために、多様なリソースを見つけることに多大な努力が払われている。 しかし、既存の研究では、リソースがデータ多様体に近いという要件がしばしば無視されている。それゆえ、構築されたリソースは、ユーザにとってあり得ない、満足のいくものではない可能性がある。 このような問題に対処するため、我々は、多様な行動可能リソース集合をデータ多様体に明示的に向ける新しいアプローチを提案する。 我々はまず、多様性と近接性のトレードオフのバランスをとる、好ましいクラスの多様なプロトタイプ集合を見つける。 これらのプロトタイプを見つける2つの具体的な方法を示す：行列式点過程の最大事後推定値を求める方法と、2次二元プログラムを解く方法である。 行動可能性制約を保証するために、我々は行動可能性グラフを構築し、ノードは訓練サンプルを表し、エッジは2つのインスタンス間の実行可能な行動を示す。 そして、各プロトタイプへの実現可能な経路を見つけ、この経路はプラン内の各リコースに対する実現可能なアクションを示す。 実験結果は、我々の手法が、既存のアプローチよりも優れたコストと多様性のトレードオフを実現しつつ、データ多様体に近いリコースの集合を生成することを示している。
    </summary>
    Explaining algorithmic decisions and recommending actionable feedback is increasingly important for machine learning applications. Recently, significant efforts have been invested in finding a diverse set of recourses to cover the wide spectrum of users’ preferences. However, existing works often neglect the requirement that the recourses should be close to the data manifold; hence, the constructed recourses might be implausible and unsatisfying to users. To address these issues, we propose a novel approach that explicitly directs the diverse set of actionable recourses towards the data manifold. We first find a diverse set of prototypes in the favorable class that balances the trade-off between diversity and proximity. We demonstrate two specific methods to find these prototypes: either by finding the maximum a posteriori estimate of a determinantal point process or by solving a quadratic binary program. To ensure the actionability constraints, we construct an actionability graph in which the nodes represent the training samples and the edges indicate the feasible action between two instances. We then find a feasible path to each prototype, and this path demonstrates the feasible actions for each recourse in the plan. The experimental results show that our method produces a set of recourses that are close to the data manifold while delivering a better cost-diversity trade-off than existing approaches.
</details>

#### [On the Privacy Risks of Algorithmic Recourse (Martin Pawelczyk, Himabindu Lakkaraju, Seth Neel )](https://proceedings.mlr.press/v206/pawelczyk23a.html)
<details>
    <summary>
        結果的な意思決定を行うために予測モデルがますます採用されるようになるにつれて、影響を受ける個人に対してアルゴリズムによる救済手段を提供できる技術の開発が重視されるようになってきている。 このような手段は、影響を受ける個人にとって非常に有益である一方で、潜在的な敵対者がこのような手段を悪用してプライバシーを侵害する可能性もある。 本研究では、敵対者がリソースを活用して、基礎となるモデルの学習データに関するプライベート情報を推論できるかどうか、またどのように推論できるかを調査する初めての試みを行う。 この目的のために、アルゴリズム的リコースを活用した一連の新しいメンバーシップ推論攻撃を提案する。 より具体的には、データインスタンスと、それに対応する最新の遡及手法が出力する反事実との間の距離を活用することで、メンバーシップ推論攻撃に関する先行文献を遡及設定に拡張する。 実世界のデータセットと合成データセットを用いた広範な実験により、リコースによるプライバシーの著しい漏洩が実証された。 我々の研究は、意図しないプライバシーの漏洩が、リコース手法の広範な採用における重要なリスクであることを立証している。
    </summary>
    As predictive models are increasingly being employed to make consequential decisions, there is a growing emphasis on developing techniques that can provide algorithmic recourse to affected individuals. While such recourses can be immensely beneficial to affected individuals, potential adversaries could also exploit these recourses to compromise privacy. In this work, we make the first attempt at investigating if and how an adversary can leverage recourses to infer private information about the underlying model’s training data. To this end, we propose a series of novel membership inference attacks which leverage algorithmic recourse. More specifically, we extend the prior literature on membership inference attacks to the recourse setting by leveraging the distances between data instances and their corresponding counterfactuals output by state-of-the-art recourse methods. Extensive experimentation with real world and synthetic datasets demonstrates significant privacy leakage through recourses. Our work establishes unintended privacy leakage as an important risk in the widespread adoption of recourse methods.
</details>


### [AAAI2023](https://dblp.org/db/conf/aaai/aaai2023.html)
#### [Improvement-Focused Causal Recourse (ICR) (König, G., Freiesleben, T., & Grosse-Wentrup, M.)](https://ojs.aaai.org/index.php/AAAI/article/view/26398)
<details>
    <summary>
        アルゴリズムによるリコースの推奨は、不利な決定を戻すためにどのように行動すべきかを利害関係者に知らせる。 しかし、既存の方法は、受け入れ（すなわち、モデルの決定を戻す）にはつながるが、改善にはつながらない（すなわち、根本的な実世界の状態を戻さない）行動を推奨することがある。 そのような行動を推奨することは、予測モデルを騙すことを推奨することになる。 我々は、概念的な転換を伴う新しい方法、ICR（Improvement-Focused Causal Recourse）を紹介する： 第一に、我々はICRの勧告を改善に向けて導くことを要求する。 第二に、我々は特定の予測因子によって受け入れられるように勧告を調整しない。 その代わりに、我々は原因知識を活用して、改善保証が受け入れ保証に変換されるように、コース前とコース後を正確に予測する決定システムを設計する。 不思議なことに、最適な再コース前の分類器は、ICR行動に対して頑健であり、したがって再コース後に適している。 半合成実験において、我々は、正しい因果関係の知識が与えられた場合、既存のアプローチとは対照的に、ICRが受容と改善の両方を導くことを実証する。
    </summary>
    Algorithmic recourse recommendations inform stakeholders of how to act to revert unfavorable decisions. However, existing methods may recommend actions that lead to acceptance (i.e., revert the model's decision) but do not lead to improvement (i.e., may not revert the underlying real-world state). To recommend such actions is to recommend fooling the predictor. We introduce a novel method, Improvement-Focused Causal Recourse (ICR), which involves a conceptual shift: Firstly, we require ICR recommendations to guide toward improvement. Secondly, we do not tailor the recommendations to be accepted by a specific predictor. Instead, we leverage causal knowledge to design decision systems that predict accurately pre- and post-recourse, such that improvement guarantees translate into acceptance guarantees. Curiously, optimal pre-recourse classifiers are robust to ICR actions and thus suitable post-recourse. In semi-synthetic experiments, we demonstrate that given correct causal knowledge ICR, in contrast to existing approaches, guides toward both acceptance and improvement.
</details>

#### [Explaining Model Confidence Using Counterfactuals (Le, T., Miller, T., Singh, R., & Sonenberg, L.)](https://ojs.aaai.org/index.php/AAAI/article/view/26399)
<details>
    <summary>
        人間とAIの対話において信頼スコアを表示することは、人間とAIシステム間の信頼構築に役立つことが示されている。 しかし、既存の研究のほとんどは、コミュニケーションの形として信頼度スコアのみを使用している。 信頼度スコアはモデル出力の一つに過ぎないため、ユーザーは信頼度スコアを受け入れるかどうかを判断するために、アルゴリズムがなぜ自信を持っているのかを理解したいと思うかもしれない。 本論文では、信頼スコアの反実仮想的説明が、研究参加者が機械学習モデルの予測をより良く理解し、より良く信頼するのに役立つことを示す。 (1)反実仮想例に基づく方法、(2)反実仮想空間の視覚化に基づく方法である。 両者とも、研究参加者の理解と信頼を、説明なしのベースラインよりも増加させるが、定性的な結果は、両者が全く異なる使われ方をすることを示しており、それぞれをどのような場合に使うべきか、また、より良い説明を設計するための方向性を提言するに至っている。
    </summary>
    Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model's prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanations.
</details>

#### [Formalising the Robustness of Counterfactual Explanations for Neural Networks (Jiang, J., Leofante, F., Rago, A., & Toni, F.)](https://ojs.aaai.org/index.php/AAAI/article/view/26740)
<details>
    <summary>
        反実仮想説明（CFX）の使用は、機械学習モデルの説明戦略としてますます一般的になってきている。 しかし、最近の研究では、これらの説明は、基礎となるモデルの変化（例えば、再学習後）に対して頑健でない可能性があることが示されており、実世界での応用における信頼性に疑問が投げかけられている。 この問題を解決するための既存の試みは発見的であり、結果として得られるCFXのモデル変更に対する頑健性は、少数の再トレーニングされたモデルのみで評価され、網羅的な保証を提供することができない。 この問題を解決するために、我々は、ニューラルネットワークのCFXのロバスト性（モデル変更に対するロバスト性）を形式的かつ決定論的に評価する最初の概念である∆-ロバスト性を提案する。 区間ニューラルネットワークに基づく抽象化フレームワークを導入し、モデルパラメータ、すなわち重みとバイアスの無限大の変更に対するCFXの∆ロバストネスを検証する。 次に、このアプローチの有用性を2つの異なる方法で実証する。 第一に、文献にある多くのCFX生成手法の∆ロバスト性を分析し、それらがこの点で重大な欠陥を抱えていることを示す。 次に、∆ロバスト性を既存の手法に組み込むことで、証明可能なロバスト性を持つCFXを提供できることを示す。
    </summary>
    The use of counterfactual explanations (CFXs) is an increasingly popular explanation strategy for machine learning models. However, recent studies have shown that these explanations may not be robust to changes in the underlying model (e.g., following retraining), which raises questions about their reliability in real-world applications. Existing attempts towards solving this problem are heuristic, and the robustness to model changes of the resulting CFXs is evaluated with only a small number of retrained models, failing to provide exhaustive guarantees. To remedy this, we propose ∆-robustness, the first notion to formally and deterministically assess the robustness (to model changes) of CFXs for neural networks. We introduce an abstraction framework based on interval neural networks to verify the ∆-robustness of CFXs against a possibly infinite set of changes to the model parameters, i.e., weights and biases. We then demonstrate the utility of this approach in two distinct ways. First, we analyse the ∆-robustness of a number of CFX generation methods from the literature and show that they unanimously host significant deficiencies in this regard. Second, we demonstrate how embedding ∆-robustness within existing methods can provide CFXs which are provably robust.
</details>

#### [Very Fast, Approximate Counterfactual Explanations for Decision Forests (Carreira-Perpinan, M. Á., & Hada, S. S. )](https://ojs.aaai.org/index.php/AAAI/article/view/25848)
<details>
    <summary>
        我々は、ランダム・フォレストのような分類フォレストや回帰フォレストの反事実説明を見つけることを考える。 このためには、森が望ましい値を出力する与えられたインスタンスに最も近い入力インスタンスを見つける最適化問題を解く必要がある。 厳密解を求めるには、森の葉の数に対して指数関数的なコストがかかる。 我々は単純だが非常に効果的なアプローチを提案する。それは、実際のデータ点が存在する入力空間領域に最適化を制約することである。 この問題は、あるデータセット上で、ある距離を用いた最近傍探索の形に帰着する。 これには2つの利点がある：第一に、解を非常に素早く見つけることができ、大規模な森や高次元データに対応し、対話的な利用が可能である。 第二に、入力空間の高密度領域に向かって導かれるため、発見された解が現実的である可能性が高い。
    </summary>
    We consider finding a counterfactual explanation for a classification or regression forest, such as a random forest. This requires solving an optimization problem to find the closest input instance to a given instance for which the forest outputs a desired value. Finding an exact solution has a cost that is exponential on the number of leaves in the forest. We propose a simple but very effective approach: we constrain the optimization to input space regions populated by actual data points. The problem reduces to a form of nearest-neighbor search using a certain distance on a certain dataset. This has two advantages: first, the solution can be found very quickly, scaling to large forests and high-dimensional data, and enabling interactive use. Second, the solution found is more likely to be realistic in that it is guided towards high-density areas of input space.
</details>


### [FAccT2023](https://dblp.org/db/conf/fat/facct2023.html)
#### [Robustness Implies Fairness in Causal Algorithmic Recourse (Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schoelkopf, Setareh Maghsudi)](https://dl.acm.org/doi/10.1145/3593013.3594057)
<details>
    <summary>
        アルゴリズムによる救済は、受益者により有利な結果をもたらすための推奨事項を提供することで、決定が重大な結果をもたらすブラックボックス化された決定プロセスの内部手続きを開示する。 効果的な救済を確実にするためには、提案される介入策は費用対効果が高いだけでなく、強固で公正でなければならない。 そのためには、同様の個人に対して同様の説明を行うことが不可欠である。 本研究では、因果的アルゴリズムによる遡及における個人の公平性と敵対的頑健性の概念を探求し、その両方を達成するという課題に取り組む。 課題を解決するために、敵対的に頑健なリコースを定義するための新しい枠組みを提案する。 この枠組みでは、保護される特徴を擬似計量として観測し、個人の公平性が敵対的ロバスト性の特別な場合であることを示す。 最後に、公平なロバスト遡及問題を導入し、理論的にも経験的にも望ましい性質を達成するための解を確立する。
    </summary>
    Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.
</details>

#### [Achieving Diversity in Counterfactual Explanations: a Review and Discussion (Thibault Laugel, Adulam Jeyasothy, Marie-Jeanne Lesot, Christophe Marsala, Marcin Detyniecki)](https://dl.acm.org/doi/10.1145/3593013.3594122)
<details>
    <summary>
        説明可能な人工知能（Explainable Artificial Intelligence：XAI）の分野では、学習された決定モデルの予測を、関連する予測を変更するためにインスタンスに加えられる修正を示すことによって、反実仮想例がユーザに説明する。 これらの反実仮想例は、一般に、コスト関数が、ユーザーのニーズを満たす良い説明のための望ましさを定量化するいくつかの基準を組み合わせた最適化問題の解として定義される。 ユーザーニーズは一般的に未知であり、ユーザーごとに異なるため、このような適切な特性の多種多様を考慮することができる。 この問題を回避するために、いくつかのアプローチは、予測を説明するために、単一のものではなく、多様な反事実例の集合を生成することを提案している。 本稿では、この多様性の概念について提案されてきた数多くの、時には相反する定義のレビューを提案する。 その根底にある原理と、それらが依拠するユーザーニーズに関する仮説について議論し、いくつかの次元（明示的か暗黙的か、定義される世界、適用されるレベル）に沿ってそれらを分類することを提案し、このトピックに関するさらなる研究課題の特定へと導く。
    </summary>
    In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.
</details>



## 2022
### [NeurIPS2022](https://dblp.org/db/conf/nips/neurips2022.html)
#### [Bayesian Persuasion for Algorithmic Recourse (Keegan Harris, Valerie Chen, Joon Kim, Ameet Talwalkar, Hoda Heidari, Steven Z. Wu)](https://papers.nips.cc/paper_files/paper/2022/hash/480150047ecb2187a3a8b8dccfd8f2de-Abstract-Conference.html)
<details>
    <summary>
        
    </summary>
    
</details>

#### [Learning Recourse on Instance Environment to Enhance Prediction Accuracy (Lokesh N, Guntakanti Sai Koushik, Abir De, Sunita Sarawagi)](https://papers.nips.cc/paper_files/paper/2022/hash/a399456a191ca36c7c78dff367887f0a-Abstract-Conference.html)
<details>
    <summary>
        
    </summary>
    
</details>

#### [CLEAR: Generative Counterfactual Explanations on Graphs (Jing Ma, Ruocheng Guo, Saumitra Mishra, Aidong Zhang, Jundong Li)](https://papers.nips.cc/paper_files/paper/2022/hash/a69d7f3a1340d55c720e572742439eaf-Abstract-Conference.html)
<details>
    <summary>
        
    </summary>
    
</details>

#### [Diffusion Visual Counterfactual Explanations (Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein)](https://papers.nips.cc/paper_files/paper/2022/hash/025f7165a452e7d0b57f1397fed3b0fd-Abstract-Conference.html)
<details>
    <summary>
        
    </summary>
    
</details>


### [ICML2022](https://dblp.org/db/conf/icml/icml2022.html)
### [IJCAI2022](https://dblp.org/db/conf/ijcai/ijcai2022.html)
### [KDD2022](https://dblp.org/db/conf/kdd/kdd2022.html)
### [ICLR2022](https://dblp.org/db/conf/iclr/iclr2022.html)
### [AISTATS2022](https://dblp.org/db/conf/aistats/aistats2022.html)
### [AAAI2022](https://dblp.org/db/conf/aaai/aaai2022.html)
### [FAccT2022](https://dblp.org/db/conf/fat/facct2022.html)



## 2021
### [NeurIPS2021](https://dblp.org/db/conf/nips/neurips2021.html)
### [ICML2021](https://dblp.org/db/conf/icml/icml2021.html)
### [IJCAI2021](https://dblp.org/db/conf/ijcai/ijcai2021.html)
### [KDD2021](https://dblp.org/db/conf/kdd/kdd2021.html)
### [ICLR2021](https://dblp.org/db/conf/iclr/iclr2021.html)
### [AISTATS2021](https://dblp.org/db/conf/aistats/aistats2021.html)
### [AAAI2021](https://dblp.org/db/conf/aaai/aaai2021.html)
### [FAccT2021](https://dblp.org/db/conf/fat/facct2021.html)


## 2020
### [NeurIPS2020](https://dblp.org/db/conf/nips/neurips2020.html)
### [ICML2020](https://dblp.org/db/conf/icml/icml2020.html)
### [IJCAI2020](https://dblp.org/db/conf/ijcai/ijcai2020.html)
### [KDD2020](https://dblp.org/db/conf/kdd/kdd2020.html)
### [ICLR2020](https://dblp.org/db/conf/iclr/iclr2020.html)
### [AISTATS2020](https://dblp.org/db/conf/aistats/aistats2020.html)
### [AAAI2020](https://dblp.org/db/conf/aaai/aaai2020.html)
### [FAccT2020](https://dblp.org/db/conf/fat/facct2020.html)


---
```html

#### [ ()]()
<details>
    <summary>
        
    </summary>
    
</details>


```