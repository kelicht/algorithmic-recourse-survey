# A Survey of Algorithmic Recourse (in Japanese)

**ルール**
- DBLPを「recourse」「counterfactual」「actionable」などで検索
- タイトルと著者とURLをコピペ
- アブストラクトとその和訳（DeepL）をコピペ


## 2024
### [NeurIPS2024 (to appear)]()

### [ICML2024](https://dblp.org/db/conf/icml/icml2024.html)
#### [Learning Decision Trees and Forests with Algorithmic Recourse (Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike)](https://openreview.net/forum?id=blGpu9aGs6)
<details>
    <summary>
        本稿では、リコースアクションの存在を保証しつつ、正確なツリーベースのモデルを学習するための新しいアルゴリズムを提案する。アルゴリズム的リコース（AR）は、モデルによって与えられた望ましくない予測結果を変更するためのリコースアクションを提供することを目的とする。典型的なAR手法は、実行可能なアクションの中で必要な労力を最小化する最適化タスクを解くことで、妥当なアクションを提供する。しかし実際には、予測性能のみに最適化されたモデルには、そのようなアクションは必ずしも存在しない。この問題を緩和するために、我々は、できるだけ多くのインスタンスに対して妥当なアクションの存在を保証するという制約の下で、正確な分類木を学習するというタスクを定式化する。そして、敵対的学習技術を活用した効率的なトップダウン貪欲アルゴリズムを提案する。また、提案するアルゴリズムが、樹木アンサンブルを学習するための一般的なフレームワークとして知られるランダムフォレストに適用可能であることを示す。実験の結果、我々の手法は、精度や計算効率を大幅に低下させることなく、ベースラインよりも多くのインスタンスに対して妥当な行動を提供することに成功した。
    </summary>
    This paper proposes a new algorithm for learning accurate tree-based models while ensuring the existence of recourse actions. Algorithmic Recourse (AR) aims to provide a recourse action for altering the undesired prediction result given by a model. Typical AR methods provide a reasonable action by solving an optimization task of minimizing the required effort among executable actions. In practice, however, such actions do not always exist for models optimized only for predictive performance. To alleviate this issue, we formulate the task of learning an accurate classification tree under the constraint of ensuring the existence of reasonable actions for as many instances as possible. Then, we propose an efficient top-down greedy algorithm by leveraging the adversarial training techniques. We also show that our proposed algorithm can be applied to the random forest, which is known as a popular framework for learning tree ensembles. Experimental results demonstrated that our method successfully provided reasonable actions to more instances than the baselines without significantly degrading accuracy and computational efficiency.
</details>

#### [Counterfactual Metarules for Local and Global Recourse (Tom Bewley, Salim I. Amoukou, Saumitra Mishra, Daniele Magazzeni, Manuela Veloso)](https://openreview.net/forum?id=Ad9msn1SKC)
<details>
    <summary>
        T-CRExは、局所的・大域的な反事実的説明（CE）のための、モデルに依存しない新しい手法であり、個人と集団の両方に対する救済の選択肢を一般化されたルールの形で要約する。T-CRExは、ツリーベースの代理モデルを利用して、反事実的ルールを学習し、その最適化領域を示すメタルールも学習する。実験によれば、T-CRExは既存のルールベースのベースラインと比較して、CEに求められる様々な要求を満たす優れた集計性能を達成し、同時に実行速度も桁違いに高速である。
    </summary>
    We introduce T-CREx, a novel model-agnostic method for local and global counterfactual explanation (CE), which summarises recourse options for both individuals and groups in the form of generalised rules. It leverages tree-based surrogate models to learn the counterfactual rules, alongside metarules denoting their regimes of optimality, providing both a global analysis of model behaviour and diverse recourse options for users. Experiments indicate that T-CREx achieves superior aggregate performance over existing rule-based baselines on a range of CE desiderata, while being orders of magnitude faster to run.
</details>

#### [CF-OPT: Counterfactual Explanations for Structured Prediction (Germain Vivier-Ardisson, Alexandre Forel, Axel Parmentier, Thibaut Vidal)](https://openreview.net/forum?id=xSkIxKdO08)
<details>
    <summary>
        ディープニューラルネットワークの最適化層は、構造化学習において人気が高まっており、様々なアプリケーションの技術水準を向上させている。しかし、これらのパイプラインは、ディープニューラルネットワークのような高度に非線形な予測モデルと、一般的に複雑なブラックボックスソルバーである最適化層という、2つの不透明な層で構成されているため、解釈可能性に欠けている。我々の目標は、反事実的な説明を提供することによって、このような手法の透明性を向上させることである。我々は、変分オートエンコーダをベースに、反事実を得るための原理的な方法を構築する。最後に、VAEトレーニングのための古典的な損失の変形を導入し、我々の特定の構造化された文脈における性能を向上させる。これらはCF-OPTの基礎となるものであり、構造化学習アーキテクチャの幅広いクラスに対して、反事実的説明を見つけることができる一次最適化アルゴリズムである。我々の数値結果は、最近の文献から得られた問題に対して、近い説明ともっともらしい説明の両方が得られることを示している。
    </summary>
    Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.
</details>

#### [Trustworthy Actionable Perturbations (Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon)](https://openreview.net/forum?id=zkjGpZrIX3)
<details>
    <summary>
        反事実、すなわち異なる結果を導く修正入力は、機械学習分類器が使用するロジックを理解し、望ましくない分類を変更する方法を理解するための重要なツールである。しかし、たとえ反事実が分類器の判断を変えたとしても、その反事実が真のクラス確率に影響を与えない可能性がある。つまり、反事実は敵対的な攻撃のように振る舞い、分類器を「愚弄」する可能性がある。我々は、Trustworthy Actionable Perturbations (TAP)と呼ぶ、真の基礎となる確率を有益に変化させる修正入力を作成するための新しいフレームワークを提案する。これには、TAPが敵対的に作用するのではなく、真のクラス確率を変更することを保証するための新しい検証手順が含まれる。また、我々のフレームワークには、現実世界において変化をもたらすのに適した、新しいコスト、報酬、目標の定義が含まれている。検証手順に関するPAC学習可能性の結果を示し、報酬を測定するための新しい方法を理論的に分析する。また、TAPを作成するための方法論を開発し、これまでの反事実的手法で達成された結果と比較する。
    </summary>
    Counterfactuals, or modified inputs that lead to a different outcome, are an important tool for understanding the logic used by machine learning classifiers and how to change an undesirable classification. Even if a counterfactual changes a classifier's decision, however, it may not affect the true underlying class probabilities, i.e. the counterfactual may act like an adversarial attack and ``fool'' the classifier. We propose a new framework for creating modified inputs that change the true underlying probabilities in a beneficial way which we call Trustworthy Actionable Perturbations (TAP). This includes a novel verification procedure to ensure that TAP change the true class probabilities instead of acting adversarially. Our framework also includes new cost, reward, and goal definitions that are better suited to effectuating change in the real world. We present PAC-learnability results for our verification procedure and theoretically analyze our new method for measuring reward. We also develop a methodology for creating TAP and compare our results to those achieved by previous counterfactual methods.
</details>

### [IJCAI2024](https://www.ijcai.org/proceedings/2024/)
#### [A New Paradigm for Counterfactual Reasoning in Fairness and Recourse (Lucius E.J. Bynum, Joshua R. Loftus, Julia Stoyanovich)](https://www.ijcai.org/proceedings/2024/784)
<details>
    <summary>
        反実仮想は、人工知能（AI）システムを監査し理解するための数多くの技術を支えている。 この文献における反事実推論の伝統的なパラダイムは介入的反事実であり、そこでは仮想的な介入が想像され、シミュレートされる。 このため、AIにおける法的保護と人口統計データに関する因果推論の出発点は、民族、人種、性別、障害、年齢など、法的に保護された特性に対する想像上の介入である。 例えば、あなたの人種が違っていたらどうなっていただろうかと問うのである。 このパラダイムの本質的な限界は、人種に関する介入のような人口統計学的介入がうまく定義されなかったり、介入的反事実の形式論に当てはまらなかったりすることである。 そこでは、法的に保護された特性に対する仮定の介入を想像するのではなく、これらの特性を固定したまま別の初期条件を想像するのである。 その代わりに、実際に自分がそうである、あるいはそうなりうるという反事実的な結果を説明するものは何か、と問うのである。 この別の枠組みは、同じ社会的懸念の多くに取り組むことを可能にするが、人口統計学的介入に依存しない根本的に異なる質問をしながらそうすることを可能にする。 
    </summary>
    Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions — like interventions on race — may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.
</details>

#### [Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective (Tomu Tominaga, Naomi Yamashita, Takeshi Kurashima)](https://www.ijcai.org/proceedings/2024/876)
<details>
    <summary>
        本研究では、アルゴリズム的リコース（AIシステムによって下された不利な決定を覆すために、個人を支援する反事実的行動計画（すなわちリコース）を生成するプロセス）の基礎的前提を批判的に検討する。 アルゴリズミック・リコースの根底にある仮定は、個人が現在の状態と望ましい状態との間のギャップを最小化するようなリコースを受け入れ、それに基づいて行動するというものである。 しかし、この仮定は経験的に検証されていない。 この問題に対処するため、我々は362人の参加者を対象にユーザー研究を実施し、現在と望ましい状態のギャップの指標である距離関数を最小化することが、本当に提案されたリカースを受け入れ、行動することを促すかどうかを評価した。 我々の発見は、微妙な情景を明らかにした：参加者のリコースの受け入れは、リコース距離と相関しなかった。 さらに、参加者のリコースに対する行動意欲は、リコース距離が最小のときにピークに達するが、それ以外は一定であった。 これらの知見は、アルゴリズムによるリコース研究の一般的な前提に疑問を投げかけ、人間中心のリコース生成への道を開くために評価機能を再考する必要性を示唆している。
    </summary>
    In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.
</details>

#### [CMACE: CMAES-based Counterfactual Explanations for Black-box Models (Xudong Yin, Yao Yang)](https://www.ijcai.org/proceedings/2024/60)
<details>
    <summary>
        説明型人工知能は機械学習において重要な役割を担っている。それは、例えば信用融資のような意思決定の場面で広く応用されているからである。 反実仮想説明（CFE）は、「もしも」、すなわちモデル入力がわずかに変化したらどうなったかを問う、新しいタイプの説明手法である。 この問いに答えるために、Counterfactual Explanationは、異なるモデル決定を導くモデル入力の最小摂動を見つけることを目的としている。 モデルにとらわれないアプローチと比較すると、特定のタイプのモデルに対してのみ設計されたモデル固有のCFEアプローチは、モデルの内部構造にアクセスできるため、最適な反事実的摂動を見つけることにおいて、通常、より良い性能を持つ。 このジレンマに対処するため、本研究ではまず、CMAES-based Counterfactual Explanations(CMACE)を提案する。CMACEは、共分散行列適応進化戦略(CMA-ES)と、学習サンプルの事前情報を利用してCMA-ESのための反事実の平均と共分散パラメータの良好な初期化を提供するウォームスタートスキームに基づく、効果的なモデル非依存的反事実生成アプローチである。 CMACEは、様々な実験設定において、他の最新（SOTA）のモデル不可知的アプローチ（Bayesian Counterfactual Generator, BayCon）を大幅に上回る性能を示した。 また、広範な実験により、CMACEは、勾配ベースの最適化を用いたツリーベースのモデル用に設計されたSOTAモデル固有のアプローチ（Flexible Optimizable Counterfactual Explanations for Tree Ensembles, FOCUS）よりも優れていることが実証された。
    </summary>
    Explanatory Artificial Intelligence plays a vital role in machine learning, due to its widespread application in decision-making scenarios, e.g., credit lending. Counterfactual Explanation (CFE) is a new kind of explanatory method that involves asking “what if ”, i.e. what would have happened if model inputs slightly change. To answer the question, Counterfactual Explanation aims at finding a minimum perturbation in model inputs leading to a different model decision. Compared with model-agnostic approaches, model-specific CFE approaches designed only for specific type of models usually have better performance in finding optimal counterfactual perturbations, owing to access to the inner workings of models. To deal with this dilemma, this work first proposes CMAES-based Counterfactual Explanations (CMACE): an effective model-agnostic counterfactual generating approach based on Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and a warm starting scheme that provides good initialization of the counterfactual's mean and covariance parameters for CMA-ES taking advantage of prior information of training samples. CMACE significantly outperforms another state-of-art (SOTA) model-agnostic approach (Bayesian Counterfactual Generator, BayCon) with various experimental settings. Extensive experiments also demonstrate that CMACE is superior to a SOTA model-specific approach (Flexible Optimizable Counterfactual Explanations for Tree Ensembles, FOCUS) that is designed for tree-based models using gradient-based optimization.
</details>

#### [Robust Counterfactual Explanations in Machine Learning: A Survey (Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni)](https://www.ijcai.org/proceedings/2024/894)
<details>
    <summary>
       反実仮想説明（CE）は、機械学習モデルの予測によって影響を受けた被験者にアルゴリズムによる救済を提供するのに理想的であると提唱されている。 CEは影響を受ける被験者にとって有益であるが、最近の研究では、CEを得るための最新の手法のロバスト性に関する深刻な問題が露呈している。 ロバスト性の欠如はCEの妥当性を損なう可能性があるため、このリスクを軽減する技術が求められている。 本サーベイでは、急成長しているロバストCEの分野の研究をレビューし、それらが考慮するロバスト性の形態について詳細な分析を行う。 また、既存の解決策とその限界についても議論し、今後の開発のための確かな基礎を提供する。 
    </summary>
    Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
</details>


### [KDD2024](https://dblp.org/db/conf/kdd/kdd2024.html)
#### [Unraveling Block Maxima Forecasting Models with Counterfactual Explanation (Yue Deng, Asadullah Hill Galib, Pang-Ning Tan, Lifeng Luo)](https://dl.acm.org/doi/10.1145/3637528.3671923)
<details>
    <summary>
        疾病監視、交通管理、気象予測は、時系列のブロック最大値予測から恩恵を受ける可能性のある重要なアプリケーションの一部である。 ブロック・マキシマ予測へのディープ・ニューラル・ネットワーク・モデルの利用が増加するにつれて、このようなブラックボックス・モデルの内部構造を解明できる説明可能なAI手法の必要性も高まっている。 このニーズを満たすために、本稿ではブロック最大値予測モデルのための新しい反事実説明フレームワークを提示する。 既存の手法とは異なり、我々の提案するフレームワークであるDiffusionCFは、予測された極端なブロック最大値を説明するのに役立つ時系列中の異常なパターンを識別するために、ディープアノマリー検出と条件付き拡散モデルを組み合わせる。 幾つかの実世界のデータセットを用いた実験結果は、様々なメトリクス、特に情報量と接近度に従って評価した場合、DiffusionCFが他のベースライン手法よりも優れていることを実証する。
    </summary>
    Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF.
</details>

#### [Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals (Bardh Prenkaj, Mario Villaizán-Vallelado, Tobias Leemann, Gjergji KasneciAuthors)](https://dl.acm.org/doi/10.1145/3637528.3671831)
<details>
    <summary>
        我々は、動的に変化するグラフデータの生成的分類と反事実的説明のための新しいアプローチであるGRACIE（Graph Recalibration and Adaptive Counterfactual Inspection and Explanation）を発表する。 我々は、生成的分類器のレンズを通してグラフ分類問題を研究する。 入力グラフのもっともらしい反事実を特定し、対比的最適化によって決定境界を再調整することによって更新する、動的な自己教師付き潜在変数モデルを提案する。 先行研究とは異なり、学習されたグラフ表現間の線形分離性に依存することなく、妥当な反事実を発見する。 さらに、GRACIEは、潜在空間における確率的サンプリングやグラフマッチングのヒューリスティックを必要としない。 我々の研究は、潜在空間における生成的分類と損失関数の間の暗黙のリンクを抽出するものであり、このアーキテクチャにおける最近の成功を理解するための重要な洞察である。 さらに、有効性と潜在空間の中心領域への説明対象インスタンスの引き込みとの間の本質的なトレードオフを観察し、我々の理論的知見を経験的に実証する。 合成グラフデータと実世界のグラフデータを用いた広範な実験において、動的なデータランドスケープという困難な設定においても、反事実の集合をサンプリングする際に、妥当性が～99%に達するという大幅な改善を達成した。
    </summary>
    We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.
</details>

#### [Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning (Danqing Wang, Antonis Antoniades, Kha-Dinh Luong, Edwin Zhang, Mert Kosan, Jiachen Li, Ambuj Singh, William Yang Wang, Lei Li)](https://dl.acm.org/doi/10.1145/3637528.3672045)
<details>
    <summary>
        グラフニューラルネットワーク（GNN）の反実仮想的説明は、グラフ構造で自然に表現できるデータを理解するための強力な方法を提供する。 さらに、多くの領域において、問題となっているモデルやデータのハイレベルな特性をより良く説明できるような、データ駆動型のグローバルな説明やルールを導出することが非常に望まれている。 しかし、実世界のデータセットでは、人間が注釈を付けたグランドトゥルースがないため、グローバルな反事実的説明を評価することは困難であり、分子科学のような分野での利用が制限されている。 さらに、これらのデータセットの規模が大きくなっているため、ランダムサーチベースの手法には課題がある。 本論文では、分子物性予測のための新しいグローバル説明モデルRLHEXを開発する。 RLHEXは、反実仮想的な説明を人間が定義した原理に合わせることで、説明をより解釈しやすくし、専門家が評価しやすくする。 RLHEXには、グローバルな説明を生成するためのVAEベースのグラフ生成器と、潜在表現空間を人間が定義した原理に合わせるためのアダプターが含まれている。 Proximal Policy Optimization (PPO)により最適化されたRLHEXにより生成されたグローバル説明は、4.12%多くの入力グラフをカバーし、3つの分子データセットにおいて、反事実説明セットと入力セットの間の距離を平均0.47%減少させた。 RLHEXは、人間が設計した様々な原理を反事実説明生成プロセスに組み込む柔軟なフレームワークを提供し、これらの説明をドメインの専門知識と整合させる。
    </summary>
    Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX.
</details>


### [ICLR2024](https://dblp.org/db/conf/iclr/iclr2024.html)
#### [Prediction without Preclusion: Recourse Verification with Reachable Sets (Avni Kothari, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun)](https://openreview.net/forum?id=SCQfYpdoGE)
<details>
    <summary>
       機械学習モデルは、ローンや就職面接、公的給付を誰が受けるかを決めるためによく使われる。 このような場 合のモデルは、行動可能性を考慮せずに特徴を使用する。 つまり、ローンや面接を拒否された個人は、実際、信用や雇用へのアクセスから排除されるのである。 この研究では、モデルが決定対象に固定的な予測を割り当てているかどうかをテストするために、リコース検証（recourse verification）と呼ばれる手順を導入する。 我々は、到達可能集合を用いた検証のための、モデルにとらわれないアプローチを提案する。 我々は、離散的な特徴空間に対して到達可能集合を構築する手法を開発し、その予測値を問い合わせるだけであらゆるモデルの応答性を証明することができる。 我々は、消費者金融のデータセットにおけるリコースの実現不可能性に関する包括的な実証研究を行う。 我々の結果は、モデルが固定的な予測値を割り当てることによって、いかに不注意にアクセスを妨げるかを浮き彫りにし、モデル開発においてアクショナビリティを考慮する必要性を強調している。 
    </summary>
    Machine learning models are often used to decide who receives a loan, a job interview, or a public benefit. Models in such settings use features without considering their actionability. As a result, they can assign predictions that are \emph{fixed} -- meaning that individuals who are denied loans and interviews are, in fact, precluded from access to credit and employment. In this work, we introduce a procedure called recourse verification to test if a model assigns fixed predictions to its decision subjects. We propose a model-agnostic approach for verification with reachable sets -- i.e., the set of all points that a person can reach through their actions in feature space. We develop methods to construct reachable sets for discrete feature spaces, which can certify the responsiveness of any model by simply querying its predictions. We conduct a comprehensive empirical study on the infeasibility of recourse on datasets from consumer finance. Our results highlight how models can inadvertently preclude access by assigning fixed predictions and underscore the need to account for actionability in model development.
</details>

#### [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations (Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah)](https://openreview.net/forum?id=qoHeuRAcSl)
<details>
    <summary>
        物理的領域における大規模言語モデルの常識的推論の根拠付けは、具現化AIにとって極めて重要でありながら未解決の問題である。 先行研究が、記号空間におけるプランニングのためにLLMを直接活用することに焦点を当てているのに対し、本研究では、マルチステップのデモンストレーションに暗黙的に含まれるタスク構造と制約の探索を導くためにLLMを使用する。 具体的には、LLMの高レベル言語表現とロボットの低レベル物理軌道の間の抽象化レイヤとして機能するために、特定の動作制約によってロボット構成をグループ化するモードファミリーの概念を操作計画文献から借用する。 合成的な摂動で人間の実演を数回再生することにより、実演の状態空間に対するカバレッジを生成し、成功した実行とタスクに失敗した反事実を追加する。 我々の説明ベースの学習フレームワークは、失敗から成功の軌跡を予測するために、エンドツーエンドの微分可能なニューラルネットワークを学習し、副産物として、密なラベリングなしに、モードファミリーの低レベルの状態と画像を接地する分類器を学習する。 学習された接地分類器はさらに、言語計画を解釈可能な方法で物理領域の反応ポリシーに変換するために使用することができる。 我々は、2次元ナビゲーションとシミュレーションされた実際のロボット操作タスクを通して、我々のアプローチが模倣学習の解釈可能性と反応性を向上させることを示す。
    </summary>
    Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/
</details>

#### [UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models (Hyunju Kang, Geonhee Han, Hogun Park)](https://openreview.net/forum?id=0j9ZDzMPqr)
<details>
    <summary>
        グラフニューラルネットワーク(GNN)などのノード表現学習は、機械学習における重要な学習手法の一つとなっており、信頼性の高い説明生成の需要が高まっている。 教師ありノード表現学習における説明生成に関する研究は盛んに行われているが、教師なしモデルの説明はあまり研究されていない。 このギャップを解決するために、我々は教師なしノード表現学習における反実仮想（CF）説明生成法を提案する。この方法は、学習された埋め込み空間において、摂動によって注目ノードの最近傍に大きな変化を引き起こす最も重要な部分グラフを同定することを目的とする。 k-最近傍ベースのCF説明手法は、トップkリンク予測やクラスタリングといった教師なしタスクの下流を理解するための、シンプルでありながら極めて重要な情報を提供する。 さらに、教師なしノード表現学習法に対する表現力豊かなCF説明を生成するための、モンテカルロ木探索(MCTS)に基づく説明可能性手法を紹介する。 提案手法は、教師なしGraphSAGEとDGIの両方について、6つのデータセットで改善された性能を示す。
    </summary>
    Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the nearest neighbors of a node of interest in the learned embedding space upon perturbation. The k-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-k link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for Unsupervised Node Representation learning methods, which we call UNR-Explainer. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI.
</details>

#### [Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals (Yair Ori Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart)](https://openreview.net/forum?id=UMfcdRIotC)
<details>
    <summary>
        自然言語処理システムの予測に関する因果関係の説明は、安全性を確保し、信頼を確立するために不可欠である。 しかし、既存の手法では、モデルの予測を効果的、効率的に説明できないことが多く、また、モデルに依存することが多い。 本稿では、モデルにとらわれない説明に取り組み、反事実（CF）近似のための2つのアプローチを提案する。 第一のアプローチはCF生成であり、大規模言語モデル（LLM）が、交絡する概念を変更しないまま、特定のテキスト概念を変更するように促す。 このアプローチは非常に効果的であることが実証されているが、推論時にLLMを適用することはコストがかかる。 そこで我々は、マッチングに基づく第二のアプローチを提示し、学習時にLLMによって導かれ、専用の埋め込み空間を学習する手法を提案する。 この空間は与えられた因果グラフに忠実であり、CFに近似するマッチングを効果的に識別するのに役立つ。 忠実な説明を構築するためにはCFを近似することが必要であることを理論的に示した後、我々のアプローチをベンチマークし、数十億のパラメータを持つLLMを含むいくつかのモデルを説明する。 我々の実証結果は、モデルにとらわれない説明器としてのCF生成モデルの優れた性能を実証している。 さらに、テスト時間のリソースがはるかに少ない我々のマッチングアプローチも、多くのベースラインを上回る効果的な説明を提供する。 また、Top-K手法はテストされた全ての手法を普遍的に改善することがわかった。 最後に、モデル説明のための新しいベンチマークを構築する際のLLMの可能性を示し、その後に我々の結論を検証する。 我々の研究は、自然言語処理システムを解釈するための効率的で正確なアプローチのための新たな道筋を示すものである。
    </summary>
    Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.
</details>


### [AISTATS2024](https://dblp.org/db/conf/aistats/aistats2024.html)
#### [The Risks of Recourse in Binary Classification (Hidde Fokkema, Damien Garreau, Tim van Erven)](https://proceedings.mlr.press/v238/fokkema24a.html)
<details>
    <summary>
        アルゴリズム・リコース（algorithmic recourse）は、機械学習システムによる不利な決定をユーザーが覆すための説明を提供する。 しかし、これまでのところ、遡求を提供することが有益であるか否かについては、ほとんど注目されていない。 我々は抽象的な学習理論的枠組みを導入し、アルゴリズムによる遡及がある場合とない場合の分類のリスク（すなわち期待損失）を比較する。 これにより、集団レベルにおいて、どのような場合に再利用を提供することが有益か有害かという問いに答えることができる。 意外なことに、我々は、遡及手段を提供することが有害であることが判明する多くのもっともらしいシナリオが存在することを発見した。 さらに、分類器を配置する側に、遡及手段を提供しなければならないことを予期して戦略を立てるインセンティブがあるかどうかを調査したところ、ユーザにとって不利になるような遡及手段を提供することがあることがわかった。 従って、アルゴリズムによる救済を提供することは、システムレベルにおいても有害である可能性がある。 我々は、シミュレーションと実世界のデータを用いた実験で、理論的な発見を確認した。 全体として、我々は、アルゴリズムによるリコースの現在のコンセプトは、確実には有益ではなく、したがって再考が必要であると結論づける。
    </summary>
    Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e., expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level. We confirm our theoretical findings in experiments on simulated and real-world data. All in all, we conclude that the current concept of algorithmic recourse is not reliably beneficial, and therefore requires rethinking.
</details>

#### [Manifold-Aligned Counterfactual Explanations for Neural Networks (Asterios Tsiourvas, Wei Sun, Georgia Perakis)](https://proceedings.mlr.press/v238/tsiourvas24a.html)
<details>
    <summary>
        我々は、ニューラルネットワークのための最適な多様体整合的な反事実説明を見つける問題について研究する。 複雑な混合整数最適化(MIP)問題を解く既存のアプローチは、しばしばスケーラビリティの問題に悩まされ、実用的な有用性が制限される。 さらに、解がデータ多様体に従うことが保証されていないため、非現実的な反事実説明が生じる。 これらの課題に対処するため、我々はまず、非線形性の高い局所外れ値因子（LOF）メトリックを混合整数制約として再定式化することにより、多様体の整合を明示的に強制するMIP定式化を提示する。 計算上の課題に対処するため、学習済みニューラルネットワークの幾何学的形状を活用し、探索空間を「生きた」ポリトープ、すなわち、少なくとも1つの実際のデータ点を含む領域に制約することにより、最初の大きくて解きにくい最適化問題を、一連の非常に小さくて解きやすい問題に縮小する効率的な分解スキームを提案する。 実世界のデータセットを用いた実験により、最適かつ現実的な反事実の説明と計算の追跡可能性の両方を生成する上で、我々のアプローチの有効性が実証された。
    </summary>
    We study the problem of finding optimal manifold-aligned counterfactual explanations for neural networks. Existing approaches that involve solving a complex mixed-integer optimization (MIP) problem frequently suffer from scalability issues, limiting their practical usefulness. Furthermore, the solutions are not guaranteed to follow the data manifold, resulting in unrealistic counterfactual explanations. To address these challenges, we first present a MIP formulation where we explicitly enforce manifold alignment by reformulating the highly nonlinear Local Outlier Factor (LOF) metric as mixed-integer constraints. To address the computational challenge, we leverage the geometry of a trained neural network and propose an efficient decomposition scheme that reduces the initial large, hard-to-solve optimization problem into a series of significantly smaller, easier-to-solve problems by constraining the search space to “live” polytopes, i.e., regions that contain at least one actual data point. Experiments on real-world datasets demonstrate the efficacy of our approach in producing both optimal and realistic counterfactual explanations, and computational traceability.
</details>


### [AAAI2024](https://dblp.org/db/conf/aaai/aaai2024.html)
#### [SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies (Wu, H., Sharma, S., Patra, S., & Gopalakrishnan, S.)](https://ojs.aaai.org/index.php/AAAI/article/view/29522)
<details>
    <summary>
        我々は、ニューラルネットワークのための最適な多様体整合的な反事実説明を見つける問題について研究する。 複雑な混合整数最適化(MIP)問題を解く既存のアプローチは、しばしばスケーラビリティの問題に悩まされ、実用的な有用性が制限される。 さらに、解がデータ多様体に従うことが保証されていないため、非現実的な反事実説明が生じる。 これらの課題に対処するため、我々はまず、非線形性の高い局所外れ値因子（LOF）メトリックを混合整数制約として再定式化することにより、多様体の整合を明示的に強制するMIP定式化を提示する。 計算上の課題に対処するため、学習済みニューラルネットワークの幾何学的形状を活用し、探索空間を「生きた」ポリトープ、すなわち、少なくとも1つの実際のデータ点を含む領域に制約することにより、最初の大きくて解きにくい最適化問題を、一連の非常に小さくて解きやすい問題に縮小する効率的な分解スキームを提案する。 実世界のデータセットを用いた実験により、最適かつ現実的な反事実の説明と計算の追跡可能性の両方を生成する上で、我々のアプローチの有効性が実証された。
    </summary>
    With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receiving a favorable decision. Prior work on sequential algorithmic recourse---which recommends a series of changes---focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safe Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures "Value at Risk" and "Conditional Value at Risk" from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different risk-aversion levels using risk measures and recourse desiderata (sparsity and proximity). 
</details>

#### [Providing Fair Recourse over Plausible Groups (Yetukuri, J., Hardy, I., Vorobeychik, Y., Ustun, B., & Liu, Y.)](https://ojs.aaai.org/index.php/AAAI/article/view/30175)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    Machine learning models now automate decisions in applications where we may wish to provide recourse to adversely affected individuals. In practice, existing methods to provide recourse return actions that fail to account for latent characteristics that are not captured in the model (e.g., age, sex, marital status). In this paper, we study how the cost and feasibility of recourse can change across these latent groups. We introduce a notion of group-level plausibility to identify groups of individuals with a shared set of latent characteristics. We develop a general-purpose clustering procedure to identify groups from samples. Further, we propose a constrained optimization approach to learn models that equalize the cost of recourse over latent groups. We evaluate our approach through an empirical study on simulated and real-world datasets, showing that it can produce models that have better performance in terms of overall costs and feasibility at a group level.
</details>

#### [Counterfactual Explanations for Misclassified Images: How Human and Machine Explanations Differ (Abstract Reprint) (Delaney, E., Pakrashi, A., Greene, D., & Keane, M. T.)](https://ojs.aaai.org/index.php/AAAI/article/view/30596)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    Counterfactual explanations have emerged as a popular solution for the eXplainable AI (XAI) problem of elucidating the predictions of black-box deep-learning systems because people easily understand them, they apply across different problem domains and seem to be legally compliant. Although over 100 counterfactual methods exist in the XAI literature, each claiming to generate plausible explanations akin to those preferred by people, few of these methods have actually been tested on users (∼7%). Even fewer studies adopt a user-centered perspective; for instance, asking people for their counterfactual explanations to determine their perspective on a “good explanation”. This gap in the literature is addressed here using a novel methodology that (i) gathers human-generated counterfactual explanations for misclassified images, in two user studies and, then, (ii) compares these human-generated explanations to computationally-generated explanations for the same misclassifications. Results indicate that humans do not “minimally edit” images when generating counterfactual explanations. Instead, they make larger, “meaningful” edits that better approximate prototypes in the counterfactual class. An analysis based on “explanation goals” is proposed to account for this divergence between human and machine explanations. The implications of these proposals for future work are discussed.
</details>

#### [A General Search-Based Framework for Generating Textual Counterfactual Explanations (Gilo, D., & Markovitch, S.)](https://ojs.aaai.org/index.php/AAAI/article/view/29764)
<details>
    <summary>
        機械学習モデルは現在、不利な影響を受けた個人に救済措置を提供したいと考えるアプリケーションにおいて、決定を自動化している。 実際には、遡求を提供するための既存の方法は、モデルで捕捉されていない潜在的な特性（例えば、年齢、性別、婚姻状況）を考慮しないアクションを返す。 本稿では、このような潜在的なグループ間で遡求のコストと実現可能性がどのように変化するかを研究する。 潜在的特徴を共有する個人のグループを特定するために、グループレベルの妥当性という概念を導入する。 サンプルからグループを同定するための汎用的なクラスタリング手法を開発する。 さらに、潜在的なグループに対するリコースのコストを等しくするモデルを学習するための制約付き最適化アプローチを提案する。 シミュレーションと実世界のデータセットを用いた実証研究により、本アプローチを評価し、全体的なコストとグループレベルでの実現可能性の点でより良い性能を持つモデルを生成できることを示す。
    </summary>
    One of the prominent methods for explaining the decision of a machine-learning classifier is by a counterfactual example. Most current algorithms for generating such examples in the textual domain are based on generative language models. Generative models, however, are trained to minimize a specific loss function in order to fulfill certain requirements for the generated texts. Any change in the requirements may necessitate costly retraining, thus potentially limiting their applicability. In this paper, we present a general search-based framework for generating counterfactual explanations in the textual domain. Our framework is model-agnostic, domain-agnostic, anytime, and does not require retraining in order to adapt to changes in the user requirements. We model the task as a search problem in a space where the initial state is the classified text, and the goal state is a text in a given target class. Our framework includes domain-independent modification operators, but can also exploit domain-specific knowledge through specialized operators. The search algorithm attempts to find a text from the target class with minimal user-specified distance from the original classified object.
</details>

#### [Robust Stochastic Graph Generator for Counterfactual Explanations (Prado-Romero, M. A., Prenkaj, B., & Stilo, G.)](https://ojs.aaai.org/index.php/AAAI/article/view/30149)
<details>
    <summary>
        反事実的説明（Counterfactual Explanation：CE）手法は、AIシステムに関わるユーザに洞察を提供する手段として注目されている。 医療画像や自律走行車などの領域で広く研究されている一方で、グラフ反実仮想説明（GCE）手法は比較的十分に研究されていない。 GCEは、基礎となる予測モデルに基づいた異なる結果を持つ、元のグラフに類似した新しいグラフを生成する。 これらの GCE 技術のうち、生成メカニズムに根ざしたものは、芸術的なスタイルや自然言語モデリングなど、他の領域で印象的な成果を示しているにもかかわらず、比較的限定的な調査しか受けていない。 生成的説明器が好まれるのは、入力グラフの摂動を自律的に獲得することで、推論中に反事実的な事例を生成する能力に由来する。 本研究では、RSGG-CEを導入する。RSGG-CEは、学習された潜在空間から、部分的に順序付けられた生成順序を考慮して、反事実的な事例を生成することができる、新しい反事実的説明のための頑健な確率的グラフ生成器である。 さらに、RSGG-CEの性能をSoA生成説明器と比較するための定量的・定性的分析を行い、もっともらしい反事実候補を生成する能力が向上していることを明らかにする。
    </summary>
    Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanations able to produce counterfactual examples from the learned latent space considering a partially ordered generation sequence. Furthermore, we undertake quantitative and qualitative analyses to compare RSGG-CE's performance against SoA generative explainers, highlighting its increased ability to engendering plausible counterfactual candidates.
</details>

#### [Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks (Wang, Z., Zeng, Q., Lin, W., Jiang, M., & Tan, K. C.)](https://ojs.aaai.org/index.php/AAAI/article/view/30168)
<details>
    <summary>
        公平なグラフ・ニューラル・ネットワーク（GNN）は、実生活における大きな賭けに対してアルゴリズムの公平性を促進するために数多く提案されている。 一方、説明可能性は、一般に、人間が理解可能な説明を提供することによって、機械学習の実務家がモデルをデバッグするのを助けるために提案されている。 しかし、GNNにおける公平性診断のための説明を生成するための説明可能性に関する研究はほとんど行われていない。 本稿では、説明可能性の観点から、どのような部分グラフパターンがGNNの偏った振る舞いを引き起こすのか、また、偏りを是正するために実務者はどのような行動を取ることができるのか、という問題を探求する。 この2つの問いに答えることで、本論文は差別的振る舞いの原因となる、コンパクトで診断可能、かつ実行可能な説明を生成することを目的とする。 具体的には、診断可能で行動可能な説明を生成する問題を、多目的組み合わせ最適化問題として定式化する。 この問題を解決するために、GNNの説明可能性と公平性を一度に確保するための多目的進化アルゴリズムを提示する。 特に、進化アルゴリズムの計算効率を高めるために、影響ノードに基づく勾配近似を開発する。 提案するフレームワークの有効性を説明するために、理論的分析を行う。 広範な実験を行い、分類性能、公平性、解釈可能性の観点から提案手法の優位性を実証する。
    </summary>
    A plethora of fair graph neural networks (GNNs) have been proposed to promote algorithmic fairness for high-stake real-life contexts. Meanwhile, explainability is generally proposed to help machine learning practitioners debug models by providing human-understandable explanations. However, seldom work on explainability is made to generate explanations for fairness diagnosis in GNNs. From the explainability perspective, this paper explores the problem of what subgraph patterns cause the biased behavior of GNNs, and what actions could practitioners take to rectify the bias? By answering the two questions, this paper aims to produce compact, diagnostic, and actionable explanations that are responsible for discriminatory behavior. Specifically, we formulate the problem of generating diagnostic and actionable explanations as a multi-objective combinatorial optimization problem. To solve the problem, a dedicated multi-objective evolutionary algorithm is presented to ensure GNNs' explainability and fairness in one go. In particular, an influenced nodes-based gradient approximation is developed to boost the computation efficiency of the evolutionary algorithm. We provide a theoretical analysis to illustrate the effectiveness of the proposed framework. Extensive experiments have been conducted to demonstrate the superiority of the proposed method in terms of classification performance, fairness, and interpretability.
</details>


### [FAccT2024](https://dblp.org/db/conf/fat/facct2024.html)
#### [Actionable Recourse for Automated Decisions: Examining the Effects of Counterfactual Explanation Type and Presentation on Lay User Understanding (Peter M. VanNostrand, Dennis M. Hofmann, Lei Ma, Elke A. Rundensteiner)](https://dl.acm.org/doi/10.1145/3630106.3658997)
<details>
    <summary>
        自動意思決定システムは、ネガティブな結果が意思決定対象者に大きな影響を与える可能性のある、雇用や信用承認などの領域でますます導入されるようになっている。 そのため、最近の研究では、意思決定対象者が意思決定システムを理解し、結果を変更するために行動可能な手段を取ることを可能にする説明を提供することに焦点が当てられている。 一般的な反実仮想説明技法は、否定的な結果を肯定的な結果に変えるインスタンスへの変更を説明することによって、これを達成することを目的としている。 残念なことに、多くの反実仮想的アプローチのうち、どれが最もこの目的を達成できるかを評価するためのユーザー評価はほとんど行われていない。 本研究では、クラウドソースによる被験者間ユーザー研究（N = 252）を実施し、自動意思決定システムに対する素人の意思決定被験者の理解に及ぼす反事実説明の種類と提示の効果を検証する。 その結果、地域ベースの反実仮想タイプは、点ベースのタイプと比較して、客観的理解、主観的理解、回答確信度を有意に増加させることがわかった。 また、反実仮想の提示は回答時間に有意に影響し、回答確信度に対する反実仮想タイプの効果を緩和するが、理解度には影響しないことがわかった。 質的分析により、意思決定主体が異なる説明構成とどのように相互作用するかが明らかになり、説明正当化に対する満たされていないニーズが浮き彫りになった。 我々の結果は、自動化された意思決定ワークフローにおいて、実用的な行動可能手段を達成し、一般利用者に正義と機会を求める力を与えるための、反実仮想的説明技術の開発に対する貴重な洞察と提言を提供する。
    </summary>
    Automated decision-making systems are increasingly deployed in domains such as hiring and credit approval where negative outcomes can have substantial ramifications for decision subjects. Thus, recent research has focused on providing explanations that help decision subjects understand the decision system and enable them to take actionable recourse to change their outcome. Popular counterfactual explanation techniques aim to achieve this by describing alterations to an instance that would transform a negative outcome to a positive one. Unfortunately, little user evaluation has been performed to assess which of the many counterfactual approaches best achieve this goal. In this work, we conduct a crowd-sourced between-subjects user study (N = 252) to examine the effects of counterfactual explanation type and presentation on lay decision subjects’ understandings of automated decision systems. We find that the region-based counterfactual type significantly increases objective understanding, subjective understanding, and response confidence as compared to the point-based type. We also find that counterfactual presentation significantly effects response time and moderates the effect of counterfactual type for response confidence, but not understanding. A qualitative analysis reveals how decision subjects interact with different explanation configurations and highlights unmet needs for explanation justification. Our results provide valuable insights and recommendations for the development of counterfactual explanation techniques towards achieving practical actionable recourse and empowering lay users to seek justice and opportunity in automated decision workflows.
</details>

#### [CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale (Ayan Majumdar, Isabel Valera)](https://dl.acm.org/doi/10.1145/3630106.3659003)
<details>
    <summary>
        例えば、融資、雇用、教育において即座に決定を下すオンライン・プラットフォームなど、大規模な意思決定プロセスを自動化するためにアルゴリズムが使われるようになってきている。 このような自動化されたシステムによって不利な決定が下された場合、影響を受けた個人がそれを覆すことができるような勧告を、即座に否定的な決定に付随させることによって、救済を可能にすることが不可欠である。 しかし、大規模な環境においてアルゴリズムによる救済を提供することの実際的な課題は無視できない。行動に移せる救済勧告を与えるには、応募者の特徴間の関係に関する因果的な知識だけでなく、却下された応募者ごとに複雑な組み合わせ最適化問題を解く必要がある。 本研究では、因果関係のある再コース推薦を大規模に生成するための新しいフレームワークであるCARMAを紹介する。 CARMAは、因果情報が限られた実用的な設定において、事前に訓練された最新の因果生成モデルを活用し、再コース推薦を見つける。 さらに重要なことに、CARMAは、複雑なリコース最適化問題を予測タスクとしてキャストすることで、これらの推奨を見つけるスケーラビリティに対処している。 新しいニューラルネットワークベースのフレームワークをトレーニングすることで、CARMAは最適なリコースアクションの監視を必要とせずに予測タスクを効率的に解く。 我々の広範な評価により、学習後にCARMA上で推論を実行すると、因果的なリコースが確実に償却され、最適なリコメンデーションが瞬時に生成されることが示された。 CARMAは柔軟性があり、アルゴリズムによる意思決定や事前に訓練された因果生成モデルに対して、微分可能性が確保されている限り、その最適化は汎用的である。 さらに、CARMAをケーススタディで紹介し、難易度や所要時間などの要因に基づく母集団レベルの特徴嗜好を容易に取り入れることで、因果的な再コース推奨を調整する能力を説明する。
    </summary>
    Algorithms are increasingly used to automate large-scale decision-making processes, e.g., online platforms that make instant decisions in lending, hiring, and education. When such automated systems yield unfavorable decisions, it is imperative to allow for recourse by accompanying the instantaneous negative decisions with recommendations that can help affected individuals to overturn them. However, the practical challenges of providing algorithmic recourse in large-scale settings are not negligible: giving recourse recommendations that are actionable requires not only causal knowledge of the relationships between applicant features but also solving a complex combinatorial optimization problem for each rejected applicant. In this work, we introduce CARMA, a novel framework to generate causal recourse recommendations at scale. For practical settings with limited causal information, CARMA leverages pre-trained state-of-the-art causal generative models to find recourse recommendations. More importantly, CARMA addresses the scalability of finding these recommendations by casting the complex recourse optimization problem as a prediction task. By training a novel neural-network-based framework, CARMA efficiently solves the prediction task without requiring supervision for optimal recourse actions. Our extensive evaluations show that post-training, running inference on CARMA reliably amortizes causal recourse, generating optimal and instantaneous recommendations. CARMA exhibits flexibility, as its optimization is versatile with respect to the algorithmic decision-making and pre-trained causal generative models, provided their differentiability is ensured. Furthermore, we showcase CARMA in a case study, illustrating its ability to tailor causal recourse recommendations by readily incorporating population-level feature preferences based on factors such as difficulty or time needed.
</details>

#### [MiMICRI: Towards Domain-centered Counterfactual Explanations of Cardiovascular Image Classification Models (Grace Guo, Lifu Deng, Animesh Tandon, Alex Endert, Bum Chul Kwon)](https://dl.acm.org/doi/10.1145/3630106.3659011)
<details>
    <summary>
        近年、一般に公開された大規模な医用画像データセットの普及により、心血管画像の分類と解析のための人工知能（AI）モデルが急増している。 同時に、これらのモデルが重大な影響を及ぼす可能性があることから、特定の画像入力が与えられた場合にモデルの予測を説明することを目的とした、さまざまな説明可能なAI（XAI）手法の開発が進められている。 しかし、これらの手法の多くは、領域専門家と共に開発・評価されたものではなく、また、説明は、医療専門家や領域知識の観点から文脈化されていない。 本論文では、心血管画像分類モデルのドメイン中心の反事実的説明を提供する、新しいフレームワークとPythonライブラリMiMICRIを提案する。 MiMICRIは、ユーザが対話的に、形態学的構造に対応する医用画像のセグメントを選択し、置き換えることを支援する。 生成された反実仮想から、ユーザーは各セグメントがモデルの予測に与える影響を評価し、既知の医学的事実に対してモデルを検証することができる。 我々はこのライブラリを2人の医療専門家を用いて評価した。 我々の評価は、領域中心のXAIアプローチがモデル説明の解釈可能性を高め、専門家が関連する領域知識の観点からモデルについて推論するのを助けることができることを示している。 しかしながら、生成された反事実の臨床的妥当性についての懸念も浮上した。 最後に、MiMICRIフレームワークの一般化可能性と信頼性、およびヘルスケア文脈におけるモデル解釈可能性のためのドメイン中心XAI手法の開発に対する我々の知見の意味について議論する。
    </summary>
    The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.
</details>



## 2023
### [NeurIPS2023](https://dblp.org/db/conf/nips/neurips2023.html)
#### [Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models (Chirag Raman, Alec Nonnemaker, Amelia Villegas-Morcillo, Hayley Hung, Marco Loog)](https://papers.nips.cc/paper_files/paper/2023/hash/694ec0018b9fd0ebe863ec29fa5a89b9-Abstract-Conference.html)
<details>
    <summary>
        確率的多変量時系列予測（回帰）設定における反事実推論のための、その場限りの顕著性に基づく説明の枠組みを提案する。 複数の社会科学分野の研究から導き出されたMillerの説明の枠組みを基に、反事実推論と顕著性に基づく説明手法の間の概念的なつながりを確立する。 顕著性の原則的な概念の欠如に対処するため、人間の前注意的視覚認知に基づく情報理論的顕著性の統一的定義を活用し、それを予測設定に拡張する。 具体的には、確率的予測を行う際に、どの観測されたタイムステップが基礎モデルにとって顕著に見えるかを特定するために、一般的に使用される密度関数の閉形式を得る。 実世界のデータでは得られない真実の顕著性を確立するために、合成データを用いて原理的な方法で我々のフレームワークを実証的に検証する。 最後に、実世界のデータと予測モデルを用いて、本フレームワークが、野生の特徴間の因果関係に関する新しいデータ駆動型の仮説を形成する際に、領域の専門家をどのように支援できるかを示す。
    </summary>
    We propose a post hoc saliency-based explanation framework for counterfactual reasoning in probabilistic multivariate time-series forecasting (regression) settings. Building upon Miller's framework of explanations derived from research in multiple social science disciplines, we establish a conceptual link between counterfactual reasoning and saliency-based explanation techniques. To address the lack of a principled notion of saliency, we leverage a unifying definition of information-theoretic saliency grounded in preattentive human visual cognition and extend it to forecasting settings. Specifically, we obtain a closed-form expression for commonly used density functions to identify which observed timesteps appear salient to an underlying model in making its probabilistic forecasts. We empirically validate our framework in a principled manner using synthetic data to establish ground-truth saliency that is unavailable for real-world data. Finally, using real-world data and forecasting models, we demonstrate how our framework can assist domain experts in forming new data-driven hypotheses about the causal relationships between features in the wild.
</details>

#### [COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs (Tiep Le, VASUDEV LAL, Phillip Howard)](https://papers.nips.cc/paper_files/paper/2023/hash/e14e4cb8266184ceb234973dfe07faed-Abstract-Datasets_and_Benchmarks.html)
<details>
    <summary>
        反事実例は、自然言語処理（NLP）の分野において、データセット中の偽の相関に対する言語モデルの評価と頑健性の向上の両方に有用であることが証明されている。 自然言語処理においてその有用性が実証されているにもかかわらず、マルチモーダルな反事実例は、反事実の変化を最小限に抑えた画像とテキストのペアデータを作成することが困難であるため、比較的未開拓であった。 この課題を解決するために、我々はテキスト-画像拡散モデルを用いて反事実例を自動生成するスケーラブルなフレームワークを紹介する。 COCO-Counterfactualsは、MS-COCOデータセットに基づく、画像とテキストキャプションの対からなるマルチモーダルな反事実データセットである。 COCO-Counterfactualsの品質を人間による評価によって検証し、既存のマルチモーダルモデルが、我々の反事実画像とテキストのペアによって挑戦されることを示す。 さらに、COCO-Counterfactualsが、訓練データの増強を通じて、マルチモーダル視覚言語モデルの領域外汎化を改善するのに有用であることを実証する。 我々のコードとCOCO-Counterfactualsデータセットを公開する。
    </summary>
    Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code and the COCO-Counterfactuals dataset publicly available.
</details>


### [ICML2023](https://dblp.org/db/conf/icml/icml2023.html)
#### [On the Impact of Algorithmic Recourse on Social Segregation (Ruijiang Gao, Himabindu Lakkaraju)](https://proceedings.mlr.press/v202/gao23d.html)
<details>
    <summary>
        予測モデルが実世界の様々な用途に浸透するにつれ、こうしたモデルの結果によって悪影響を受けた個人が、救済の手段を確実に得られるようにすることが重要になってきている。 このため、近年、アルゴリズムによる救済に関する研究が活発化している。 救済措置は、影響を受ける個人にとっては非常に有益なものですが、大規模に実施されると、データ分配のシフトやその他の意図しない結果を招く可能性があります。 しかし、アルゴリズムによるリコースの実施後の影響を理解する研究はほとんどありません。 本研究では、アルゴリズムによるリコースの社会的影響の遅れを分析する最初の試みの1つを行うことで、前述のギャップに対処する。 そのために、最先端のアルゴリズムが出力するリコースを理論的かつ実証的に分析する。 我々の分析は、エンドユーザーによるリソースの大規模な実装が、社会的分離を悪化させる可能性があることを示す。 この問題に対処するために、我々は、分離の可能性を最小化するだけでなく、現実的なリソースを提供するために、暗黙的及び明示的な条件付き生成モデルを活用する新しいアルゴリズムを提案する。 実世界のデータセットを用いた広範な実験により、提案するアプローチの有効性を実証する。
    </summary>
    As predictive models seep into several real-world applications, it has become critical to ensure that individuals who are negatively impacted by the outcomes of these models are provided with a means for recourse. To this end, there has been a growing body of research on algorithmic recourse in recent years. While recourses can be extremely beneficial to affected individuals, their implementation at a large scale can lead to potential data distribution shifts and other unintended consequences. However, there is little to no research on understanding the impact of algorithmic recourse after implementation. In this work, we address the aforementioned gaps by making one of the first attempts at analyzing the delayed societal impact of algorithmic recourse. To this end, we theoretically and empirically analyze the recourses output by state-of-the-art algorithms. Our analysis demonstrates that large-scale implementation of recourses by end users may exacerbate social segregation. To address this problem, we propose novel algorithms which leverage implicit and explicit conditional generative models to not only minimize the chance of segregation but also provide realistic recourses. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed approaches.
</details>

#### [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees (Faisal Hamman, Erfaun Noorani, Saumitra Mishra, Daniele Magazzeni, Sanghamitra Dutta)](https://proceedings.mlr.press/v202/hamman23a.html)
<details>
    <summary>
        モデルが更新されたり、わずかでも変更されたりしても有効であり続けるような、ロバストな反事実説明を生成することに新たな関心が集まっています。 ロバストな反事実を見つけるために、既存の文献では、元のモデル m と新しいモデル M がパラメータ空間で境界がある、すなわち、$Params(M)−Params(m) \leq \Delta$ を仮定しています。 しかし、モデルは、与えられたデータセット上での予測や精度がほとんど変化しないまま、パラメータ空間内で大きく変化することがよくある。 本研究では、自然発生的モデル変化と呼ばれる数学的抽象化を導入し、データ多様体上にある点の予測値の変化が制限されるような、パラメータ空間における任意の変化を可能にする。 次に、ニューラルネットワークのような微分可能なモデルに対して、モデル変化に対する反事実の頑健性を定量化するための、安定性（Stability）と呼ぶ尺度を提案する。 我々の主な貢献は、我々の尺度によって定義されるStabilityの値が十分に高い反事実は、高い確率で潜在的な「自然発生的」モデル変更後も有効であることを示すことである（独立ガウシアンのリプシッツ関数の濃度境界を活用）。 我々の定量化は、常に利用可能とは限らないデータ点周りの局所リプシッツ定数に依存するため、我々の提案する尺度の実用的な緩和も検討し、それらをどのように組み込めば、近い、現実的で、潜在的なモデル変更後も有効なニューラルネットワークのロバストな反事実を見つけることができるかを実験的に実証する。
    </summary>
    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model m and the new model M are bounded in the parameter space, i.e., $Params(M)−Params(m) \leq \Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed naturally-occurring model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure – that we call Stability – to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counterfactuals with sufficiently high value of Stability as defined by our measure will remain valid after potential “naturally-occurring” model changes with high probability (leveraging concentration bounds for Lipschitz function of independent Gaussians). Since our quantification depends on the local Lipschitz constant around a data point which is not always available, we also examine practical relaxations of our proposed measure and demonstrate experimentally how they can be incorporated to find robust counterfactuals for neural networks that are close, realistic, and remain valid after potential model changes.
</details>

#### [GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations (Dan Ley, Saumitra Mishra, Daniele Magazzeni)](https://proceedings.mlr.press/v202/ley23a.html)
<details>
    <summary>
        反実仮想的説明は、説明可能性において広く研究されており、公平性、再帰性、モデル理解において顕著な応用依存的な手法がある。 しかし、これらの手法の大きな欠点は、ローカルレベルやインスタンスレベルを超えた説明ができないことである。 多くの研究がグローバルな説明の概念に触れているが、一般的には、グローバルな特性を確認することを期待して、大量のローカルな説明を集約することを提案している。 一方、実務家はより効率的でインタラクティブな説明可能性ツールを求めている。 我々はこの機会に、特に高次元のデータセットや連続的特徴の存在下で、現在の最新技術に関連する信頼性とスケーラビリティの問題に取り組む柔軟なフレームワーク、Global and Efficient Counterfactual Explanations (GLOBE-CE)を提案する。 さらに、カテゴリ特徴変換のユニークな数学的分析を提供し、我々の手法に利用する。 GLOBE-CEは、一般に公開されているデータセットとユーザスタディを用いた実験評価により、複数の評価指標（速度、信頼性など）において、現在の最先端技術を大幅に上回る性能を示すことが実証された。
    </summary>
    Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global and Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathematical analysis of categorical feature translations, utilising it in our method. Experimental evaluation with publicly available datasets and user studies demonstrate that GLOBE-CE performs significantly better than the current state-of-the-art across multiple metrics (e.g., speed, reliability).
</details>

#### [Self-Interpretable Time Series Prediction with Counterfactual Explanations (Jingquan Yan, Hao Wang)](https://proceedings.mlr.press/v202/yan23d.html)
<details>
    <summary>
        解釈可能な時系列予測は、ヘルスケアや自律走行などのセーフティクリティカルな分野において極めて重要である。 既存の手法のほとんどは、時系列のセグメントに重要なスコアを割り当てることによって予測を解釈することに焦点を当てている。 本論文では、異なる、より挑戦的なルートを取り、時系列予測に対する反実仮想的で実行可能な説明を生成する、Counterfactual Time Series（CounTS）と呼ばれる、自己解釈可能なモデルの開発を目指す。 具体的には、時系列の反事実的説明の問題を形式化し、関連する評価プロトコルを確立し、時系列のアブダクション、アクション、予測の反事実的推論能力を備えた変分ベイズ深層学習モデルを提案する。 最先端のベースラインと比較して、我々の自己解釈可能なモデルは、同等の予測精度を維持しながら、より優れた反事実的説明を生成することができる。
    </summary>
    Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
</details>


### [IJCAI2023](https://dblp.org/db/conf/ijcai/ijcai2023.html)
#### [Incentivizing Recourse through Auditing in Strategic Classification (Andrew Estornell, Yatong Chen, Sanmay Das, Yang Liu, Yevgeniy Vorobeychik)]()
<details>
    <summary>
        個人の生活や幸福に直接影響を与えるような重大な意思決定の自動化が進むにつれ、多くの重要な検討事項が提起される。 その中でも特に重要なのは、より望ましい結果を得ようとする個人の戦略的行動である。 このような行動には2つの形態があり、1）個人属性の虚偽申告、2）リコース、つまりそのような属性を真に変える行動、がよく研究される。 前者は欺瞞を伴い、本質的に望ましくないが、後者は真の個人の資質を変える限りにおいて望ましい目標である可能性がある。 我々は、個人の戦略的選択として、虚偽報告と遡及を統一的な枠組みで研究する。 特に、属性操作よりも遡及行動にインセンティブを与える手段として監査を提案し、効用最大化型と遡及最大化型の2種類のプリンシパルに対する最適な監査政策を特徴付ける。 さらに、操作に対するリコースのインセンティブとしての補助金について考察し、効用最大化型のプリンシパルでさえ、そのような補助金を提供するためにかなりの監査予算を割くことを厭わないであろうことを示す。 最後に、失敗した監査に対する罰金を最適化する問題を検討し、監査の結果として集団が被る総費用を制約する。
    </summary>
    The increasing automation of high-stakes decisions with direct impact on the lives and well-being of individuals raises a number of important considerations. Prominent among these is strategic behavior by individuals hoping to achieve a more desirable outcome. Two forms of such behavior are commonly studied: 1) misreporting of individual attributes, and 2) recourse, or actions that truly change such attributes. The former involves deception, and is inherently undesirable, whereas the latter may well be a desirable goal insofar as it changes true individual qualification. We study misreporting and recourse as strategic choices by individuals within a unified framework. In particular, we propose auditing as a means to incentivize recourse actions over attribute manipulation, and characterize optimal audit policies for two types of principals, utility-maximizing and recourse-maximizing. Additionally, we consider subsidies as an incentive for recourse over manipulation, and show that even a utility-maximizing principal would be willing to devote a considerable amount of audit budget to providing such subsidies. Finally, we consider the problem of optimizing fines for failed audits, and bound the total cost incurred by the population as a result of audits.
</details>


### [KDD2023](https://dblp.org/db/conf/kdd/kdd2023.html)
#### [CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations (Hangzhi Guo, Thanh H. Nguyen, Amulya Yadav)](https://dl.acm.org/doi/10.1145/3580305.3599290)
<details>
    <summary>
        本研究では、機械学習（ML）モデルの学習と、それに対応する反事実（CF）説明の生成を、単一のエンドツーエンドパイプラインに統合した、新しいエンドツーエンド学習フレームワークであるCounterNetを紹介する。 反実仮想的説明は対照的なケースを提供する、すなわち、そのインスタンスに関するMLモデルの予測を予め定義された出力に変更する、あるインスタンスの特徴値に対する最小の修正を見つけようとするものである。 CF説明を生成する先行技術は、2つの主要な制限に苦しんでいる：(i)それらの全ては、独自のMLモデルで使用するために設計されたポストホックメソッドである---その結果、CF説明を生成するためのそれらの手順は、モデルの予測と説明の間の不整合をもたらすMLモデルの訓練によって知らされていない、(ii)それらのほとんどは、各入力データ点に対するCF説明を見つけるために、別々の時間集約的な最適化問題を解くことに依存している（これは、それらの実行時間に悪影響を与える）。 本研究では、予測モデルの学習と反実仮想（CF）説明の生成を単一のパイプラインに統合するエンドツーエンドの学習フレームワークであるCounterNetを提示することで、（CF説明を生成する）一般的なポストホックパラダイムからの斬新な出発を行う。 ポストホックメソッドとは異なり、CounterNetはCF説明生成の最適化を予測モデルと共に一度だけ可能にする。 我々は、CounterNetのネットワークを効果的に訓練するのに役立つブロック単位の座標降下手順を採用する。 複数の実世界データセットに対する我々の広範な実験により、CounterNetは高品質の予測を生成し、どのような新しい入力インスタンスに対しても一貫して100%のCF妥当性と低い近接スコアを達成し（それによりコストと妥当性のトレードオフをバランスよく達成する）、既存の最先端ベースラインよりも3倍高速に動作することが示された。
    </summary>
    This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. Unlike post-hoc methods, CounterNet enables the optimization of the CF explanation generation only once together with the predictive model. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100% CF validity and low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.
</details>

#### [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations (Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin V. Bonilla, Gholamreza Haffari, Dinh Phung)](https://dl.acm.org/doi/10.1145/3580305.3599343)
<details>
    <summary>
        解釈可能な機械学習は、説明不可能なことで長い間悪名高い複雑なブラックボックスシステムの推論プロセスを理解しようとするものである。 このアプローチは、ユーザーが結果を変えるために何ができるかを提案する、反実仮想的な説明によるものである。 反事実的な例は、ブラックボックス分類器からのオリジナルの予測に対抗するものでなければならないだけでなく、実用的なアプリケーションのための様々な制約を満たすものでなければなりません。 多様性は重要な制約の1つであるが、あまり議論されていない。 多様な反事実は理想的であるが、他のいくつかの制約に同時に対処することは計算上困難である。 さらに、公開された反事実データに対するプライバシーの懸念が高まっている。 このため、我々は、反事実制約を効果的に扱い、限られた私的説明モデルのプールに貢献する、特徴ベースの学習フレームワークを提案する。 我々は、行動可能性と妥当性の多様な反事実を生成する際に、本手法の柔軟性と有効性を実証する。 我々の反実仮想エンジンは、同容量のものよりも効率的であると同時に、最も低い再識別リスクをもたらす。
    </summary>
    Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.
</details>


### [ICLR2023](https://dblp.org/db/conf/iclr/iclr2023.html)
#### [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse (Martin Pawelczyk, Teresa Datta, Johan Van den Heuvel, Gjergji Kasneci, Himabindu Lakkaraju)](https://openreview.net/forum?id=sC-PmTsiTB)
<details>
    <summary>
        機械学習モデルが実世界の環境において結果的な意思決定を行うために採用されることが多くなっているため、これらのモデルの予測によって不利な影響を受けた（例えば、融資が拒否された）個人が、救済の手段を確実に提供されることが重要になっている。 影響を受ける個人のための救済手段を構築するために、いくつかのアプローチが提案されているが、これらの方法によって出力される救済手段は、低コスト（すなわち、実装の容易さ）を達成するか、小さな摂動（すなわち、救済手段のノイズの多い実装）に対して頑健であるかのいずれかであり、救済コストと頑健性の間の本質的なトレードオフのため、両方ではない。 さらに、先行するアプローチは、前述のトレードオフをナビゲートするためのエージェンシーをエンドユーザーに提供していない。 本研究では、ユーザがリコースコスト対ロバストネスのトレードオフを効果的に管理できるようにする初のアルゴリズムフレームワークを提案することにより、上記の課題に取り組む。 より具体的には、我々のフレームワークProbabilistically ROBust rEcourse (PROBE)は、リコースに小さな変更が加えられた場合、すなわちリコースが多少ノイズを含んで実装された場合に、リコースが無効になる確率（リコース無効率）をユーザに選択させる。 この目的のために、我々は、達成された（結果の）リコース無効率と望ましいリコース無効率との間のギャップを同時に最小化し、リコースコストを最小化し、さらに、結果のリコースが正のモデル予測を達成することを保証する、新しい目的関数を提案する。 我々は、異なるクラスの基礎モデル（線形モデル、木ベースモデルなど）に対する、任意のインスタンスに対応する再コース無効率を特徴付ける新しい理論結果を開発し、提案する目的を効率的に最適化するためにこれらの結果を活用する。 複数の実世界データセットを用いた実験評価により、提案フレームワークの有効性を実証する。
    </summary>
    As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcourse (PROBE) lets users choose the probability with which a recourse could get invalidated (recourse invalidation rate) if small changes are made to the recourse i.e., the recourse is implemented somewhat noisily. To this end, we propose a novel objective function which simultaneously minimizes the gap between the achieved (resulting) and desired recourse invalidation rates, minimizes recourse costs, and also ensures that the resulting recourse achieves a positive model prediction. We develop novel theoretical results to characterize the recourse invalidation rates corresponding to any given instance w.r.t. different classes of underlying models (e.g., linear models, tree based models etc.), and leverage these results to efficiently optimize the proposed objective. Experimental evaluation with multiple real world datasets demonstrates the efficacy of the proposed framework.
</details>

#### [Distributionally Robust Recourse Action (Duy Nguyen, Ngoc Bui, Viet Anh Nguyen)](https://openreview.net/forum?id=E3ip6qBLF7)
<details>
    <summary>
        リコース・アクションは、インスタンスを修正して別の結果を得るための具体的な方法を示すことで、特定のアルゴリズム決定を説明することを目的としている。 既存のリコース生成手法は、機械学習モデルが時間の経過とともに変化しないことを前提としていることが多い。 しかし、この仮定はデータ分布の変化により、実際には必ずしも成立せず、この場合、再コースアクションは無効となる可能性がある。 この欠点を改善するために、我々は分布にロバストな再コースアクション（DiRRAc）フレームワークを提案する。 我々はまず、ロバスト化された再コース設定を最小-最大最適化問題として定式化し、最大問題はモデルパラメータ分布の曖昧さ集合上のゲルブリッヒ距離によって指定される。 次に、最小-最大目的に従ってロバストリコースを求める投影勾配降下アルゴリズムを提案する。 また、我々のDiRRAcフレームワークが、混合重みの誤指定に対するヘッジに拡張可能であることを示す。 合成データセットと3つの実世界データセットの両方を用いた数値実験により、ロバストなリコースを生成する最先端のリコース手法に対する我々の提案するフレームワークの利点を実証する。
    </summary>
    A recourse action aims to explain a particular algorithmic decision by showing one specific way in which the instance could be modified to receive an alternate outcome. Existing recourse generation methods often assume that the machine learning model does not change over time. However, this assumption does not always hold in practice because of data distribution shifts, and in this case, the recourse action may become invalid. To redress this shortcoming, we propose the Distributionally Robust Recourse Action (DiRRAc) framework, which generates a recourse action that has high probability of being valid under a mixture of model shifts. We first formulate the robustified recourse setup as a min-max optimization problem, where the max problem is specified by Gelbrich distance over an ambiguity set around the distribution of model parameters. Then we suggest a projected gradient descent algorithm to find a robust recourse according to the min-max objective. We also show that our DiRRAc framework can be extended to hedge against the misspecification of the mixture weights. Numerical experiments with both synthetic and three real-world datasets demonstrate the benefits of our proposed framework over the state-of-the-art recourse methods, which generate robust recourses.
</details>

#### [On the Trade-Off between Actionable Explanations and the Right to be Forgotten (Martin Pawelczyk, Tobias Leemann, Asia Biega, Gjergji Kasneci)](https://openreview.net/forum?id=HWt4BBZjVW)
<details>
    <summary>
        機械学習（ML）モデルがますます重要なアプリケーションに導入されるようになり、政策立案者たちはデータ保護規制の強化を提案している（GDPR、CCPAなど）。 重要な原則のひとつは「忘れられる権利」であり、ユーザーに自分のデータを削除してもらう権利を与える。 もう1つの重要な原則は、ユーザーが不利な決定を覆すことを可能にする、アルゴリズムによる救済としても知られる「行動可能な説明を受ける権利」である。 現在までのところ、これら2つの原則を同時に運用できるかどうかは不明である。 そこで我々は、データ削除要求の文脈におけるリコースの無効化問題を導入し、研究する。 より具体的には、一般的な最新アルゴリズムの挙動を理論的・経験的に解析し、少数のデータ削除要求（例えば1～2件）が予測モデルの更新を正当化する場合、これらのアルゴリズムによって生成されたリコースが無効になる可能性が高いことを示す。 微分可能なモデルの設定のために、我々は、削除された場合に、無効化されたリソースの割合を最大化する、重要な訓練ポイントの最小のサブセットを特定するフレームワークを提案する。我々のフレームワークを用いて、訓練セットからわずか2個のデータインスタンスを削除するだけで、一般的な最先端のアルゴリズムが出力する全てのリソースの最大95％を無効化できることを実証的に示す。 このように、我々の研究は、「忘れられる権利」の文脈における「行動可能な説明を受ける権利」の互換性に関する基本的な問題を提起し、同時に、リコースの頑健性の決定要因に関する建設的な洞察を提供する。
    </summary>
    As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the “right to be forgotten” which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms. Thus, our work raises fundamental questions about the compatibility of ``the right to an actionable explanation'' in the context of the ``right to be forgotten'', while also providing constructive insights on the determining factors of recourse robustness.
</details>


### [AISTATS2023](https://dblp.org/db/conf/aistats/aistats2023.html)
#### [Feasible Recourse Plan via Diverse Interpolation (Duy Nguyen, Ngoc Bui, Viet Anh Nguyen)](https://proceedings.mlr.press/v206/nguyen23b.html)
<details>
    <summary>
        アルゴリズムの決定を説明し、実用的なフィードバックを推奨することは、機械学習アプリケーションにとってますます重要になっている。 近年、ユーザの幅広い嗜好をカバーするために、多様なリソースを見つけることに多大な努力が払われている。 しかし、既存の研究では、リソースがデータ多様体に近いという要件がしばしば無視されている。それゆえ、構築されたリソースは、ユーザにとってあり得ない、満足のいくものではない可能性がある。 このような問題に対処するため、我々は、多様な行動可能リソース集合をデータ多様体に明示的に向ける新しいアプローチを提案する。 我々はまず、多様性と近接性のトレードオフのバランスをとる、好ましいクラスの多様なプロトタイプ集合を見つける。 これらのプロトタイプを見つける2つの具体的な方法を示す：行列式点過程の最大事後推定値を求める方法と、2次二元プログラムを解く方法である。 行動可能性制約を保証するために、我々は行動可能性グラフを構築し、ノードは訓練サンプルを表し、エッジは2つのインスタンス間の実行可能な行動を示す。 そして、各プロトタイプへの実現可能な経路を見つけ、この経路はプラン内の各リコースに対する実現可能なアクションを示す。 実験結果は、我々の手法が、既存のアプローチよりも優れたコストと多様性のトレードオフを実現しつつ、データ多様体に近いリコースの集合を生成することを示している。
    </summary>
    Explaining algorithmic decisions and recommending actionable feedback is increasingly important for machine learning applications. Recently, significant efforts have been invested in finding a diverse set of recourses to cover the wide spectrum of users’ preferences. However, existing works often neglect the requirement that the recourses should be close to the data manifold; hence, the constructed recourses might be implausible and unsatisfying to users. To address these issues, we propose a novel approach that explicitly directs the diverse set of actionable recourses towards the data manifold. We first find a diverse set of prototypes in the favorable class that balances the trade-off between diversity and proximity. We demonstrate two specific methods to find these prototypes: either by finding the maximum a posteriori estimate of a determinantal point process or by solving a quadratic binary program. To ensure the actionability constraints, we construct an actionability graph in which the nodes represent the training samples and the edges indicate the feasible action between two instances. We then find a feasible path to each prototype, and this path demonstrates the feasible actions for each recourse in the plan. The experimental results show that our method produces a set of recourses that are close to the data manifold while delivering a better cost-diversity trade-off than existing approaches.
</details>

#### [On the Privacy Risks of Algorithmic Recourse (Martin Pawelczyk, Himabindu Lakkaraju, Seth Neel )](https://proceedings.mlr.press/v206/pawelczyk23a.html)
<details>
    <summary>
        結果的な意思決定を行うために予測モデルがますます採用されるようになるにつれて、影響を受ける個人に対してアルゴリズムによる救済手段を提供できる技術の開発が重視されるようになってきている。 このような手段は、影響を受ける個人にとって非常に有益である一方で、潜在的な敵対者がこのような手段を悪用してプライバシーを侵害する可能性もある。 本研究では、敵対者がリソースを活用して、基礎となるモデルの学習データに関するプライベート情報を推論できるかどうか、またどのように推論できるかを調査する初めての試みを行う。 この目的のために、アルゴリズム的リコースを活用した一連の新しいメンバーシップ推論攻撃を提案する。 より具体的には、データインスタンスと、それに対応する最新の遡及手法が出力する反事実との間の距離を活用することで、メンバーシップ推論攻撃に関する先行文献を遡及設定に拡張する。 実世界のデータセットと合成データセットを用いた広範な実験により、リコースによるプライバシーの著しい漏洩が実証された。 我々の研究は、意図しないプライバシーの漏洩が、リコース手法の広範な採用における重要なリスクであることを立証している。
    </summary>
    As predictive models are increasingly being employed to make consequential decisions, there is a growing emphasis on developing techniques that can provide algorithmic recourse to affected individuals. While such recourses can be immensely beneficial to affected individuals, potential adversaries could also exploit these recourses to compromise privacy. In this work, we make the first attempt at investigating if and how an adversary can leverage recourses to infer private information about the underlying model’s training data. To this end, we propose a series of novel membership inference attacks which leverage algorithmic recourse. More specifically, we extend the prior literature on membership inference attacks to the recourse setting by leveraging the distances between data instances and their corresponding counterfactuals output by state-of-the-art recourse methods. Extensive experimentation with real world and synthetic datasets demonstrates significant privacy leakage through recourses. Our work establishes unintended privacy leakage as an important risk in the widespread adoption of recourse methods.
</details>


### [AAAI2023](https://dblp.org/db/conf/aaai/aaai2023.html)
#### [Improvement-Focused Causal Recourse (ICR) (König, G., Freiesleben, T., & Grosse-Wentrup, M.)](https://ojs.aaai.org/index.php/AAAI/article/view/26398)
<details>
    <summary>
        アルゴリズムによるリコースの推奨は、不利な決定を戻すためにどのように行動すべきかを利害関係者に知らせる。 しかし、既存の方法は、受け入れ（すなわち、モデルの決定を戻す）にはつながるが、改善にはつながらない（すなわち、根本的な実世界の状態を戻さない）行動を推奨することがある。 そのような行動を推奨することは、予測モデルを騙すことを推奨することになる。 我々は、概念的な転換を伴う新しい方法、ICR（Improvement-Focused Causal Recourse）を紹介する： 第一に、我々はICRの勧告を改善に向けて導くことを要求する。 第二に、我々は特定の予測因子によって受け入れられるように勧告を調整しない。 その代わりに、我々は原因知識を活用して、改善保証が受け入れ保証に変換されるように、コース前とコース後を正確に予測する決定システムを設計する。 不思議なことに、最適な再コース前の分類器は、ICR行動に対して頑健であり、したがって再コース後に適している。 半合成実験において、我々は、正しい因果関係の知識が与えられた場合、既存のアプローチとは対照的に、ICRが受容と改善の両方を導くことを実証する。
    </summary>
    Algorithmic recourse recommendations inform stakeholders of how to act to revert unfavorable decisions. However, existing methods may recommend actions that lead to acceptance (i.e., revert the model's decision) but do not lead to improvement (i.e., may not revert the underlying real-world state). To recommend such actions is to recommend fooling the predictor. We introduce a novel method, Improvement-Focused Causal Recourse (ICR), which involves a conceptual shift: Firstly, we require ICR recommendations to guide toward improvement. Secondly, we do not tailor the recommendations to be accepted by a specific predictor. Instead, we leverage causal knowledge to design decision systems that predict accurately pre- and post-recourse, such that improvement guarantees translate into acceptance guarantees. Curiously, optimal pre-recourse classifiers are robust to ICR actions and thus suitable post-recourse. In semi-synthetic experiments, we demonstrate that given correct causal knowledge ICR, in contrast to existing approaches, guides toward both acceptance and improvement.
</details>

#### [Explaining Model Confidence Using Counterfactuals (Le, T., Miller, T., Singh, R., & Sonenberg, L.)](https://ojs.aaai.org/index.php/AAAI/article/view/26399)
<details>
    <summary>
        人間とAIの対話において信頼スコアを表示することは、人間とAIシステム間の信頼構築に役立つことが示されている。 しかし、既存の研究のほとんどは、コミュニケーションの形として信頼度スコアのみを使用している。 信頼度スコアはモデル出力の一つに過ぎないため、ユーザーは信頼度スコアを受け入れるかどうかを判断するために、アルゴリズムがなぜ自信を持っているのかを理解したいと思うかもしれない。 本論文では、信頼スコアの反実仮想的説明が、研究参加者が機械学習モデルの予測をより良く理解し、より良く信頼するのに役立つことを示す。 (1)反実仮想例に基づく方法、(2)反実仮想空間の視覚化に基づく方法である。 両者とも、研究参加者の理解と信頼を、説明なしのベースラインよりも増加させるが、定性的な結果は、両者が全く異なる使われ方をすることを示しており、それぞれをどのような場合に使うべきか、また、より良い説明を設計するための方向性を提言するに至っている。
    </summary>
    Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model's prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanations.
</details>

#### [Formalising the Robustness of Counterfactual Explanations for Neural Networks (Jiang, J., Leofante, F., Rago, A., & Toni, F.)](https://ojs.aaai.org/index.php/AAAI/article/view/26740)
<details>
    <summary>
        反実仮想説明（CFX）の使用は、機械学習モデルの説明戦略としてますます一般的になってきている。 しかし、最近の研究では、これらの説明は、基礎となるモデルの変化（例えば、再学習後）に対して頑健でない可能性があることが示されており、実世界での応用における信頼性に疑問が投げかけられている。 この問題を解決するための既存の試みは発見的であり、結果として得られるCFXのモデル変更に対する頑健性は、少数の再トレーニングされたモデルのみで評価され、網羅的な保証を提供することができない。 この問題を解決するために、我々は、ニューラルネットワークのCFXのロバスト性（モデル変更に対するロバスト性）を形式的かつ決定論的に評価する最初の概念である∆-ロバスト性を提案する。 区間ニューラルネットワークに基づく抽象化フレームワークを導入し、モデルパラメータ、すなわち重みとバイアスの無限大の変更に対するCFXの∆ロバストネスを検証する。 次に、このアプローチの有用性を2つの異なる方法で実証する。 第一に、文献にある多くのCFX生成手法の∆ロバスト性を分析し、それらがこの点で重大な欠陥を抱えていることを示す。 次に、∆ロバスト性を既存の手法に組み込むことで、証明可能なロバスト性を持つCFXを提供できることを示す。
    </summary>
    The use of counterfactual explanations (CFXs) is an increasingly popular explanation strategy for machine learning models. However, recent studies have shown that these explanations may not be robust to changes in the underlying model (e.g., following retraining), which raises questions about their reliability in real-world applications. Existing attempts towards solving this problem are heuristic, and the robustness to model changes of the resulting CFXs is evaluated with only a small number of retrained models, failing to provide exhaustive guarantees. To remedy this, we propose ∆-robustness, the first notion to formally and deterministically assess the robustness (to model changes) of CFXs for neural networks. We introduce an abstraction framework based on interval neural networks to verify the ∆-robustness of CFXs against a possibly infinite set of changes to the model parameters, i.e., weights and biases. We then demonstrate the utility of this approach in two distinct ways. First, we analyse the ∆-robustness of a number of CFX generation methods from the literature and show that they unanimously host significant deficiencies in this regard. Second, we demonstrate how embedding ∆-robustness within existing methods can provide CFXs which are provably robust.
</details>

#### [Very Fast, Approximate Counterfactual Explanations for Decision Forests (Carreira-Perpinan, M. Á., & Hada, S. S. )](https://ojs.aaai.org/index.php/AAAI/article/view/25848)
<details>
    <summary>
        我々は、ランダム・フォレストのような分類フォレストや回帰フォレストの反事実説明を見つけることを考える。 このためには、森が望ましい値を出力する与えられたインスタンスに最も近い入力インスタンスを見つける最適化問題を解く必要がある。 厳密解を求めるには、森の葉の数に対して指数関数的なコストがかかる。 我々は単純だが非常に効果的なアプローチを提案する。それは、実際のデータ点が存在する入力空間領域に最適化を制約することである。 この問題は、あるデータセット上で、ある距離を用いた最近傍探索の形に帰着する。 これには2つの利点がある：第一に、解を非常に素早く見つけることができ、大規模な森や高次元データに対応し、対話的な利用が可能である。 第二に、入力空間の高密度領域に向かって導かれるため、発見された解が現実的である可能性が高い。
    </summary>
    We consider finding a counterfactual explanation for a classification or regression forest, such as a random forest. This requires solving an optimization problem to find the closest input instance to a given instance for which the forest outputs a desired value. Finding an exact solution has a cost that is exponential on the number of leaves in the forest. We propose a simple but very effective approach: we constrain the optimization to input space regions populated by actual data points. The problem reduces to a form of nearest-neighbor search using a certain distance on a certain dataset. This has two advantages: first, the solution can be found very quickly, scaling to large forests and high-dimensional data, and enabling interactive use. Second, the solution found is more likely to be realistic in that it is guided towards high-density areas of input space.
</details>


### [FAccT2023](https://dblp.org/db/conf/fat/facct2023.html)
#### [Robustness Implies Fairness in Causal Algorithmic Recourse (Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schoelkopf, Setareh Maghsudi)](https://dl.acm.org/doi/10.1145/3593013.3594057)
<details>
    <summary>
        アルゴリズムによる救済は、受益者により有利な結果をもたらすための推奨事項を提供することで、決定が重大な結果をもたらすブラックボックス化された決定プロセスの内部手続きを開示する。 効果的な救済を確実にするためには、提案される介入策は費用対効果が高いだけでなく、強固で公正でなければならない。 そのためには、同様の個人に対して同様の説明を行うことが不可欠である。 本研究では、因果的アルゴリズムによる遡及における個人の公平性と敵対的頑健性の概念を探求し、その両方を達成するという課題に取り組む。 課題を解決するために、敵対的に頑健なリコースを定義するための新しい枠組みを提案する。 この枠組みでは、保護される特徴を擬似計量として観測し、個人の公平性が敵対的ロバスト性の特別な場合であることを示す。 最後に、公平なロバスト遡及問題を導入し、理論的にも経験的にも望ましい性質を達成するための解を確立する。
    </summary>
    Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.
</details>

#### [Achieving Diversity in Counterfactual Explanations: a Review and Discussion (Thibault Laugel, Adulam Jeyasothy, Marie-Jeanne Lesot, Christophe Marsala, Marcin Detyniecki)](https://dl.acm.org/doi/10.1145/3593013.3594122)
<details>
    <summary>
        説明可能な人工知能（Explainable Artificial Intelligence：XAI）の分野では、学習された決定モデルの予測を、関連する予測を変更するためにインスタンスに加えられる修正を示すことによって、反実仮想例がユーザに説明する。 これらの反実仮想例は、一般に、コスト関数が、ユーザーのニーズを満たす良い説明のための望ましさを定量化するいくつかの基準を組み合わせた最適化問題の解として定義される。 ユーザーニーズは一般的に未知であり、ユーザーごとに異なるため、このような適切な特性の多種多様を考慮することができる。 この問題を回避するために、いくつかのアプローチは、予測を説明するために、単一のものではなく、多様な反事実例の集合を生成することを提案している。 本稿では、この多様性の概念について提案されてきた数多くの、時には相反する定義のレビューを提案する。 その根底にある原理と、それらが依拠するユーザーニーズに関する仮説について議論し、いくつかの次元（明示的か暗黙的か、定義される世界、適用されるレベル）に沿ってそれらを分類することを提案し、このトピックに関するさらなる研究課題の特定へと導く。
    </summary>
    In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.
</details>



## 2022
### [NeurIPS2022](https://dblp.org/db/conf/nips/neurips2022.html)
#### [Bayesian Persuasion for Algorithmic Recourse (Keegan Harris, Valerie Chen, Joon Kim, Ameet Talwalkar, Hoda Heidari, Steven Z. Wu)](https://papers.nips.cc/paper_files/paper/2022/hash/480150047ecb2187a3a8b8dccfd8f2de-Abstract-Conference.html)
<details>
    <summary>
        自動化された意思決定の対象となった場合、意思決定対象者は、有利な決定を受ける可能性が最大になると考える方法で、観察可能な特徴を戦略的に変更することができる。 多くの実際的な状況において、基本的な評価ルールは、賭博を回避し競争上の優位性を維持するために意図的に秘密にされている。 その結果、戦略的な特徴の修正を行う際に、決定主体は不完全な情報に頼らざるを得なくなる。 我々はこのような設定をベイズ説得ゲームとしてとらえ、意思決定者が意思決定対象者に行動勧告（またはシグナル）を提供することによって、望ましい方法で特徴を修正するように動機付ける。 説得を用いた場合、意思決定者と意思決定対象者は期待値において決して悪くならないが、意思決定者は著しく良くなる可能性があることを示す。 意思決定者が最適なベイズインセンティブ適合(BIC)シグナリング政策を見つける問題は、無限に多くの変数に対する最適化の形をとるが、我々はこの最適化が、可能な評価ルールの空間の有限個の領域に対する線形プログラムとしてキャストできることを示す。 この再定式化により問題は劇的に単純化されるが、線形プログラムを解くには、比較的単純なケースであっても指数関数的に多くの変数についての推論が必要となる。 この観察に動機づけられ、我々は、ほぼ最適なシグナリング政策を回復する多項式時間近似スキームを提供する。 最後に、半合成データを用いた数値シミュレーションにより、アルゴリズムによるリコース設定において説得を用いることの利点を実証的に示す。
    </summary>
    When subjected to automated decision-making, decision subjects may strategically modify their observable features in ways they believe will maximize their chances of receiving a favorable decision. In many practical situations, the underlying assessment rule is deliberately kept secret to avoid gaming and maintain competitive advantage. The resulting opacity forces the decision subjects to rely on incomplete information when making strategic feature modifications. We capture such settings as a game of Bayesian persuasion, in which the decision maker offers a form of recourse to the decision subject by providing them with an action recommendation (or signal) to incentivize them to modify their features in desirable ways. We show that when using persuasion, the decision maker and decision subject are never worse off in expectation, while the decision maker can be significantly better off. While the decision maker’s problem of finding the optimal Bayesian incentive compatible (BIC) signaling policy takes the form of optimization over infinitely many variables, we show that this optimization can be cast as a linear program over finitely-many regions of the space of possible assessment rules. While this reformulation simplifies the problem dramatically, solving the linear program requires reasoning about exponentially-many variables, even in relatively simple cases. Motivated by this observation, we provide a polynomial-time approximation scheme that recovers a near-optimal signaling policy. Finally, our numerical simulations on semi-synthetic data empirically demonstrate the benefits of using persuasion in the algorithmic recourse setting.
</details>

#### [Learning Recourse on Instance Environment to Enhance Prediction Accuracy (Lokesh N, Guntakanti Sai Koushik, Abir De, Sunita Sarawagi)](https://papers.nips.cc/paper_files/paper/2022/hash/a399456a191ca36c7c78dff367887f0a-Abstract-Conference.html)
<details>
    <summary>
        機械学習モデルは、しばしば悪い環境からサンプリングされたインスタンスに対してパフォーマンスが低下しやすい。 例えば、画像分類器は、低照度下で撮影された画像に対して低い精度を提供する可能性がある。 AI駆動の医療診断のようなMLアプリケーションでは、より信頼性の高い診断のために、インスタンスを再取得するための代替環境設定の形でリコースを提供することがより良い選択肢となり得る。 本論文では、再利用されたインスタンスが分類器により良い予測に従うように、環境空間上で再利用を適用することを学習する{em RecourseNet}と呼ばれるモデルを提案する。 最適なrecourseを出力する学習は、recoursedインスタンスを生成する基礎的な物理過程へのアクセスを仮定していないため、困難である。 また、最適な設定はインスタンスに依存する可能性がある。例えば、物体認識に最適なカメラの角度は物体の形状の関数である可能性がある。 我々は、(a)recourse下で高い性能を発揮するように最適化された分類器を学習し、(b)良好な環境設定の下で、学習データが限られたインスタンスしか含まない可能性がある場合に、recourse予測器を学習し、(c)recourseが分類器の信頼性を向上させる可能性が高い場合にのみ、選択的にrecourseをトリガーする、新しい3レベル学習法を提案する。
    </summary>
    Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called {\em RecourseNet} that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier. Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent --- for example the best camera angle for object recognition could be a function of the object's shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence.
</details>

#### [CLEAR: Generative Counterfactual Explanations on Graphs (Jing Ma, Ruocheng Guo, Saumitra Mishra, Aidong Zhang, Jundong Li)](https://papers.nips.cc/paper_files/paper/2022/hash/a69d7f3a1340d55c720e572742439eaf-Abstract-Conference.html)
<details>
    <summary>
        反実仮想的説明は、「望ましい予測ラベルを得るためには、入力インスタンスをどのように変更すべきか」という問いに答えることで、機械学習モデルの説明可能性を促進する。 摂動前後のインスタンスを比較することで、人間による解釈を強化することができる。 反実仮想的説明に関する既存の研究の多くは、表データや画像データに限定されている。 本稿では、グラフ上での反実仮想説明生成の問題を研究する。 グラフ上での反実仮想的説明の生成はいくつかの研究により検討されているが、この問題の多くの課題はまだ十分に解決されていない： 1)グラフの離散的で無秩序な空間における最適化、2)未見のグラフにおける汎化、3)因果モデルの事前知識なしに、生成された反事実における因果性の維持。 これらの課題に取り組むため、我々は、グラフレベル予測モデルのためのグラフ上の反事実説明を生成することを目的とした新しいフレームワークCLEARを提案する。 具体的には、CLEARはグラフ変分オートエンコーダーに基づくメカニズムを活用し、最適化と汎化を容易にし、因果モデルをより良く同定するための補助変数を活用して因果性を促進する。 合成グラフと実世界グラフの両方を用いた広範な実験により、CLEARがグラフ上の最新の反事実説明手法よりも優れていることが、様々な側面から検証された。 
    </summary>
    Counterfactual explanations promote explainability in machine learning models by answering the question “how should the input instance be altered to obtain a desired predicted label?". The comparison of this instance before and after perturbation can enhance human interpretation. Most existing studies on counterfactual explanations are limited in tabular data or image data. In this paper, we study the problem of counterfactual explanation generation on graphs. A few studies have explored to generate counterfactual explanations on graphs, but many challenges of this problem are still not well-addressed: 1) optimizing in the discrete and disorganized space of graphs; 2) generalizing on unseen graphs; 3) maintaining the causality in the generated counterfactuals without prior knowledge of the causal model. To tackle these challenges, we propose a novel framework CLEAR which aims to generate counterfactual explanations on graphs for graph-level prediction models. Specifically, CLEAR leverages a graph variational autoencoder based mechanism to facilitate its optimization and generalization, and promotes causality by leveraging an auxiliary variable to better identify the causal model. Extensive experiments on both synthetic and real-world graphs validate the superiority of CLEAR over state-of-the-art counterfactual explanation methods on graphs in different aspects. 
</details>

#### [Diffusion Visual Counterfactual Explanations (Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein)](https://papers.nips.cc/paper_files/paper/2022/hash/025f7165a452e7d0b57f1397fed3b0fd-Abstract-Conference.html)
<details>
    <summary>
        視覚的反実仮想説明（Visual Counterfactual Explanations: VCEs）は、画像分類器の決定を理解するための重要なツールである。 VCEとは、分類器の判定を変化させる、"小さい "が "現実的な "画像の意味的変化のことである。 VCEの生成のための現在のアプローチは、敵対的に頑健なモデルに限定され、しばしば非現実的な人工物を含むか、クラス数の少ない画像分類問題に限定される。 本論文では、拡散過程を介して任意のImageNet分類器に対して拡散視覚的反事実説明（DVCE）を生成することで、これを克服する。 拡散過程に対する2つの改良が、我々のDVCEの鍵となる。第一に、ハイパーパラメータが画像やモデル間で汎化される適応的パラメタリゼーションと、距離正則化と拡散過程の後期開始により、元の画像に対する意味的変化は最小だが分類は異なる画像を生成することができる。 第二に、逆境に強いモデルを介した我々のコーン正則化により、拡散過程が些細な非意味的変化に収束することなく、分類器による高い信頼性を達成するターゲットクラスの現実的な画像を生成することが保証される。
    </summary>
    Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are “small” but “realistic” semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.
</details>


### [ICML2022](https://dblp.org/db/conf/icml/icml2022.html)
#### [On the Adversarial Robustness of Causal Algorithmic Recourse (Ricardo Dominguez-Olmedo, Amir H Karimi, Bernhard Schölkopf)](https://proceedings.mlr.press/v162/dominguez-olmedo22a.html)
<details>
    <summary>
        アルゴリズムによる救済は、自動意思決定システムによる不利な分類結果を克服するために、個人に対して実行可能な推奨を提供することを目指す。 リコースの推奨は、リコースを求める個人の特徴における適度に小さな不確実性に対してロバストであることが理想的である。 本研究では、敵対的にロバストな再コース問題を定式化し、最小限のコストで再コースを提供する再コース手法がロバストでないことを示す。 次に、線形および微分可能な分類器に対して、敵対的にロバストな遡求を生成する方法を示す。 最後に、意思決定分類器を局所的に線形に振る舞い、行動可能な特徴により強く依存するように正則化することで、敵対的にロバストなリコースの存在が容易になることを示す。
    </summary>
    Algorithmic recourse seeks to provide actionable recommendations for individuals to overcome unfavorable classification outcomes from automated decision-making systems. Recourse recommendations should ideally be robust to reasonably small uncertainty in the features of the individual seeking recourse. In this work, we formulate the adversarially robust recourse problem and show that recourse methods that offer minimally costly recourse fail to be robust. We then present methods for generating adversarially robust recourse for linear and for differentiable classifiers. Finally, we show that regularizing the decision-making classifier to behave locally linearly and to rely more strongly on actionable features facilitates the existence of adversarially robust recourse.
</details>

#### [Meaningfully debugging model mistakes using conceptual counterfactual explanations (Abubakar Abid, Mert Yuksekgonul, James Zou)](https://proceedings.mlr.press/v162/abid22a.html)
<details>
    <summary>
        学習済みモデルが犯した間違いを理解し説明することは、ロバスト性の向上、コンセプト・ドリフトへの対処、バイアスの緩和など、多くの機械学習の目的にとって非常に重要である。 しかし、これは多くの場合、多くのテストサンプルにおけるモデルの間違いを手作業で調べ、それらの間違った予測の根本的な理由を推測する、その場限りのプロセスである。 例えば、このシマウマは縞模様が淡いので犬と誤分類された。 我々はCCEを、反事実的説明と概念活性化ベクトルという2つの先行アイデアに基づき、よく知られた事前学習済みモデルで検証し、モデルの間違いを有意義に説明できることを示す。 さらに、偽の相関を持つデータで訓練された新しいモデルに対して、CCEは、1つの誤分類されたテストサンプルから、モデルの間違いの原因として偽の相関を正確に特定する。 2つの困難な医療アプリケーションにおいて、CCEは臨床医によって確認された、実世界の設定でモデルが犯す偏りや間違いに関する有用な洞察を生み出した。 CCEのコードは公開されており、新しいモデルの誤りを説明するために簡単に適用することができる。
    </summary>
    Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model’s mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings. The code for CCE is publicly available and can easily be applied to explain mistakes in new models.
</details>

#### [Robust Counterfactual Explanations for Tree-Based Ensembles (Sanghamitra Dutta, Jason Long, Saumitra Mishra, Cecilia Tilli, Daniele Magazzeni)](https://proceedings.mlr.press/v162/dutta22a.html)
<details>
    <summary>
        反事実的説明は、機械学習モデルから望ましい結果を得るための方法を知らせる。 しかしながら、このような説明は、基礎となるモデルのある種の実世界の変化（例えば、モデルの再トレーニング、ハイパーパラメータの変更など）に対して頑健ではなく、例えば信用貸しなどのいくつかの応用において、その信頼性が疑問視されている。 本研究では、ツリーベースのアンサンブル（例えばXGBoost）に対してロバストな反事実を生成する、RobXと呼ぶ新しい戦略を提案する。 ツリーベースのアンサンブルは、ロバストな反事実生成においてさらなる課題をもたらす。例えば、非平滑で微分不可能な目的関数を持ち、非常に類似したデータに対する再学習によってパラメータ空間が大きく変化する可能性がある。 この指標は、再学習によるモデルの変化に対して、反事実がどの程度頑健であるかを定量化しようとするものであり、望ましい理論的特性を備えている。 我々の提案する戦略RobXは、どのような反事実生成手法（基本手法）でも動作し、我々の指標Counterfactual Stabilityを用いて基本手法で生成された反事実を反復的に改良することで、頑健な反事実を探索する。 ベンチマークデータセットにおいて、RobXの性能を（木ベースのアンサンブルの）一般的な反事実生成手法と比較する。 その結果、我々の戦略は、既存の最先端手法よりも有意に頑健（実際のモデル変更後の妥当性がほぼ100％）で、かつ現実的（局所的外れ値要因の観点から）な反事実を生成することが実証された。
    </summary>
    Counterfactual explanations inform ways to achieve a desired outcome from a machine learning model. However, such explanations are not robust to certain real-world changes in the underlying model (e.g., retraining the model, changing hyperparameters, etc.), questioning their reliability in several applications, e.g., credit lending. In this work, we propose a novel strategy - that we call RobX - to generate robust counterfactuals for tree-based ensembles, e.g., XGBoost. Tree-based ensembles pose additional challenges in robust counterfactual generation, e.g., they have a non-smooth and non-differentiable objective function, and they can change a lot in the parameter space under retraining on very similar data. We first introduce a novel metric - that we call Counterfactual Stability - that attempts to quantify how robust a counterfactual is going to be to model changes under retraining, and comes with desirable theoretical properties. Our proposed strategy RobX works with any counterfactual generation method (base method) and searches for robust counterfactuals by iteratively refining the counterfactual generated by the base method using our metric Counterfactual Stability. We compare the performance of RobX with popular counterfactual generation methods (for tree-based ensembles) across benchmark datasets. The results demonstrate that our strategy generates counterfactuals that are significantly more robust (nearly 100% validity after actual model changes) and also realistic (in terms of local outlier factor) over existing state-of-the-art methods.
</details>


### [IJCAI2022](https://dblp.org/db/conf/ijcai/ijcai2022.html)
#### [BayCon: Model-agnostic Bayesian Counterfactual Generator (Piotr Romashov, Martin Gjoreski, Kacper Sokol, Maria Vanina Martinez, Marc Langheinrich)](https://www.ijcai.org/proceedings/2022/104)
<details>
    <summary>
        仮想的な予測シナリオを発見するために反事実を生成することは、機械学習モデルとその予測を説明するためのデファクトスタンダードである。 しかし、時間効率が良く、スケーラブルで、モデルにとらわれない反実仮想説明器を構築することは、連続属性とカテゴリー属性に対応することに加えて、依然として未解決の課題である。 さらに問題を複雑にしているのは、対比インスタンスが特徴スパース性を最適化され、被説明インスタンスに近く、データ多様体の外から引き出されないことを保証することは、些細なことからは程遠いことである。 このギャップに対処するために、我々はBayConを提案する。BayConは確率的特徴サンプリングとベイズ最適化に基づいた新しい反事実生成器である。 このようなアプローチは、代用モデルを用いて反事実探索を導くことにより、複数の目的を組み合わせることができる。 我々は、3つの回帰タスクと3つの分類タスクを表す6つの実データセットに基づく実験を通して、我々の手法の利点を実証する。
    </summary>
    Generating counterfactuals to discover hypothetical predictive scenarios is the de facto standard for explaining machine learning models and their predictions. However, building a counterfactual explainer that is time-efficient, scalable, and model-agnostic, in addition to being compatible with continuous and categorical attributes, remains an open challenge. To complicate matters even more, ensuring that the contrastive instances are optimised for feature sparsity, remain close to the explained instance, and are not drawn from outside of the data manifold, is far from trivial. To address this gap we propose BayCon: a novel counterfactual generator based on probabilistic feature sampling and Bayesian optimisation. Such an approach can combine multiple objectives by employing a surrogate model to guide the counterfactual search. We demonstrate the advantages of our method through a collection of experiments based on six real-life datasets representing three regression tasks and three classification tasks.
</details>


### [KDD2022](https://dblp.org/db/conf/kdd/kdd2022.html)
#### [Framing Algorithmic Recourse for Anomaly Detection (Debanjan Datta, Feng Chen, Naren Ramakrishnan)](https://dl.acm.org/doi/10.1145/3534678.3539344)
<details>
    <summary>
        アルゴリズム再利用の問題は、教師あり機械学習モデルにおいて、意思決定支援システムから、より解釈しやすく、透明性が高く、ロバストな結果を提供するために検討されてきた。 未開拓の領域は、特に離散的な特徴値のみを持つ表形式データに対する異常検出のためのアルゴリズム的再コースである。 ここで問題となるのは、基礎となる異常検知モデルによって正常とみなされる反事実の集合を提示することであり、アプリケーションはこの情報を説明目的や対策推奨のために利用することができる。 我々は、効果的でスケーラブルであり、基礎となる異常検知モデルに依存しないアプローチ、Context preserving Algorithmic Recourse for Anomalies in Tabular data(CARAT)を提示する。 CARATは変換器ベースのエンコーダデコーダモデルを用い、尤度の低い特徴を見つけることで異常を説明する。 その後、異常なインスタンスにおける特徴の全体的な文脈を用いて、ハイライトされた特徴を修正することにより、意味的に首尾一貫した反事実が生成される。 広範な実験により、CARATの有効性が実証された。
    </summary>
    The problem of algorithmic recourse has been explored for supervised machine learning models, to provide more interpretable, transparent and robust outcomes from decision support systems. An unexplored area is that of algorithmic recourse for anomaly detection, specifically for tabular data with only discrete feature values. Here the problem is to present a set of counterfactuals that are deemed normal by the underlying anomaly detection model so that applications can utilize this information for explanation purposes or to recommend countermeasures. We present an approach-Context preserving Algorithmic Recourse for Anomalies in Tabular data(CARAT), that is effective, scalable, and agnostic to the underlying anomaly detection model. CARAT uses a transformer based encoder-decoder model to explain an anomaly by finding features with low likelihood. Subsequently semantically coherent counterfactuals are generated by modifying the highlighted features, using the overall context of features in the anomalous instance(s). Extensive experiments help demonstrate the efficacy of CARAT.
</details>

#### [Ask to Know More: Generating Counterfactual Explanations for Fake Claims (Shih-Chieh Dai, Yi-Li Hsu, Aiping Xiong, Lun-Wei Ku)](https://dl.acm.org/doi/10.1145/3534678.3539205)
<details>
    <summary>
        フェイクニュースが人々や世論に与える悪影響を軽減するために、真偽予測を迅速に行う自動ファクトチェックシステムが提案されている。 しかし、ほとんどの研究は、単にニュース記事の真実性を予測するシステムの真実性分類器に焦点を当てている。 我々は、効果的なファクトチェックは、予測に対する人々の理解にも依存していると考える。 本論文では、特定のニュースがフェイクであると認識された理由を人々が理解できるように、反実仮想的な説明を使ってファクトチェックの予測を解明することを提案する。 この研究では、フェイクニュースに対する反実仮想的な説明を生成するには、3つのステップが必要である：良い質問をすること、矛盾を見つけること、そして適切に推論することである。 私たちはこの研究課題を、質問応答（QA）を通じた矛盾含意推論として組み立てている。 我々はまず、虚偽の主張に対する質問を行い、関連する証拠文書から潜在的な答えを取り出す。 次に、含意分類器を用いて、虚偽の主張に対して最も矛盾する答えを特定する。 最後に、3つの異なる反事実説明形式を持つマッチしたQAペアを用いて、反事実説明を作成する。 FEVERデータセットを用いて、システム評価と人間評価の両方で実験を行った。 その結果、提案手法は、最新の手法と比較して、最も有用な説明を生成することが示唆された。 
    </summary>
    Automated fact-checking systems have been proposed that quickly provide veracity prediction at scale to mitigate the negative influence of fake news on people and on public opinion. However, most studies focus on veracity classifiers of those systems, which merely predict the truthfulness of news articles. We posit that effective fact checking also relies on people's understanding of the predictions. In this paper, we propose elucidating fact-checking predictions using counterfactual explanations to help people understand why a specific piece of news was identified as fake. In this work, generating counterfactual explanations for fake news involves three steps: asking good questions, finding contradictions, and reasoning appropriately. We frame this research question as contradicted entailment reasoning through question answering (QA). We first ask questions towards the false claim and retrieve potential answers from the relevant evidence documents. Then, we identify the most contradictory answer to the false claim by use of an entailment classifier. Finally, a counterfactual explanation is created using a matched QA pair with three different counterfactual explanation forms. Experiments are conducted on the FEVER dataset for both system and human evaluations. Results suggest that the proposed approach generates the most helpful explanations compared to state-of-the-art methods. Our code and data is publicly available https://github.com/yilihsu/AsktoKnowMore.
</details>


### [ICLR2022](https://dblp.org/db/conf/iclr/iclr2022.html)


### [AISTATS2022](https://dblp.org/db/conf/aistats/aistats2022.html)
#### [Counterfactual Explanation Trees: Transparent and Consistent Actionable Recourse with Decision Trees (Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike)](https://proceedings.mlr.press/v151/kanamori22a.html)
<details>
    <summary>
        反事実的説明（Counterfactual Explanation: CE）は、分類器の予測結果を変更するための摂動を提供する事後的説明手法である。 個人はこの摂動を、望ましい決定結果を得るための「行動」として解釈することができる。 既存のCE手法は、与えられた単一のインスタンスに対して最適化されたアクションを提供することに焦点を当てている。 しかし、これらのCE手法は、複数のインスタンスに同時にアクションを割り当てなければならないケースには対応していない。 このような場合、透明で一貫した方法で複数のインスタンスにアクションを割り当てるCEのフレームワークが必要となる。 本研究では、決定木を用いて効果的な行動を割り当てるCounterfactual Explanation Tree（CET）を提案する。 決定木の特性により、我々のCETは2つの利点を持つ：(1)透明性：行動を割り当てる理由が解釈可能な構造に要約される、(2)一貫性：これらの理由は互いに矛盾しない。 我々は2つのステップでCETを学習する： (i)複数のインスタンスに対して1つの有効なアクションを計算し、(ii)有効性と解釈可能性のバランスをとるためにインスタンスを分割する。 数値実験とユーザースタディにより、既存の手法と比較して本CETの有効性が実証された。
    </summary>
    Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. An individual can interpret the perturbation as an "action" to obtain the desired decision results. Existing CE methods focus on providing an action, which is optimized for a given single instance. However, these CE methods do not address the case where we have to assign actions to multiple instances simultaneously. In such a case, we need a framework of CE that assigns actions to multiple instances in a transparent and consistent way. In this study, we propose Counterfactual Explanation Tree (CET) that assigns effective actions with decision trees. Due to the properties of decision trees, our CET has two advantages: (1) Transparency: the reasons for assigning actions are summarized in an interpretable structure, and (2) Consistency: these reasons do not conflict with each other. We learn a CET in two steps: (i) compute one effective action for multiple instances and (ii) partition the instances to balance the effectiveness and interpretability. Numerical experiments and user studies demonstrated the efficacy of our CET in comparison with existing methods.
</details>

#### [CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks (Ana Lucic, Maartje A. Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, Fabrizio Silvestri)](https://proceedings.mlr.press/v151/lucic22a.html)
<details>
    <summary>
        グラフ・ニューラル・ネットワーク（GNN）が実世界のアプリケーションでますます期待されていることから、その予測を説明するための手法がいくつか開発されている。 GNNからの予測を解釈するための既存の方法は、主に特定の予測に特に関連する部分グラフを生成することに焦点を当てている。 しかし、このような方法は、本質的に反事実的（CF）ではない。つまり、ある予測が与えられたとき、別の結果を達成するために、予測をどのように変更できるかを理解したい。 この研究では、GNNのCF説明を生成するための方法を提案する：予測が変化するような入力（グラフ）データへの最小の摂動。 エッジの削除のみを用いることで、我々の手法CF-GNNExplainerは、GNN説明のために広く使われている3つのデータセットにおいて、平均3エッジ以下の削除で、少なくとも94の精度で、大半のインスタンスに対してCF説明を生成できることを発見した。 これは、CF-GNNExplainerが主に元の予測にとって重要な辺を除去し、最小限のCF説明をもたらすことを示している。
    </summary>
    Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94  accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.
</details>

#### [Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis (Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, Himabindu Lakkaraju)](https://proceedings.mlr.press/v151/pawelczyk22a.html)
<details>
    <summary>
        機械学習(ML)モデルが重要なアプリケーションに広く導入されるようになるにつれ、反事実的説明は、実用的なモデル説明を提供するための重要なツールとして浮上してきた。 反事実的説明の人気が高まっているにもかかわらず、これらの説明の理論的理解はまだ遅れている。 本研究では、反事実的説明を敵対的事例というレンズを通して体系的に分析する。 一般的な反実仮想説明と敵対的事例生成法の類似性を定式化し、両者が等価である条件を特定する。 そして、反実仮想的説明と敵対的事例生成手法の出力する解の上限を導出し、それをいくつかの実世界データセットで検証する。 反実仮想的説明と敵対的事例との間の理論的・経験的類似性を確立することにより、我々の研究は、既存の反実仮想的説明アルゴリズムの設計と開発に関する基本的な問題を提起する。
    </summary>
    As machine learning (ML) models becomemore widely deployed in high-stakes applications, counterfactual explanations have emerged as key tools for providing actionable model explanations in practice. Despite the growing popularity of counterfactual explanations, the theoretical understanding of these explanations is still lacking behind. In this work, we systematically analyze counterfactual explanations through the lens of adversarial examples. We do so by formalizing the similarities between popular counterfactual explanation and adversarial example generation methods identifying conditions when they are equivalent. We then derive upper bounds between the solutions output by counterfactual explanation and adversarial example generation methods, which we validate on several real world data sets. By establishing these theoretical and empirical similarities between counterfactual explanations and adversarial examples, our work raises fundamental questions about the design and development of existing counterfactual explanation algorithms.
</details>


### [AAAI2022](https://dblp.org/db/conf/aaai/aaai2022.html)
#### [Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models (Verma, S., Hines, K., & Dickerson, J. P.)](https://ojs.aaai.org/index.php/AAAI/article/view/20828)
<details>
    <summary>
        説明可能な機械学習(ML)は、MLベースのシステムが多くの分野で採用されるようになったため、近年注目を集めている。 アルゴリズム・リソース（AR）は、「もし入力データポイントがxの代わりにx'だったら、MLベースのシステムの出力はyの代わりにy'になるだろう」という形の「もしも」のフィードバックを提供する。 リコースは、その実用的なフィードバック、既存の法的枠組みへの適合性、基礎となるMLモデルへの忠実性から魅力的である。 しかし、現在のリコース・アプローチはシングル・ショットであり、つまり、xが1回の期間でx'に変化すると仮定している。 すなわち、xが中間状態を確率的に順次移動して最終状態x'に到達することを可能にするリコースである。 我々のアプローチはモデルにとらわれないブラックボックスである。 さらに、リソースの計算は、一度訓練されれば、再最適化の必要なく、複数のデータポイントに適用できるように償却される。 これらの主要な特徴に加え、我々のアプローチは、データ多様体への準拠、因果関係の尊重、過去の研究によりリソースの望ましい性質として特定されたスパース性などのオプション的な望みを満たす。 我々は3つの実世界のデータセットを用いて我々のアプローチを評価し、他のリコースの望ましい特性を尊重した逐次リコースの生成に成功したことを示す。
    </summary>
    Explainable machine learning (ML) has gained traction in recent years due to the increasing adoption of ML-based systems in many sectors. Algorithmic Recourses (ARs) provide "what if" feedback of the form "if an input datapoint were x' instead of x, then an ML-based system's output would be y' instead of y." Recourses are attractive due to their actionable feedback, amenability to existing legal frameworks, and fidelity to the underlying ML model. Yet, current recourse approaches are single shot that is, they assume x can change to x' in a single time period. We propose a novel stochastic-control-based approach that generates sequential recourses, that is, recourses that allow x to move stochastically and sequentially across intermediate states to a final state x'. Our approach is model agnostic and black box. Furthermore, the calculation of recourses is amortized such that once trained, it applies to multiple datapoints without the need for re-optimization. In addition to these primary characteristics, our approach admits optional desiderata such as adherence to the data manifold, respect for causal relations, and sparsity identified by past research as desirable properties of recourses. We evaluate our approach using three real-world datasets and show successful generation of sequential recourses that respect other recourse desiderata.
</details>

#### [On the Fairness of Causal Algorithmic Recourse (Kügelgen, J. von, Karimi, A.-H., Bhatt, U., Valera, I., Weller, A., & Schölkopf, B.)](https://ojs.aaai.org/index.php/AAAI/article/view/21192)
<details>
    <summary>
        アルゴリズムの公正さは、一般的に予測の観点から研究されている。 その代わりに、ここでは不利な分類を是正するために個人に提案される救済措置の観点から公平性を調査する。 この基準は、決定境界からの平均的なグループ間距離を等しくする先行研究とは異なり、特徴間の因果関係を明示的に考慮することで、物理的世界で実行される遡及行動の下流効果を捉える。 我々は、我々の基準が、反実仮想の公正さなどの他の基準とどのように関連するかを探求し、再履行の公正さが予測の公正さと相補的であることを示す。 また、分類器を変更することによって公正な因果的遡及を強制する方法を理論的・実証的に研究し、Adultデータセットを用いたケーススタディを行う。 最後に、我々の基準によって明らかになったデータ生成過程における公正違反は、分類器に対する制約とは対照的に、社会的介入によってよりよく対処される可能性があるかどうかを議論する。
    </summary>
    Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which—unlike prior work on equalising the average group-wise distance from the decision boundary—explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.
</details>

#### [FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles (Lucic, A., Oosterhuis, H., Haned, H., & Rijke, M. de. )](https://ojs.aaai.org/index.php/AAAI/article/view/20468)
<details>
    <summary>
        機械学習（ML）では、アルゴリズムによる決定が人間に与える影響が大きくなっているため、モデルの解釈可能性が重要な問題になっている。 反実仮想的な説明は、MLモデルが特定の決定を行う理由だけでなく、これらの決定をどのように変更することができるかをユーザに理解させることができる。 我々は、反事実的な説明を見つけるという問題を最適化タスクとしてとらえ、微分可能なモデルにしか適用できなかった従来の研究を拡張する。 ツリーアンサンブルのような微分不可能なモデルに対応するため、最適化の枠組みにおいて確率的モデル近似を用いる。 我々は、元のモデルの予測に対する反事実的説明を見つけるのに有効な近似手法を導入し、我々の反事実的例が、木アンサンブルのために特別に設計された他の手法によって生成されたものよりも、元の事例に著しく近いことを示す。
    </summary>
    Model interpretability has become an important problem in machine learning (ML) due to the increased effect algorithmic decisions have on humans. Counterfactual explanations can help users understand not only why ML models make certain decisions, but also how these decisions can be changed. We frame the problem of finding counterfactual explanations as an optimization task and extend previous work that could only be applied to differentiable models. In order to accommodate non-differentiable models such as tree ensembles, we use probabilistic model approximations in the optimization framework. We introduce an approximation technique that is effective for finding counterfactual explanations for predictions of the original model and show that our counterfactual examples are significantly closer to the original instances than those produced by other methods specifically designed for tree ensembles.
</details>

#### [Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates (Ley, D., Bhatt, U., & Weller, A.)](https://ojs.aaai.org/index.php/AAAI/article/view/20702)
<details>
    <summary>
        微分可能確率モデルからの不確実性推定値を解釈するために、最近の研究では、モデルが不確実である所与のデータ点に対して、単一のCounterfactual Latent Uncertainty Explanation (CLUE)を生成することが提案されている。 我々は、δ-CLUE（潜在空間における元の入力のδ球以内の潜在的なCLUEの集合）を検討するために探索を広げる。 このような集合の多様性を研究し、多くのCLUEが冗長であることを発見する。そのため、入力に関連する不確実性を減少させる方法について、それぞれが明確な説明を提案するCLUEの集合であるDIVerse CLUE（∇-CLUE）を提案する。 さらに、我々はGLobal AMortised CLUE（GLAM-CLUE）を提案する。これは、不確実な入力の特定のグループに適用されるamortisedマッピングを学習し、それらを取り込み、単一の関数呼び出しで効率的にモデルが確実な入力に変換する、独特で新しい手法である。 我々の実験によると、δ-CLUE、∇-CLUE、GLAM-CLUEはいずれもCLUEの欠点に対処し、実務家に不確実性の推定について有益な説明を提供する。
    </summary>
    To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating a single Counterfactual Latent Uncertainty Explanation (CLUE) for a given data point where the model is uncertain. We broaden the exploration to examine δ-CLUE, the set of potential CLUEs within a δ ball of the original input in latent space. We study the diversity of such sets and find that many CLUEs are redundant; as such, we propose DIVerse CLUE (∇-CLUE), a set of CLUEs which each propose a distinct explanation as to how one can decrease the uncertainty associated with an input. We then further propose GLobal AMortised CLUE (GLAM-CLUE), a distinct, novel method which learns amortised mappings that apply to specific groups of uncertain inputs, taking them and efficiently transforming them in a single function call into inputs for which a model will be certain. Our experiments show that δ-CLUE, ∇-CLUE, and GLAM-CLUE all address shortcomings of CLUE and provide beneficial explanations of uncertainty estimates to practitioners.
</details>


### [FAccT2022](https://dblp.org/db/conf/fat/facct2022.html)
#### [Counterfactual Shapley Additive Explanations (Emanuele Albini, Jason Long, Danial Dervovic, Daniele Magazzeni)](https://dl.acm.org/doi/10.1145/3531146.3533168)
<details>
    <summary>
        特徴帰属は、モデルへの各入力特徴に対して単一の数値スコアを割り当てるという単純さゆえに、モデル説明のための一般的なパラダイムである。 説明の目標がモデルの消費者の結果を改善することである、実行可能なリコースの設定では、特徴帰属がどのように正しく使用されるべきかはしばしば不明確である。 本研究では、行動可能なリコースと特徴帰属の関連を強化・明確化することを目的とする。 具体的には、SHAPの変形であるCounterfactual SHAP(CF-SHAP)を提案する。これは、限界(介入)シャプレー値の枠組みで使用するための背景データセットを生成するために、反事実情報を組み込んだものである。 このCF-SHAPは、シャプレー値フレームワークで使用するための背景データセットを生成するために、反事実情報を組み込んだものである。 さらに、特徴帰属のための定量的スコアである反実仮想可能性を提案し、正当化することで、CF-SHAPの有効性を実証し、この指標で測定されるように、CF-SHAPは、木アンサンブルを用いた公開データセットで評価された場合、既存の手法よりも優れていることを示す。
    </summary>
    Feature attributions are a common paradigm for model explanations due to their simplicity in assigning a single numeric score for each input feature to a model. In the actionable recourse setting, wherein the goal of the explanations is to improve outcomes for model consumers, it is often unclear how feature attributions should be correctly used. With this work, we aim to strengthen and clarify the link between actionable recourse and feature attributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP (CF-SHAP), that incorporates counterfactual information to produce a background dataset for use within the marginal (a.k.a. interventional) Shapley value framework. We motivate the need within the actionable recourse setting for careful consideration of background datasets when using Shapley values for feature attributions with numerous synthetic examples. Moreover, we demonstrate the efficacy of CF-SHAP by proposing and justifying a quantitative score for feature attributions, counterfactual-ability, showing that as measured by this metric, CF-SHAP is superior to existing methods when evaluated on public datasets using tree ensembles.
</details>

#### [DualCF: Efficient Model Extraction Attack from Counterfactual Explanations (Yongjie Wang, Hangwei Qian, Chunyan Miao)](https://dl.acm.org/doi/10.1145/3531146.3533188)
<details>
    <summary>
        クラウドサービスプロバイダーは、ユーザーがAPIを介して大規模なクラウドベースのモデルにアクセスできるように、Machine-Learning-as-a-Service（MLaaS）プラットフォームを立ち上げた。 これらのAPIは、予測出力に加えて、反事実説明（CF）のような、より人間が理解しやすい方法で他の情報を提供することもできる。 しかし、このような余分な情報は必然的に、クラウド上のモデルの内部機能を盗むことを目的とした抽出攻撃に対して、クラウドモデルをより脆弱にする。 しかし、クラウドモデルのブラックボックス的な性質により、既存の攻撃戦略では、代替モデルが高い忠実度を達成するまでに、膨大な数のクエリが必然的に必要となる。 本論文では、分類モデルを盗むためのクエリ効率を大幅に向上させる、シンプルかつ効率的な新しいクエリ戦略を提案する。 これは、現在のクエリ戦略が、遠く離れたクエリと境界の近いCFを代替モデル学習に取り込むことによって引き起こされる決定境界シフトの問題に苦しんでいるという我々の観察によって動機付けられる。 これは、CFだけでなく、CFの反事実的説明（CCF）も代替モデルの学習サンプルのペアとして取り込むことによって達成される。 合成データセットと実世界データセットの両方を用いて、広範かつ包括的な実験評価を行った。 実験結果は、DualCFがより少ないクエリで高忠実度のモデルを効率的かつ効果的に生成できることを好意的に示している。
    </summary>
    Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.
</details>

#### [Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations (Ruoxi Shang, K. J. Kevin Feng, Chirag Shah)](https://dl.acm.org/doi/10.1145/3531146.3533189)
<details>
    <summary>
        日常的なインテリジェントアプリケーションは、一般的に自動レコメンダーシステム(RS)に依存している。 RSの複雑化とアルゴリズムによる意思決定の透明性の欠如のため、研究者は説明によってユーザーをサポートする必要性を認識している。 従来の説明可能なAI手法の多くは、推薦システムの内部的な複雑さを開示するには不十分である一方、反実仮想的説明は、既存の推薦と代替案を対比する人間のような説明を提供することで、多くの望ましい説明可能な特徴を提供する。 しかしながら、日常的な知的アプリケーションの使用におけるユーザの反実仮想的説明のニーズを理解するための実証的研究は不足している。 本論文では、質問駆動型のアプローチを通じて、日常的なレコメンデーションによる人々の意思決定を支援するために、いつ、どのような反実仮想的説明を提供すべきかを調査する。 既存の説明がどのようにユーザーをサポートするのに不十分なのかを理解し、なぜそうしないのかという質問や追加説明を求めるように促すきっかけを引き出すために、予備的な調査研究とインタビュー研究を実施した。 その結果、意思決定の有用性が、ユーザーの反実仮想的な情報ニーズに影響を与える主要な要因であることが明らかになった。 次に、効用と説明ニーズの相関関係を定量化するために、オンライン・シナリオ・ベースの調査を実施し、測定された変数間に有意な相関関係を見出した。
    </summary>
    Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.
</details>

#### [What is the Bureaucratic Counterfactual? Categorical versus Algorithmic Prioritization in U.S. Social Policy (Rebecca Ann Johnson, Simone Zhang)](https://dl.acm.org/doi/10.1145/3531146.3533223)
<details>
    <summary>
        政府が利害関係の強い決定を下すためにアルゴリズムを使用することへの懸念が高まっている。 初期の研究では、刑罰や疑惑を割り当てるためにリスクを予測するアルゴリズムに焦点が当てられていたが、最近の研究では、ホームレスの経験者を住居の必要性でランク付けするなど、有益な資源に的を絞るために「必要性」や「利益」を予測するアルゴリズムが研究されている。 本稿では、社会政策におけるアルゴリズムの役割に関する既存の研究が、「社会サービス官僚機構が誰を助けるべきかについて何らかの決定を下す必要がある場合、アルゴリズムはどのような現状の優先順位決定法に取って代わるだろうか」という反実仮想的な視点から利益を得ることができると論じている。 人間による意思決定とアルゴリズムによる意思決定を対比させる研究は数多くあるが、社会サービス官僚機構は、市井の官僚に完全な裁量を与えることで、援助の対象を決めているわけではない。 その代わりに、アルゴリズム以前の、ルールに基づいた方法によって支援を行うことが主である。 そこでは、意思決定者が手作業で、(1)援助を求める人のどの属性が優先されるべきかを決定し、(2)必要性の連続的な尺度をカテゴリーに単純化し（例えば、世帯収入が閾値を下回る）、(3)カテゴリーを優先レベルに対応付ける決定ルールを手作業で選択する。 我々は、新しいデータと量的・質的社会科学的方法を用いて、米国の社会政策の2つのケーススタディにおけるカテゴリー別優先順位付けについて概説する。 優先順位付けの根拠は公式化されているのか、優先順位付けにおいて権力はどのような役割を果たすのか、優先順位の決定ルールは手作業で選択されるのか、それとも予測モデルから帰納的に導き出されるのか。 最後に、反実仮想的な視点が、社会政策におけるカテゴリー別優先順位付けの控えめなコストと、不平等を縮小するための予測アルゴリズムの控えめな可能性の両方を強調することを示す。
    </summary>
    There is growing concern about governments’ use of algorithms to make high-stakes decisions. While an early wave of research focused on algorithms that predict risk to allocate punishment and suspicion, a newer wave of research studies algorithms that predict “need” or “benefit” to target beneficial resources, such as ranking those experiencing homelessness by their need for housing. The present paper argues that existing research on the role of algorithms in social policy could benefit from a counterfactual perspective that asks: given that a social service bureaucracy needs to make some decision about whom to help, what status quo prioritization method would algorithms replace? While a large body of research contrasts human versus algorithmic decision-making, social service bureaucracies target help not by giving street-level bureaucrats full discretion. Instead, they primarily target help through pre-algorithmic, rule-based methods. In this paper, we outline social policy’s current status quo method—categorical prioritization—where decision-makers manually (1) decide which attributes of help seekers should give those help seekers priority, (2) simplify any continuous measures of need into categories (e.g., household income falls below a threshold), and (3) manually choose the decision rules that map categories to priority levels. We draw on novel data and quantitative and qualitative social science methods to outline categorical prioritization in two case studies of United States social policy: waitlists for scarce housing vouchers and K-12 school finance formulas. We outline three main differences between categorical and algorithmic prioritization: is the basis for prioritization formalized; what role does power play in prioritization; and are decision rules for priority manually chosen or inductively derived from a predictive model. Concluding, we show how the counterfactual perspective underscores both the understudied costs of categorical prioritization in social policy and the understudied potential of predictive algorithms to narrow inequalities.
</details>

#### [Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting (Ulrike Kuhl, André Artelt, Barbara Hammer)](https://dl.acm.org/doi/10.1145/3531146.3534630)
<details>
    <summary>
        説明可能な人工知能（XAI）のための心理学的根拠のあるソリューションとして、「反実仮想説明（CFE）」は大きな支持を得ている。 最近の技術革新は、自動的に生成されるsに確からしさという概念を導入し、確からしい説明のみを作成することによって、sの頑健性を高めている。 しかし、この制約がユーザーエクスペリエンスに与える実用的な利点はまだ明らかではない。 本研究では、反復学習タスクにおける、もっともらしいSの客観的・主観的な使いやすさを評価する。 抽象的なシナリオを中心とした、ゲーム的な実験デザインを用いた。 その結果、初心者は、望ましい結果につながる最小限の変化を引き起こすような、最も近いものではなく、もっともらしいものを受け取った方が、あまり得をしないことが示された。 ゲーム後のアンケートでは、主観的な使いやすさについては、両グループ間に差は見られなかった。 心理的妥当性を比較類似性とみなす考え方に従えば、最も近い条件のユーザーは、計算上妥当な相手よりも心理的妥当性が高いと経験する可能性がある。 まとめると、我々の研究は、計算上のもっともらしさと心理的なもっともらしさの定義のあまり考慮されていない乖離を浮き彫りにし、XAIの設計段階ですでに人間の行動、嗜好、メンタルモデルを取り入れる必要性を決定的に確認した。 
    </summary>
    Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo
</details>



## 2021
### [NeurIPS2021](https://dblp.org/db/conf/nips/neurips2021.html)
#### [Towards Robust and Reliable Algorithmic Recourse
 (Sohini Upadhyay, Shalmali Joshi, Himabindu Lakkaraju)](https://proceedings.neurips.cc/paper/2021/hash/8ccfb1140664a5fa63177fb6e07352f0-Abstract.html)
<details>
    <summary>
        予測モデルが、リスクの高い意思決定（ローンの承認など）にますます導入されるようになるにつれ、影響を受ける個人に救済を提供するポストホックテクニックへの関心が高まっている。 これらの技法は、基礎となる予測モデルが変更されないという仮定のもとで遡及を生成する。 しかし実際には、モデルは様々な理由（例えばデータセットのシフト）によって定期的に更新されることが多く、それによって以前に規定されたリコースは有効でなくなる。この問題に対処するために、我々は、モデルのシフトにロバストなリコースを見つけるための敵対的学習を活用する、新しいフレームワークRObust Algorithmic Recourse（ROAR）を提案する。 我々の知る限り、この重要な問題に対する初めての解決策を提案するものである。 1)モデルシフトを考慮せずに生成されたコースが無効になる確率を定量化する。 2) 本フレームワークによって出力されるロバストなリコースによって発生する追加コストが有限であることを証明する。 複数の合成データセットと実世界データセットを用いた実験評価により、提案するフレームワークの有効性を実証する。
    </summary>
    As predictive models are increasingly being deployed in high-stakes decision making (e.g., loan approvals), there has been growing interest in post-hoc techniques which provide recourse to affected individuals. These techniques generate recourses under the assumption that the underlying predictive model does not change. However, in practice, models are often regularly updated for a variety of reasons (e.g., dataset shifts), thereby rendering previously prescribed recourses ineffective.To address this problem, we propose a novel framework, RObust Algorithmic Recourse (ROAR), that leverages adversarial training for finding recourses that are robust to model shifts. To the best of our knowledge, this work proposes the first ever solution to this critical problem. We also carry out theoretical analysis which underscores the importance of constructing recourses that are robust to model shifts: 1) We quantify the probability of invalidation for recourses generated without accounting for model shifts. 2) We prove that the additional cost incurred due to the robust recourses output by our framework is bounded. Experimental evaluation on multiple synthetic and real-world datasets demonstrates the efficacy of the proposed framework.
</details>

#### [Learning Models for Actionable Recourse (Alexis Ross, Himabindu Lakkaraju, Osbert Bastani)](https://proceedings.neurips.cc/paper/2021/hash/9b82909c30456ac902e14526e63081d4-Abstract.html)
<details>
    <summary>
        機械学習モデルが、法律や金融の意思決定のような利害の大きい領域でますます導入されるようになるにつれて、反事実的説明を生成するためのポストホックメソッドへの関心が高まっている。 このような説明は、予測された結果によって不利な影響を受ける個人（例えば、融資を拒否された申込者）に、救済手段-すなわち、肯定的な結果を得るために自分の特徴をどのように変更できるかの説明-を提供する。 我々は、敵対的な訓練とPAC信頼度セットを活用し、精度を犠牲にすることなく、理論的に高い確率で影響を受けた個人への遡及を保証するモデルを学習する新しいアルゴリズムを提案する。 実データを用いた広範な実験により、本アプローチの有効性を実証する。
    </summary>
    As machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals adversely impacted by predicted outcomes (e.g., an applicant denied a loan) with recourse---i.e., a description of how they can change their features to obtain a positive outcome. We propose a novel algorithm that leverages adversarial training and PAC confidence sets to learn models that theoretically guarantee recourse to affected individuals with high probability without sacrificing accuracy. We demonstrate the efficacy of our approach via extensive experiments on real data.
</details>

#### [Counterfactual Explanations in Sequential Decision Making Under Uncertainty (Stratis Tsirtsis, Abir De, Manuel Rodriguez)](https://proceedings.neurips.cc/paper/2021/hash/fd0a5a5e367a0955d81278062ef37429-Abstract.html)
<details>
    <summary>
        反事実的説明を見つける方法は、主に一段階の意思決定過程に焦点を当ててきた。 本研究では、複数の依存的な行動が時間をかけて連続的に実行される意思決定プロセスに対する反事実的説明を見つける手法の開発に着手する。 まず、有限地平マルコフ決定過程とGumbel-Max構造的因果モデルを用いて、一連の行動と状態を正式に特徴付けることから始める。 この特徴付けを基に、逐次的な意思決定過程に対する反事実的説明を求める問題を定式化する。 我々の問題定式化では、反実仮想的説明は、観察されたプロセス実現をより良い結果に導くことができた、観察されたシーケンスから最大kアクションだけ異なるアクションの代替シーケンスを指定する。 そして、反実仮想環境ダイナミクスの全ての可能な実現に対して、常に最適な反実仮想説明を提供することが保証される反実仮想政策を構築する、動的計画法に基づく多項式時間アルゴリズムを導入する。 我々は、認知行動療法からの合成データと実データの両方を用いてアルゴリズムを検証し、我々のアルゴリズムが見出す反事実的説明が、不確実性の下での逐次的意思決定を強化するための貴重な洞察を提供できることを示す。
    </summary>
    Methods to find counterfactual explanations have predominantly focused on one-step decision making processes. In this work, we initiate the development of methods to find counterfactual explanations for decision making processes in which multiple, dependent actions are taken sequentially over time. We start by formally characterizing a sequence of actions and states using finite horizon Markov decision processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally state the problem of finding counterfactual explanations for sequential decision making processes. In our problem formulation, the counterfactual explanation specifies an alternative sequence of actions differing in at most k actions from the observed sequence that could have led the observed process realization to a better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation on every possible realization of the counterfactual environment dynamics. We validate our algorithm using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations our algorithm finds can provide valuable insights to enhance sequential decision making under uncertainty.
</details>

#### [Counterfactual Explanations Can Be Manipulated (Dylan Slack, Anna Hilgard, Himabindu Lakkaraju, Sameer Singh)](https://proceedings.neurips.cc/paper/2021/hash/009c434cab57de48a31f6b669e7ba266-Abstract.html)
<details>
    <summary>
        反実仮想的な説明は、アルゴリズムによる決定によって不利な影響を受けた個人に救済手段を提供するための魅力的な選択肢として浮上している。 反実仮想的説明が重要な用途（法執行、金融融資など）に導入されるにつれ、こうした手法の脆弱性を明確に理解し、それに対処する方法を見出すことが重要になっている。 しかし、反事実的説明の脆弱性や欠点についてはほとんど理解されていない。 本研究では、反事実的説明の脆弱性を説明し、それをどのように操作できるかを示す初めてのフレームワークを紹介する。 より具体的には、反事実的説明が、小さな摂動下で大きく異なる反事実に収束する可能性があることを示し、反事実的説明が頑健でないことを示す。 この知見を活用し、わずかな摂動下で反事実説明がより低コストの代替手段を見つける、一見公平なモデルを訓練するための新しい目的を導入する。 このようなモデルが、監査人には公正に見えながら、データ中の特定のサブグループには不当に低コストの救済手段を提供できることを説明する。 ローンデータセットと凶悪犯罪予測データセットで実験を行ったところ、摂動下で特定のサブグループが最大20倍低いコストで救済を受けることができた。 これらの結果は、現在の反事実的説明手法の信頼性に関する懸念を提起するものであり、ロバストな反事実的説明の研究を促すものである。
    </summary>
    Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions. As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust. Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation. We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.
</details>

#### [Robust Counterfactual Explanations on Graph Neural Networks (Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, Yong Zhang)](https://proceedings.neurips.cc/paper/2021/hash/2c8c3a57383c63caef6724343eb62257-Abstract.html)
<details>
    <summary>
        グラフニューラルネットワーク(GNN)の大量導入は、ノイズに頑健で、人間の直感に合致した説明に対する強い要求を生み出す。 ほとんどの既存の手法は、予測と強い相関を持つ入力グラフのサブグラフを特定することによって説明を生成する。 このような説明はノイズに対してロバストではない。なぜなら、1つの入力に対して独立に相関を最適化すると、簡単にノイズをオーバーフィットしてしまうからである。 さらに、入力グラフから同定された部分グラフを削除しても予測結果が変わるとは限らないため、反実仮想的でもない。 本論文では、類似の入力グラフに対するGNNの共通の決定論理を明示的にモデル化することにより、GNN上のロバストな反事実説明を生成する新しい手法を提案する。 我々の説明は、多くの類似入力グラフの予測を支配するGNNの共通決定境界から生成されるため、当然ノイズに頑健である。 また、入力グラフから説明によって識別されたエッジの集合を削除することで、予測が大きく変化するため、説明は反事実的である。 多くの公開データセットを用いた徹底的な実験により、我々の手法の優れた性能が実証された。
    </summary>
    Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they are not counterfactual because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations are also counterfactual because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.
</details>


### [ICML2021](https://dblp.org/db/conf/icml/icml2021.html)
#### [Optimal Counterfactual Explanations in Tree Ensembles (Axel Parmentier, Thibaut Vidal)](https://proceedings.mlr.press/v139/parmentier21a.html)
<details>
    <summary>
        反実仮想的な説明は通常、探索の初期条件に敏感なヒューリスティックによって生成される。 性能と頑健性が保証されていないことが、信頼性の妨げとなっている。 本論文では、樹木群に対する反事実的説明に対する規律あるアプローチをとる。 我々は「最適」な説明を目指すモデルベースの探索を提唱し、効率的な混合整数計画法を提案する。 孤立林を我々のフレームワークでモデル化し、外れ値スコアの低いもっともらしい説明に探索を集中させることができることを示す。 我々は、重要な目的、異種データ型、特徴空間の構造的制約、リソースと行動可能性の制約をモデル化する追加制約を包括的にカバーする。 我々の実験的分析により、提案する探索手法は、従来の数理計画アルゴリズムよりも桁違いに少ない計算量で済むことが実証された。 また、大規模なデータセットや樹木群に対してもスケールアップが可能であり、最適に解かれた明確なモデルに基づいた系統的な説明を数秒以内に提供することができる。
    </summary>
    Counterfactual explanations are usually generated through heuristics that are sensitive to the search’s initial conditions. The absence of guarantees of performance and robustness hinders trustworthiness. In this paper, we take a disciplined approach towards counterfactual explanations for tree ensembles. We advocate for a model-based search aiming at "optimal" explanations and propose efficient mixed-integer programming approaches. We show that isolation forests can be modeled within our framework to focus the search on plausible explanations with a low outlier score. We provide comprehensive coverage of additional constraints that model important objectives, heterogeneous data types, structural constraints on the feature space, along with resource and actionability restrictions. Our experimental analyses demonstrate that the proposed search approach requires a computational effort that is orders of magnitude smaller than previous mathematical programming algorithms. It scales up to large data sets and tree ensembles, where it provides, within seconds, systematic explanations grounded on well-defined models solved to optimality.
</details>


### [IJCAI2021](https://dblp.org/db/conf/ijcai/ijcai2021.html)
#### [Counterfactual Explanations for Optimization-Based Decisions in the Context of the GDPR (Anton Korikov, Alexander Shleyfman, J. Christopher Beck)](https://www.ijcai.org/proceedings/2021/564)
<details>
    <summary>
        一般データ保護規則（GDPR）は、自動化された決定に対する説明を個人に求めている。 このような説明の形式、理解可能性、さらには存在さえも未解決の問題であり、説明可能なAIの一部として研究されている。 我々は、反事実的説明のアプローチを採用し、それを宣言的最適化モデルによる意思決定に適用する。 逆組合せ最適化は特に反事実的説明に適しているが、計算上の困難さと比較的新しい文献がその適用を困難にしていることを論じる。 この問題を解決するために、ある個体における最小限の差異を分離する反事実的説明のケースを取り上げる。 つの一般的な最適化関数の下では、完全な逆最適化は不要であることを示す。 特に、重み付きMaxSATのようなフレームワークを含む、重み付き2値変数の和の形の関数については、元の最適化モデルを少し修正したものを解くことで解が求まることを示す。 対照的に、重み付き整数変数の和は、元のモデルに対する一連の修正に対する二項探索で解くことができる。
    </summary>
    The General Data Protection Regulations (GDPR) entitle individuals to explanations for automated decisions. The form, comprehensibility, and even existence of such explanations remain open problems, investigated as part of explainable AI. We adopt the approach of counterfactual explanations and apply it to decisions made by declarative optimization models. We argue that inverse combinatorial optimization is particularly suited for counterfactual explanations but that the computational difficulties and relatively nascent literature make its application a challenge. To make progress, we address the case of counterfactual explanations that isolate the minimal differences for an individual. We show that under two common optimization functions, full inverse optimization is unnecessary. In particular, we show that for functions of the form of the sum of weighted binary variables, which includes frameworks such as weighted MaxSAT, a solution can be found by solving a slightly modified version of the original optimization model. In contrast, the sum of weighted integer variables can be solved with a binary search over a series of modifications to the original model.
</details>

#### [If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques (Mark T. Keane, Eoin M. Kenny, Eoin Delaney, Barry Smyth)](https://www.ijcai.org/proceedings/2021/609)
<details>
    <summary>
        近年、わかりやすいAI（XAI）の解決策として、反実仮想的説明に関するAI研究が爆発的に進んでいる。 これらの説明は、他の説明手法に比べて技術的、心理的、法的な利点を提供するように思われる。 我々は、文献で報告されている100の異なる反実仮想説明法を調査した。 この調査では、これらの方法が心理学的、計算学的にどの程度適切に評価されているかを取り上げ、不足している点を定量化する。 例えば、これらの手法のうち21％しかユーザーテストが行われていない。 これらの手法の評価における5つの重要な欠陥について詳述し、標準化されたベンチマーク評価によるロードマップを提案する。
    </summary>
    In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.
</details>


### [KDD2021](https://dblp.org/db/conf/kdd/kdd2021.html)
#### [Counterfactual Explanations in Explainable AI: A Tutorial (Cong Wang, Xiao-Hui Li, Haocheng Han, Shendi Wang, Luning Wang, Caleb Chen Cao, Lei Chen)](https://dl.acm.org/doi/10.1145/3447548.3470797)
<details>
    <summary>
        ディープラーニングは多くの分野で強力な性能を示しているが、そのブラックボックス的な性質がさらなる応用を妨げている。 これに対して、深層学習モデルの予測や振る舞いを説明することを目的とした、説明可能な人工知能が登場した。 このチュートリアルでは、反実仮想的説明の認知的概念と特徴、その計算形式、主な手法、様々な説明設定における適応について紹介する。 さらに、一般的な研究分野における反事実的説明の典型的な使用例をいくつか示します。 最後に、実践を踏まえて、データ補強や会話システムのような反事実的説明の潜在的な応用について概説する。 このチュートリアルが、参加者が反事実的説明の概要を理解する一助となれば幸いである。
    </summary>
    Deep learning has shown powerful performances in many fields, however its black-box nature hinders its further applications. In response, explainable artificial intelligence emerges, aiming to explain the predictions and behaviors of deep learning models. Among many explanation methods, counterfactual explanation has been identified as one of the best methods due to its resemblance to human cognitive process: to deliver an explanation by constructing a contrastive situation so that human may interpret the underlying mechanism by cognitively demonstrating the difference. In this tutorial, we will introduce the cognitive concept and characteristics of counterfactual explanation, its computational form, mainstream methods, and various adaptation in terms of different explanation settings. In addition, we will demonstrate several typical use cases of counterfactual explanations in popular research areas. Finally, in light of practice, we outline the potential applications of counterfactual explanations like data augmentation or conversation system. We hope this tutorial can help the participants get an overview sense of counterfactual explanations.
</details>


### [ICLR2021](https://dblp.org/db/conf/iclr/iclr2021.html)


### [AISTATS2021](https://dblp.org/db/conf/aistats/aistats2021.html)
#### [Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties (Lisa Schut, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, medb corcoran, Yarin Gal)](https://proceedings.mlr.press/v130/schut21a.html)
<details>
    <summary>
        反実仮想説明（CE）は、機械学習分類器が特定の決定を下す理由を示すための実用的なツールである。 CEが有用であるためには、ユーザが解釈しやすいことが重要である。 解釈可能なCEを生成するための既存の方法は、補助的な生成モデルに依存しており、複雑なデータセットには適していない可能性がある。 我々は、分類器の予測不確実性を利用することで、補助モデルを用いずに、ホワイトボックス設定において解釈可能なCEを生成するためのシンプルで高速な手法を導入する。 我々の実験は、我々の提案するアルゴリズムが、IM1スコア（Van Looveren et al. さらに、我々のアプローチでは、CEの不確実性を推定することができ、これは医療領域のようなセーフティクリティカルなアプリケーションにおいて重要である。
    </summary>
    Counterfactual explanations (CEs) are a practical tool for demonstrating why machine learning classifiers make particular decisions. For CEs to be useful, it is important that they are easy for users to interpret. Existing methods for generating interpretable CEs rely on auxiliary generative models, which may not be suitable for complex datasets, and incur engineering overhead. We introduce a simple and fast method for generating interpretable CEs in a white-box setting without an auxiliary model, by using the predictive uncertainty of the classifier. Our experiments show that our proposed algorithm generates more interpretable CEs, according to IM1 scores (Van Looveren et al., 2019), than existing methods. Additionally, our approach allows us to estimate the uncertainty of a CE, which may be important in safety-critical applications, such as those in the medical domain.
</details>


### [AAAI2021](https://dblp.org/db/conf/aaai/aaai2021.html)
#### [Counterfactual Explanations for Oblique Decision Trees:Exact, Efficient Algorithms (Carreira-Perpiñán, M. . Á., & Hada, S. S.)](https://ojs.aaai.org/index.php/AAAI/article/view/16851)
<details>
    <summary>
        我々は反事実的説明（counterfactual explanation）について考察する。この問題は、与えられた分類器のもとでターゲットクラスに分類されるように、ソース入力インスタンスの特徴を最小に調整する問題である。 これは、訓練されたモデルに問い合わせ、その決定を覆す可能性のある行動を提案する方法として、最近注目されている。 数学的には、この問題は敵対的な事例を見つける問題と形式的に等価であり、これも最近大きな注目を集めている。 反実仮想説明や敵対的な例に関するほとんどの研究は、ニューラルネットのような微分可能な分類器に焦点を当てている。 我々は、軸合わせと斜め（超平面分割を持つ）の分類木に焦点を当てる。 ここでは、反実仮想最適化問題は非凸で微分不可能であるが、高次元の特徴ベクトルや連続的特徴量とカテゴリー的特徴量の両方を用いても、厳密解が非常に効率的に計算できることを示し、様々なデータセットや設定でそれを実証する。 この結果は、解釈可能性と反事実的説明が特に重要である、金融、医学、法学への応用に特に関連する。
    </summary>
    We consider counterfactual explanations, the problem of minimally adjusting features in a source input instance so that it is classified as a target class under a given classifier. This has become a topic of recent interest as a way to query a trained model and suggest possible actions to overturn its decision. Mathematically, the problem is formally equivalent to that of finding adversarial examples, which also has attracted significant attention recently. Most work on either counterfactual explanations or adversarial examples has focused on differentiable classifiers, such as neural nets. We focus on classification trees, both axis-aligned and oblique (having hyperplane splits). Although here the counterfactual optimization problem is nonconvex and nondifferentiable, we show that an exact solution can be computed very efficiently, even with high-dimensional feature vectors and with both continuous and categorical features, and demonstrate it in different datasets and settings. The results are particularly relevant for finance, medicine or legal applications, where interpretability and counterfactual explanations are particularly important.
</details>

#### [Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization (Kanamori, K., Takagi, T., Kobayashi, K., Ike, Y., Uemura, K., & Arimura, H.)](https://ojs.aaai.org/index.php/AAAI/article/view/17376)
<details>
    <summary>
        機械学習モデルの事後説明手法は、意思決定を支援するために広く使われている。 一般的な手法の1つは、Counterfactual Explanation（CE）であり、Actionable Recourse（行動可能な再コース）としても知られている。 摂動ベクトルが与えられれば、ユーザーはそれを自分の望む決定結果を得るための「行動」として解釈することができる。 しかし実際には、摂動ベクトルだけを示しても、ユーザがアクションを実行するには不十分な場合が多い。 なぜなら、因果関係のように特徴量間に非対称な相互作用がある場合、アクションの総コストは特徴量の変化順序に依存すると予想されるからである。 そのため、実用的なCE手法では、摂動ベクトルに加えて、特徴量を変化させる適切な順序を与える必要がある。 この目的のために、我々はOrdCE（Ordered Counterfactual Explanation）と呼ばれる新しい枠組みを提案する。 我々は、行動と順序のペアを特徴量の相互作用に基づいて評価する新しい目的関数を導入する。 最適なペアを抽出するために、本目的関数を用いた混合整数線形最適化アプローチを提案する。 実際のデータセットを用いた数値実験により、我々のOrdCEの有効性が、順序のないCE手法と比較して実証された。
    </summary>
    Post-hoc explanation methods for machine learning models have been widely used to support decision-making. One of the popular methods is Counterfactual Explanation (CE), also known as Actionable Recourse, which provides a user with a perturbation vector of features that alters the prediction result. Given a perturbation vector, a user can interpret it as an "action" for obtaining one's desired decision result. In practice, however, showing only a perturbation vector is often insufficient for users to execute the action. The reason is that if there is an asymmetric interaction among features, such as causality, the total cost of the action is expected to depend on the order of changing features. Therefore, practical CE methods are required to provide an appropriate order of changing features in addition to a perturbation vector. For this purpose, we propose a new framework called Ordered Counterfactual Explanation (OrdCE). We introduce a new objective function that evaluates a pair of an action and an order based on feature interaction. To extract an optimal pair, we propose a mixed-integer linear optimization approach with our objective function. Numerical experiments on real datasets demonstrated the effectiveness of our OrdCE in comparison with unordered CE methods.
</details>


### [FAccT2021](https://dblp.org/db/conf/fat/facct2021.html)
#### [Algorithmic Recourse: from Counterfactual Explanations to Interventions (Amir-Hossein Karimi, Bernhard Schölkopf, Isabel Valera)](https://dl.acm.org/doi/10.1145/3442188.3445899)
<details>
    <summary>
        機械学習が、結果的な意思決定（例えば、公判前の保釈やローンの承認など）を通知するためにますます使用されるようになるにつれて、システムがどのようにその決定に至ったかを説明し、また好ましい決定を達成するための行動を提案することが重要になる。 反実仮想的説明（「望ましい結果が生じるために、世界がどのように違っていたら（いなければならなかった）か」）は、これらの基準を満たすことを目的としている。 既存の研究は主に、幅広い設定に対して反実仮想的説明を得るためのアルゴリズムを設計することに焦点を当ててきた。 しかし、結局のところ、主な目的の1つは、人々が理解するだけでなく、行動できるようにすることであることは、ほとんど見落とされてきた。 平たく言えば、反実仮想的な説明は、個人がどこに到達すべきかを知らせるが、そこに到達する方法は教えない。 この研究では、因果推論に基づき、反事実的説明を推奨される一連の行動として使用することに注意を喚起する。 その代わりに、最も近い反事実的説明による遡及から最小限の介入による遡及へのパラダイム転換を提案し、説明から介入へと焦点を移す。
    </summary>
    As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.
</details>



## 2020
### [NeurIPS2020](https://dblp.org/db/conf/nips/neurips2020.html)
#### [Algorithmic recourse under imperfect causal knowledge: a probabilistic approach (Amir-Hossein Karimi, Julius von Kügelgen, Bernhard Schölkopf, Isabel Valera)](https://proceedings.neurips.cc/paper/2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html)
<details>
    <summary>
        最近の研究では、アルゴリズムによる再利用のための行動を推奨するための反実仮想的説明の限界が議論され、特徴間の因果関係を考慮する必要性が主張されている。 残念なことに、実際には、真の基礎となる構造的因果モデルは一般に未知である。 本研究では、まず、真の構造方程式にアクセスすることなく、リコースを保証することは不可能であることを示す。 この限界に対処するため、限られた因果関係知識（例えば因果グラフのみ）のもとで、高い確率で遡及を達成する最適行動を選択する2つの確率論的アプローチを提案する。 1つ目は、加法性ガウスノイズ下での構造方程式の不確実性を捉え、ベイズモデル平均を用いて反事実分布を推定する。 2つ目は、構造方程式の仮定を取り除き、その代わりに、遡求を求める人に類似した個体に対する遡求行動の平均効果を計算することで、遡求の新しい部分集団ベースの介入概念を導く。 次に、最適な遡求行動を選択するための勾配ベースの手順を導出し、提案されたアプローチが、非確率的なベースラインよりも、不完全な因果的知識の下で、より信頼性の高い推薦を導くことを実証的に示す。
    </summary>
    Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines.
</details>

#### [Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses (Kaivalya Rawal, Himabindu Lakkaraju)](https://proceedings.neurips.cc/paper/2020/hash/8ee7730e97c67473a424ccfeff49ab20-Abstract.html)
<details>
    <summary>
        重大な意思決定において予測モデルの導入が進むにつれ、影響を受けた個人に救済措置を提供できるアルゴリズムの開発に多くの関心が寄せられている。 このようなツールを開発することは重要であるが、予測モデルを分析・解釈し、実世界に展開する前に、そのモデルが提供する救済措置が有意義かつ差別的でないことを確認するために、徹底的に検証することはさらに重要である。 この目的を達成するために、我々はActionable Recourse Summaries (AReS)と呼ばれる新しいモデルにとらわれないフレームワークを提案する。 我々は、母集団全体のリコースコストを最小化しながら、リコースの正しさと説明の解釈可能性を同時に最適化する新しい目的を定式化する。 より具体的には、我々の目的により、リコースの正しさを最適に保証しながら、少数のコンパクトなルールセットを学習することが可能となる。 また、個体に対するリコースを生成するために提案されているいくつかの先行アプローチが、本フレームワークの特殊なケースであることを理論的に示す。 実世界のデータセットによる実験的評価とユーザー研究により、我々のフレームワークが、意思決定者に、あらゆるブラックボックスモデルに対応するリソースの包括的な概観を提供し、その結果、望ましくないモデルの偏りや差別を検出するのに役立つことを実証する。
    </summary>
    As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyze and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination.
</details>

#### [Decisions, Counterfactual Explanations and Strategic Behavior (Stratis Tsirtsis, Manuel Gomez Rodriguez)](https://proceedings.neurips.cc/paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html)
<details>
    <summary>
        データ主導の予測モデルが意思決定にますます用いられるようになるにつれ、意思決定者は、その意思決定が有益なものであるためには何が変わらなければならないかを個人が理解できるような説明を提供すべきだと主張されてきた。 しかし、個人が上記のような反実仮想的な説明を用いて、戦略的に努力を投資し、有益な決定を受ける可能性を最大化する可能性については、これまでほとんど議論されてこなかった。 本論文では、このような戦略的設定において、効用の観点から最適な政策と反事実説明を見つけることを目的とする。 我々はまず、あらかじめ定義された政策が与えられたとき、最適な反実仮想的説明の集合を求める問題がNP困難であることを示す。 次に、対応する目的が非減少であり、部分モジュラリティを満たすことで、標準的な貪欲アルゴリズムが近似保証を享受できることを示す。 さらに、最適政策と反事実説明の集合の両方を共同で求める問題は、非単調劣モジュラ関数を最大化することに帰着することを示す。 その結果、この問題を解くために最近のランダム化アルゴリズムを用いることができ、これは近似保証も提供する。 最後に、マトロイド制約を問題定式化に組み込むことで、反実仮想説明の最適セットの多様性を増加させ、母集団の全領域にわたる個人に自己改善のインセンティブを与えることができることを示す。 合成データ、実際の貸出データ、クレジットカードデータを用いた実験により、我々の理論的知見を説明し、我々のアルゴリズムにより発見された反事実説明と決定方針が、いくつかの競合するベースラインよりも高い効用を達成することを示す。
    </summary>
    As data-driven predictive models are increasingly used to inform decisions, it has been argued that decision makers should provide explanations that help individuals understand what would have to change for these decisions to be beneficial ones. However, there has been little discussion on the possibility that individuals may use the above counterfactual explanations to invest effort strategically and maximize their chances of receiving a beneficial decision. In this paper, our goal is to find policies and counterfactual explanations that are optimal in terms of utility in such a strategic setting. We first show that, given a pre-defined policy, the problem of finding the optimal set of counterfactual explanations is NP-hard. Then, we show that the corresponding objective is nondecreasing and satisfies submodularity and this allows a standard greedy algorithm to enjoy approximation guarantees. In addition, we further show that the problem of jointly finding both the optimal policy and set of counterfactual explanations reduces to maximizing a non-monotone submodular function. As a result, we can use a recent randomized algorithm to solve the problem, which also offers approximation guarantees. Finally, we demonstrate that, by incorporating a matroid constraint into the problem formulation, we can increase the diversity of the optimal set of counterfactual explanations and incentivize individuals across the whole spectrum of the population to self improve. Experiments on synthetic and real lending and credit card data illustrate our theoretical findings and show that the counterfactual explanations and decision policies found by our algorithms achieve higher utility than several competitive baselines.
</details>


### [ICML2020](https://dblp.org/db/conf/icml/icml2020.html)


### [IJCAI2020](https://dblp.org/db/conf/ijcai/ijcai2020.html)
#### [Relation-Based Counterfactual Explanations for Bayesian Network Classifiers (Emanuele Albini, Antonio Rago, Pietro Baroni, Francesca Toni)](https://www.ijcai.org/proceedings/2020/63)
<details>
    <summary>
        我々は、様々なベイジアンネットワーク分類器（BC）、例えばシングルラベルやマルチラベル、バイナリや多次元などに対して、反事実的説明（CFX）を生成する一般的な方法を提案する。 我々は、確率的な情報ではなく、分類の理由を示す変数間の（重要かつ潜在的な）影響関係から構築される説明に焦点を当てる。 CFXの特性を理論的に分析することで、CFXが分類プロセスにおける（潜在的に）重要な要因を示す役割を果たすことを示す。 次に、CFXが、例えば人種が仮釈放違反の予測に関与している場合など、実世界の設定において有用な情報を提供することを、様々なBCについて実証的に証明し、文献にある既存の説明手法に対して固有の利点を持つことを示す。
    </summary>
    We propose a general method for generating counterfactual explanations (CFXs) for a range of Bayesian Network Classifiers (BCs), e.g. single- or multi-label, binary or multidimensional. We focus on explanations built from relations of (critical and potential) influence between variables, indicating the reasons for classifications, rather than any probabilistic information. We show by means of a theoretical analysis of CFXs’ properties that they serve the purpose of indicating (potentially) pivotal factors in the classification process, whose absence would give rise to different classifications. We then prove empirically for various BCs that CFXs provide useful information in real world settings, e.g. when race plays a part in parole violation prediction, and show that they have inherent advantages over existing explanation methods in the literature.
</details>

#### [DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization (Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Hiroki Arimura)](https://www.ijcai.org/proceedings/2020/395)
<details>
    <summary>
        逆事実説明（Counterfactual Explanation：CE）は、分類器から得られる予測結果を変更するように摂動ベクトルを提供する、事後的説明手法の1つである。 ユーザはこの摂動ベクトルを、希望する判定結果を得るための「行動」として直接解釈することができる。 しかし、既存の手法で抽出されたアクションは、特徴相関や異常値リスクといった経験的データ分布に対応する特性についてユーザが十分に考慮していないため、しばしばユーザにとって非現実的なものとなる。 そこで本研究では、経験的データ分布に対する現実性を評価することで、ユーザが実行可能なアクションを提案する新しいCEの枠組みを提案する。 提案手法のキーとなるアイデアは、マハラノビス距離と局所外れ値係数に基づく新しいコスト関数を定義することである。 そして、このコスト関数を最小化することで最適な行動を抽出する混合整数線形最適化アプローチを提案する。 実際のデータセットを用いた実験により、CEに対する既存の手法と比較して、本手法の有効性を確認する。
    </summary>
    Counterfactual Explanation (CE) is one of the post-hoc explanation methods that provides a perturbation vector so as to alter the prediction result obtained from a classifier. Users can directly interpret the perturbation as an "action" for obtaining their desired decision results. However, an action extracted by existing methods often becomes unrealistic for users because they do not adequately care about the characteristics corresponding to the empirical data distribution such as feature-correlations and outlier risk. To suggest an executable action for users, we propose a new framework of CE for extracting an action by evaluating its reality on the empirical data distribution. The key idea of our proposed method is to define a new cost function based on the Mahalanobis' distance and the local outlier factor. Then, we propose a mixed-integer linear optimization approach to extracting an optimal action by minimizing our cost function. By experiments on real datasets, we confirm the effectiveness of our method in comparison with existing methods for CE.
</details>


### [KDD2020](https://dblp.org/db/conf/kdd/kdd2020.html)


### [ICLR2020](https://dblp.org/db/conf/iclr/iclr2020.html)


### [AISTATS2020](https://dblp.org/db/conf/aistats/aistats2020.html)
#### [Model-Agnostic Counterfactual Explanations for Consequential Decisions (Amir-Hossein Karimi, Gilles Barthe, Borja Balle, Isabel Valera)](https://proceedings.mlr.press/v108/karimi20a.html)
<details>
    <summary>
        公判前保釈やローン承認などの文脈において、個人レベルでの結果的意思決定を支援するために予測モデルが使用されることが増えている。 その結果、影響を受ける個人が、なぜ予測が出力されたかを理解するだけでなく、望ましい結果を得るためにどのように行動すべきかを理解するのに役立つ説明を提供する社会的・法的圧力が高まっている。 この目的のために、いくつかの研究が、最も近い反事実説明を生成する最適化ベースの手法を提案している。 しかし、これらの手法はしばしば、モデルの特定のサブセット（例えば、決定木や線形モデル）と微分可能な距離関数に制限されている。 対照的に、我々は、形式的検証の標準的な理論とツールを基礎とし、距離関数（目的）と予測モデル（制約）の両方が論理式として表現される、一連の充足可能性問題を解く新しいアルゴリズムを提案する。 実世界のデータを用いた実験によって示されるように、我々のアルゴリズムは、i) モデルにとらわれない（{非}線形、{非}微分可能、{非}凸）、ii) データの種類にとらわれない（異種特徴）、iii) 距離にとらわれない（l0、l1、l8、およびそれらの組み合わせ）、iv) どのサンプルに対しても、もっともらしく多様な反事実を生成できる（すなわち、100%カバレッジ）、v) 証明可能な最適距離である。
    </summary>
    Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable, {non-}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic (l0, l1, l8, and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100% coverage); and v) at provably optimal distances.
</details>


### [AAAI2020](https://dblp.org/db/conf/aaai/aaai2020.html)
#### [Synthesizing Action Sequences for Modifying Model Decisions (Ramakrishnan, G., Lee, Y. C., & Albarghouthi, A.)](https://ojs.aaai.org/index.php/AAAI/article/view/5996)
<details>
    <summary>
        例えば、ある人が融資を受けられないというような、結果的な決定をモデルが下すとき、その決定を有利に変更するためにその人ができることについて、行動可能で現実的なフィードバックを追加的に生成する必要がある。 我々はこの問題をプログラム合成のレンズを通して投げかける。我々のゴールは、人がうまく実行すればその人の分類を変えることができる最適な（現実的に最も安い、あるいは最も単純な）一連の行動を合成することである。 我々は、探索ベースのプログラム合成とテスト時間敵対的攻撃を組み合わせて、ドメイン固有の行動集合に対する行動シーケンスを構築する、新規かつ一般的なアプローチを提示する。 本アプローチの有効性を多くのディープニューラルネットワークで実証する。
    </summary>
    When a model makes a consequential decision, e.g., denying someone a loan, it needs to additionally generate actionable, realistic feedback on what the person can do to favorably change the decision. We cast this problem through the lens of program synthesis, in which our goal is to synthesize an optimal (realistically cheapest or simplest) sequence of actions that if a person executes successfully can change their classification. We present a novel and general approach that combines search-based program synthesis and test-time adversarial attacks to construct action sequences over a domain-specific set of actions. We demonstrate the effectiveness of our approach on a number of deep neural networks.
</details>

#### [CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines (Akula, A., Wang, S., & Zhu, S.-C. )](https://ojs.aaai.org/index.php/AAAI/article/view/5643)
<details>
    <summary>
        我々はCoCoX（Conceptual and Counterfactual Explanationsの略）を発表する。CoCoXは深層畳み込みニューラルネットワーク（CNN）による意思決定を説明するためのモデルである。 認知心理学では、人間がモデル予測に対する代替案を想像する際にズームインする要因（または意味レベルの特徴）は、しばしばフォールトラインと呼ばれる。 これに動機づけられた我々のCoCoXモデルは、断層線を用いてCNNによる決定を説明する。 具体的には、CNN分類モデルMがクラスcpredを予測する入力画像Iが与えられたとき、我々の断層線に基づく説明は、説明可能な概念と呼ばれる、MによるIの分類カテゴリを別の指定されたクラスcaltに変更するために、Iに追加または削除する必要がある最小限の意味レベルの特徴（例えば、シマウマの縞模様、犬の尖った耳）を特定する。 我々は、フォールトラインの概念的かつ反事実的な性質により、我々のCoCoX説明は、複雑な深層学習モデルの内部動作を理解するために、専門家と非専門家の両方のユーザーにとって実用的でより自然であると主張する。 広範な定量的・定性的実験によって我々の仮説が検証され、CoCoXが最先端の説明可能なAIモデルを大幅に上回ることが示された。 
    </summary>
    We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models. Our implementation is available at https://github.com/arjunakula/CoCoX
</details>


### [FAT*2020](https://dblp.org/db/conf/fat/fat2020.html)
#### [The philosophical basis of algorithmic recourse (Suresh Venkatasubramanian, Mark Alfano)](https://dl.acm.org/doi/10.1145/3351095.3372876)
<details>
    <summary>
        哲学者たちは、ある種の倫理的に重要な価値は、さまざまな反事実的シナリオにわたって相関的な利益を体系的にもたらすという意味で、モダリー・ロバストであることを立証してきた。 本稿では、様々な反事実的シナリオにおいて、アルゴリズムや官僚機構による不利な決定を覆す体系的なプロセスであるリコースが、そのようなmodally robustな善であると主張する。 特に、時間的に拡張された主体性と信頼という、良い人生の2つの本質的な構成要素は、遡求によって支えられていると主張する。 我々は、遡求の概念化、運用化、実施に関する既存のアプローチを批判する。 これらの批判に基づき、遡求に対する修正されたアプローチを提案し、特に最も裕福でない人々のために、どのように遡求が実施されるかの例を示す。
    </summary>
    Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse. We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off.
</details>

#### [The hidden assumptions behind counterfactual explanations and principal reasons (Solon Barocas, Andrew D. Selbst, Manish Raghavan)](https://dl.acm.org/doi/10.1145/3351095.3372830)
<details>
    <summary>
        反事実的説明は、機械学習モデルの決定を説明する方法として、技術、法律、ビジネス界で注目を集めている。 これらの説明は、米国のクレジット法で要求される、長い間確立されてきた「主要な理由」説明と共通する特徴を持つ。 両者とも、最も関連性が高いとみなされる一連の特徴を強調し、他の特徴を伏せることによって、決定を説明する。 これらの「特徴を強調する説明」には、いくつかの望ましい特性がある： モデルの複雑さに制約がないこと、モデルの開示を必要としないこと、異なる決定を下すために何が必要であったかを詳細に説明できること、法律の遵守を自動化できるように見えること、などである。 本稿では、特徴強調説明の有用性が、見落とされやすい多くの仮定に依存していることを示す。すなわち、推奨される特徴値の変化が現実の行動と明確に対応すること、学習データの分布のみを見ることで特徴を相応にすることができること、特徴は目下の意思決定にのみ関連すること、基礎となるモデルは時間的に安定しており、単調であり、二値結果に限定されること、などである。次に、特徴を強調する説明が自律性を尊重することを目指すというパラドックス、特徴を強調する説明が意思決定者に与える抑制されない力、これらの説明を有用にすることとモデルを隠しておく必要性との間の緊張など、これらの仮定を認め、それに対処しようとすることのいくつかの結果を探る。新しい研究は、特徴を強調する説明が、我々が明らかにした問題のいくつかを回避することができるいくつかの方法を示唆しているが、モデルにおける特徴と現実世界における行動との間の断絶---そして、これを補うために必要な主観的選択---は、これらの技術が有用に実装される前に理解されなければならない。
    </summary>
    Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others. These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.
</details>

#### [Explaining machine learning classifiers through diverse counterfactual explanations (Ramaravind K. Mothilal, Amit Sharma, Chenhao Tan)](https://dl.acm.org/doi/10.1145/3351095.3372850)
<details>
    <summary>
        機械学習モデルのポストホック説明は、人々がアルゴリズムの予測を理解し、行動するために非常に重要である。 説明の興味深いクラスは、異なる予測を得る方法を人々に示す仮想的な例である反事実を通してである。 我々は、効果的な反実仮想説明は、ユーザの文脈と制約を考慮した反実仮想行動の実現可能性と、提示される反実仮想の多様性という2つの特性を満たすべきだと考える。 この目的のために、決定論的な点過程に基づいて、多様な反事実説明を生成し、評価する枠組みを提案する。 反実仮想の実行可能性を評価するために、反実仮想に基づく手法と他の局所的説明手法との比較を可能にする指標を提供する。 さらに、反実仮想を最適化する際に必要なトレードオフを取り上げ、因果的な意味を指摘する。 実世界の4つのデータセットを用いた実験により、我々のフレームワークが、多様で局所的な決定境界によく近似した反事実の集合を生成できることが示され、多様な反事実を生成する先行アプローチを凌駕する。 
    </summary>
    Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.
</details>



---
